{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff30e898",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08814b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import trafilatura\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# import io\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "from youtube_transcript_api.proxies import WebshareProxyConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f66c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Installing spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cac53090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    tokenizer=\"ProsusAI/finbert\"\n",
    ")\n",
    "\n",
    "def get_text_sentiment_score(text: str, max_chars=512) -> float:\n",
    "    if not text or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    chunks = [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n",
    "    scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        result = sentiment_analyzer(chunk)[0]\n",
    "        label = result[\"label\"].upper()\n",
    "        score = float(result[\"score\"])\n",
    "\n",
    "        if \"POS\" in label:\n",
    "            scores.append(score)\n",
    "        elif \"NEG\" in label:\n",
    "            scores.append(-score)\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return sum(scores) / len(scores) if scores else 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d36b180",
   "metadata": {},
   "source": [
    "# Using RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ec47e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEEDS = [\n",
    "    \"https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=10000664\",\n",
    "]\n",
    "\n",
    "MAX_ARTICLES_PER_FEED = 30\n",
    "CONTEXT_SENTENCES = 1\n",
    "TICKER_LIST_PATH = Path(\"tickers.csv\")  # optional: columns ticker,name\n",
    "\n",
    "TICKER_RE = re.compile(r\"(?<![A-Z])\\$?[A-Z]{2,5}(?![A-Z])\")\n",
    "TICKER_STOP = {\n",
    "    \"A\", \"AN\", \"AND\", \"ARE\", \"AS\", \"AT\", \"BE\", \"BUT\", \"BY\", \"CAN\", \"CO\", \"FOR\",\n",
    "    \"FROM\", \"HAS\", \"HAVE\", \"IN\", \"IS\", \"IT\", \"ITS\", \"NOT\", \"OF\", \"ON\", \"OR\",\n",
    "    \"THE\", \"TO\", \"WAS\", \"WERE\", \"WILL\", \"WITH\",\n",
    "}\n",
    "\n",
    "SECTOR_KEYWORDS = {\n",
    "    \"Technology\": [\"tech\", \"software\", \"technology\", \"cloud\", \"ai\", \"artificial intelligence\",\n",
    "                   \"chip\", \"semiconductor\", \"digital\", \"platform\", \"app\", \"data\", \"cyber\"],\n",
    "    \"Finance\": [\"bank\", \"financial\", \"finance\", \"investment\", \"trading\", \"market\",\n",
    "                \"stock\", \"equity\", \"bond\", \"credit\", \"lending\", \"mortgage\"],\n",
    "    \"Healthcare\": [\"health\", \"medical\", \"pharmaceutical\", \"drug\", \"biotech\", \"hospital\",\n",
    "                    \"treatment\", \"patient\", \"fda\", \"clinical\", \"therapy\"],\n",
    "    \"Energy\": [\"oil\", \"gas\", \"energy\", \"petroleum\", \"renewable\", \"solar\", \"wind\",\n",
    "               \"electric\", \"power\", \"fuel\", \"drilling\", \"crude\"],\n",
    "    \"Retail\": [\"retail\", \"store\", \"shopping\", \"consumer\", \"e-commerce\", \"online shopping\",\n",
    "               \"merchandise\", \"sales\", \"retailer\"],\n",
    "    \"Automotive\": [\"car\", \"automotive\", \"vehicle\", \"auto\", \"truck\", \"electric vehicle\",\n",
    "                   \"ev\", \"manufacturing\", \"tesla\"],\n",
    "    \"Real Estate\": [\"real estate\", \"property\", \"housing\", \"construction\", \"mortgage\",\n",
    "                    \"development\", \"reit\"],\n",
    "    \"Telecommunications\": [\"telecom\", \"communication\", \"wireless\", \"5g\", \"network\", \"internet\"],\n",
    "    \"Aerospace\": [\"aerospace\", \"aircraft\", \"defense\", \"boeing\", \"space\"],\n",
    "    \"Consumer Goods\": [\"consumer goods\", \"packaged goods\", \"cpg\"],\n",
    "}\n",
    "\n",
    "def extract_article_text(url: str) -> str | None:\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    if not downloaded:\n",
    "        return None\n",
    "\n",
    "    text = trafilatura.extract(\n",
    "        downloaded,\n",
    "        include_comments=False,\n",
    "        include_tables=False,\n",
    "        include_formatting=False\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def load_ticker_map(path: Path):\n",
    "    ticker_to_name = {}\n",
    "    name_to_ticker = {}\n",
    "    if not path.exists():\n",
    "        return ticker_to_name, name_to_ticker\n",
    "\n",
    "    with path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            ticker = (row.get(\"ticker\") or \"\").strip().upper()\n",
    "            name = (row.get(\"name\") or \"\").strip()\n",
    "            if not ticker or not name:\n",
    "                continue\n",
    "            ticker_to_name[ticker] = name\n",
    "            name_to_ticker[name.lower()] = ticker\n",
    "\n",
    "    return ticker_to_name, name_to_ticker\n",
    "\n",
    "\n",
    "ticker_to_name, name_to_ticker = load_ticker_map(TICKER_LIST_PATH)\n",
    "\n",
    "\n",
    "def fetch_articles(feed_url, max_items=30):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    articles = []\n",
    "    for entry in feed.entries[:max_items]:\n",
    "        text = extract_article_text(entry.link)\n",
    "        if not text:\n",
    "            continue\n",
    "        articles.append({\n",
    "            \"title\": entry.title,\n",
    "            \"url\": entry.link,\n",
    "            \"published\": entry.get(\"published\"),\n",
    "            \"text\": text,\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_tickers(text):\n",
    "    tickers = []\n",
    "    for m in TICKER_RE.findall(text):\n",
    "        t = m.replace(\"$\", \"\").upper()\n",
    "        if t in TICKER_STOP:\n",
    "            continue\n",
    "        if ticker_to_name and t not in ticker_to_name:\n",
    "            continue\n",
    "        tickers.append(t)\n",
    "    return tickers\n",
    "\n",
    "\n",
    "def get_companies(doc):\n",
    "    companies = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
    "    mapped = []\n",
    "    for name in companies:\n",
    "        ticker = name_to_ticker.get(name.lower())\n",
    "        if ticker:\n",
    "            mapped.append(ticker)\n",
    "        else:\n",
    "            mapped.append(name)\n",
    "    return mapped\n",
    "\n",
    "\n",
    "def get_sectors(text_lower):\n",
    "    return [\n",
    "        sector for sector, keywords in SECTOR_KEYWORDS.items()\n",
    "        if any(kw in text_lower for kw in keywords)\n",
    "    ]\n",
    "\n",
    "\n",
    "def sentence_windows(sentences, idx, window=1):\n",
    "    start = max(0, idx - window)\n",
    "    end = min(len(sentences), idx + window + 1)\n",
    "    return \" \".join(sentences[start:end])\n",
    "\n",
    "\n",
    "def analyze_article_entities(article, window=1):\n",
    "    text = f\"{article.get('title','')} {article.get('text','')}\"\n",
    "    if not text.strip():\n",
    "        return [], [], []\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    stock_scores = defaultdict(list)\n",
    "    company_scores = defaultdict(list)\n",
    "    sector_scores = defaultdict(list)\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sent_doc = nlp(sent)\n",
    "        tickers = set(get_tickers(sent))\n",
    "        companies = set(get_companies(sent_doc))\n",
    "        sectors = set(get_sectors(sent.lower()))\n",
    "\n",
    "        if not tickers and not companies and not sectors:\n",
    "            continue\n",
    "\n",
    "        window_text = sentence_windows(sentences, i, window=window)\n",
    "        if window_text not in cache:\n",
    "            cache[window_text] = get_text_sentiment_score(window_text)\n",
    "        s = cache[window_text]\n",
    "\n",
    "        for t in tickers:\n",
    "            stock_scores[t].append(s)\n",
    "        for c in companies:\n",
    "            company_scores[c].append(s)\n",
    "        for sec in sectors:\n",
    "            sector_scores[sec].append(s)\n",
    "\n",
    "    return stock_scores, company_scores, sector_scores\n",
    "\n",
    "\n",
    "def aggregate_entities_with_sentiment(articles, window=1):\n",
    "    stock_stats = defaultdict(lambda: {\"mentions\": 0, \"scores\": []})\n",
    "    company_stats = defaultdict(lambda: {\"mentions\": 0, \"scores\": []})\n",
    "    sector_stats = defaultdict(lambda: {\"mentions\": 0, \"scores\": []})\n",
    "\n",
    "    for article in articles:\n",
    "        stock_scores, company_scores, sector_scores = analyze_article_entities(article, window=window)\n",
    "\n",
    "        for name, scores in stock_scores.items():\n",
    "            stock_stats[name][\"mentions\"] += 1\n",
    "            stock_stats[name][\"scores\"].append(sum(scores) / len(scores))\n",
    "\n",
    "        for name, scores in company_scores.items():\n",
    "            company_stats[name][\"mentions\"] += 1\n",
    "            company_stats[name][\"scores\"].append(sum(scores) / len(scores))\n",
    "\n",
    "        for name, scores in sector_scores.items():\n",
    "            sector_stats[name][\"mentions\"] += 1\n",
    "            sector_stats[name][\"scores\"].append(sum(scores) / len(scores))\n",
    "\n",
    "    def finalize(stats):\n",
    "        rows = []\n",
    "        for name, data in stats.items():\n",
    "            avg = sum(data[\"scores\"]) / len(data[\"scores\"]) if data[\"scores\"] else 0.0\n",
    "            rows.append({\n",
    "                \"name\": name,\n",
    "                \"mentions\": data[\"mentions\"],\n",
    "                \"avg_sentiment\": avg,\n",
    "            })\n",
    "        rows.sort(key=lambda x: x[\"mentions\"], reverse=True)\n",
    "        return rows\n",
    "\n",
    "    return {\n",
    "        \"stocks\": finalize(stock_stats),\n",
    "        \"companies\": finalize(company_stats),\n",
    "        \"sectors\": finalize(sector_stats),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24598c",
   "metadata": {},
   "source": [
    "## Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = []\n",
    "for feed in FEEDS:\n",
    "    all_articles.extend(fetch_articles(feed, max_items=MAX_ARTICLES_PER_FEED))\n",
    "\n",
    "results = aggregate_entities_with_sentiment(all_articles, window=CONTEXT_SENTENCES)\n",
    "\n",
    "results[\"stocks\"][:10], results[\"sectors\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66c9499",
   "metadata": {},
   "source": [
    "# Youtube Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7331e88f",
   "metadata": {},
   "source": [
    "## With Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6b77c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Mad Money 01/09/26 | Audio Only',\n",
       "  'url': 'https://www.youtube.com/watch?v=Ctwl6H8f9o8',\n",
       "  'published': '2026-01-10T00:59:12+00:00',\n",
       "  'video_id': 'Ctwl6H8f9o8',\n",
       "  'author': 'CNBC Television',\n",
       "  'summary': 'Listen to Jim Cramer’s personal guide through the confusing jungle of Wall Street investing, navigating through opportunities and pitfalls with one goal in mind - to help you make money. \\n\\nFor access to live and exclusive video from CNBC subscribe to CNBC PRO: https://cnb.cx/42d859g\\n\\n» Subscribe to CNBC TV: https://cnb.cx/SubscribeCNBCtelevision\\n» Subscribe to CNBC: https://cnb.cx/SubscribeCNBC\\n» Watch CNBC on the go with CNBC+: https://www.cnbc.com/WatchCNBCPlus\\n\\n\\nTurn to CNBC TV for the latest stock market news and analysis. From market futures to live price updates CNBC is the leader in business news worldwide.\\n\\nConnect with CNBC News Online\\nGet the latest news: http://www.cnbc.com/\\nFollow CNBC on LinkedIn: https://cnb.cx/LinkedInCNBC\\nFollow CNBC News on Instagram: https://cnb.cx/InstagramCNBC\\nFollow CNBC News on Facebook: https://cnb.cx/LikeCNBC\\nFollow CNBC on Threads: https://cnb.cx/threads\\nFollow CNBC News on X: https://cnb.cx/FollowCNBC\\nFollow CNBC on WhatsApp: https://cnb.cx/WhatsAppCNBC\\n\\nhttps://www.cnbc.com/select/best-credit-cards/ \\n\\n#CNBC\\n#CNBCTV',\n",
       "  'transcript_text': None,\n",
       "  'mention_counts': {'stocks': 0, 'companies': 0, 'sectors': 0}},\n",
       " {'title': 'Lightning Round: Buy some Talen Energy here, says Jim Cramer',\n",
       "  'url': 'https://www.youtube.com/watch?v=G2RIbBiI_G0',\n",
       "  'published': '2026-01-10T00:35:37+00:00',\n",
       "  'video_id': 'G2RIbBiI_G0',\n",
       "  'author': 'CNBC Television',\n",
       "  'summary': \"'Mad Money' host Jim Cramer weighs in on stocks including: Dynavax Tech, Talen Energy, SailPoint, Radiant Logistics, First Solar, Amprius Tech, and Arm Holdings.\",\n",
       "  'transcript_text': None,\n",
       "  'mention_counts': {'stocks': 0, 'companies': 0, 'sectors': 1}},\n",
       " {'title': \"Jim Cramer looks ahead to next week's market moving moments\",\n",
       "  'url': 'https://www.youtube.com/watch?v=_ArKhW0jaUI',\n",
       "  'published': '2026-01-10T00:30:27+00:00',\n",
       "  'video_id': '_ArKhW0jaUI',\n",
       "  'author': 'CNBC Television',\n",
       "  'summary': \"'Mad Money' host Jim Cramer looks ahead to next week's market game plan.\",\n",
       "  'transcript_text': None,\n",
       "  'mention_counts': {'stocks': 0, 'companies': 0, 'sectors': 1}}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUTUBE_FEED = \"https://www.youtube.com/feeds/videos.xml?channel_id=UCrp_UI8XtuYfpiqluWLD7Lw\"\n",
    "MAX_VIDEOS = 30\n",
    "\n",
    "def fetch_youtube_videos(feed_url, max_items=30):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    ytt_api = YouTubeTranscriptApi()\n",
    "    videos = []\n",
    "    for entry in feed.entries[:max_items]:\n",
    "        video_id = entry.get(\"yt_videoid\") or entry.get(\"id\")\n",
    "        transcript_text = None\n",
    "        if video_id:\n",
    "            try:\n",
    "                transcript = ytt_api.fetch(video_id)\n",
    "                snippets = getattr(transcript, \"snippets\", transcript)\n",
    "                transcript_text = \" \".join((s.text or \"\").strip() for s in snippets if getattr(s, \"text\", None))\n",
    "            except Exception:\n",
    "                transcript_text = None\n",
    "\n",
    "        combined_text = f\"{entry.get('title', '')} {transcript_text or ''}\"\n",
    "        tickers = get_tickers(combined_text)\n",
    "        doc = nlp(combined_text) if combined_text.strip() else None\n",
    "        companies = get_companies(doc) if doc else []\n",
    "        sectors = get_sectors(combined_text.lower())\n",
    "\n",
    "        videos.append({\n",
    "            \"title\": entry.get(\"title\"),\n",
    "            \"url\": entry.get(\"link\"),\n",
    "            \"published\": entry.get(\"published\"),\n",
    "            \"video_id\": video_id,\n",
    "            \"author\": entry.get(\"author\"),\n",
    "            \"summary\": entry.get(\"summary\"),\n",
    "            \"transcript_text\": transcript_text,\n",
    "            \"mention_counts\": {\n",
    "                \"stocks\": len(tickers),\n",
    "                \"companies\": len(companies),\n",
    "                \"sectors\": len(sectors),\n",
    "            },\n",
    "        })\n",
    "    return videos\n",
    "\n",
    "youtube_videos = fetch_youtube_videos(YOUTUBE_FEED, max_items=MAX_VIDEOS)\n",
    "youtube_videos[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86086e",
   "metadata": {},
   "source": [
    "## Check Ban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9c63002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=dQw4w9WgXcQ! This is most likely caused by:\n",
      "\n",
      "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
      "- You have done too many requests and your IP has been blocked by YouTube\n",
      "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
      "\n",
      "Ways to work around this are explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
      "\n",
      "\n",
      "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
      "⚠️ Likely IP banned\n"
     ]
    }
   ],
   "source": [
    "# from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "test_video_id = \"dQw4w9WgXcQ\"  # Rick Roll - should have captions\n",
    "try:\n",
    "    ytt_api = YouTubeTranscriptApi()\n",
    "    ytt_api.fetch(test_video_id)\n",
    "    print(\"✅ Not banned - can fetch transcripts\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    if \"429\" in str(e) or \"banned\" in str(e).lower() or \"blocked\" in str(e).lower():\n",
    "        print(\"⚠️ Likely IP banned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227e81e",
   "metadata": {},
   "source": [
    "## Without Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48c845ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Mad Money 01/09/26 | Audio Only',\n",
       "  'url': 'https://www.youtube.com/watch?v=Ctwl6H8f9o8',\n",
       "  'published': '2026-01-10T00:59:12+00:00',\n",
       "  'video_id': 'Ctwl6H8f9o8',\n",
       "  'author': 'CNBC Television',\n",
       "  'summary': 'Listen to Jim Cramer’s personal guide through the confusing jungle of Wall Street investing, navigating through opportunities and pitfalls with one goal in mind - to help you make money. \\n\\nFor access to live and exclusive video from CNBC subscribe to CNBC PRO: https://cnb.cx/42d859g\\n\\n» Subscribe to CNBC TV: https://cnb.cx/SubscribeCNBCtelevision\\n» Subscribe to CNBC: https://cnb.cx/SubscribeCNBC\\n» Watch CNBC on the go with CNBC+: https://www.cnbc.com/WatchCNBCPlus\\n\\n\\nTurn to CNBC TV for the latest stock market news and analysis. From market futures to live price updates CNBC is the leader in business news worldwide.\\n\\nConnect with CNBC News Online\\nGet the latest news: http://www.cnbc.com/\\nFollow CNBC on LinkedIn: https://cnb.cx/LinkedInCNBC\\nFollow CNBC News on Instagram: https://cnb.cx/InstagramCNBC\\nFollow CNBC News on Facebook: https://cnb.cx/LikeCNBC\\nFollow CNBC on Threads: https://cnb.cx/threads\\nFollow CNBC News on X: https://cnb.cx/FollowCNBC\\nFollow CNBC on WhatsApp: https://cnb.cx/WhatsAppCNBC\\n\\nhttps://www.cnbc.com/select/best-credit-cards/ \\n\\n#CNBC\\n#CNBCTV',\n",
       "  'transcript_text': None,\n",
       "  'mention_counts': {'stocks': 0, 'companies': 9, 'sectors': 3}},\n",
       " {'title': 'Lightning Round: Buy some Talen Energy here, says Jim Cramer',\n",
       "  'url': 'https://www.youtube.com/watch?v=G2RIbBiI_G0',\n",
       "  'published': '2026-01-10T00:35:37+00:00',\n",
       "  'video_id': 'G2RIbBiI_G0',\n",
       "  'author': 'CNBC Television',\n",
       "  'summary': \"'Mad Money' host Jim Cramer weighs in on stocks including: Dynavax Tech, Talen Energy, SailPoint, Radiant Logistics, First Solar, Amprius Tech, and Arm Holdings.\",\n",
       "  'transcript_text': None,\n",
       "  'mention_counts': {'stocks': 0, 'companies': 4, 'sectors': 3}},\n",
       " {'title': \"Jim Cramer looks ahead to next week's market moving moments\",\n",
       "  'url': 'https://www.youtube.com/watch?v=_ArKhW0jaUI',\n",
       "  'published': '2026-01-10T00:30:27+00:00',\n",
       "  'video_id': '_ArKhW0jaUI',\n",
       "  'author': 'CNBC Television',\n",
       "  'summary': \"'Mad Money' host Jim Cramer looks ahead to next week's market game plan.\",\n",
       "  'transcript_text': None,\n",
       "  'mention_counts': {'stocks': 0, 'companies': 0, 'sectors': 1}}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YOUTUBE_FEED = \"https://www.youtube.com/feeds/videos.xml?channel_id=UCrp_UI8XtuYfpiqluWLD7Lw\"\n",
    "MAX_VIDEOS = 30\n",
    "\n",
    "def fetch_youtube_videos_no_transcript(feed_url, max_items=30):\n",
    "    \"\"\"Fetch YouTube videos without transcripts - uses only title and summary from RSS feed\"\"\"\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    videos = []\n",
    "    \n",
    "    for entry in feed.entries[:max_items]:\n",
    "        video_id = entry.get(\"yt_videoid\") or entry.get(\"id\")\n",
    "        \n",
    "        # Use only title and summary (no transcript fetching)\n",
    "        combined_text = f\"{entry.get('title', '')} {entry.get('summary', '')}\"\n",
    "        \n",
    "        tickers = get_tickers(combined_text)\n",
    "        doc = nlp(combined_text) if combined_text.strip() else None\n",
    "        companies = get_companies(doc) if doc else []\n",
    "        sectors = get_sectors(combined_text.lower())\n",
    "\n",
    "        videos.append({\n",
    "            \"title\": entry.get(\"title\"),\n",
    "            \"url\": entry.get(\"link\"),\n",
    "            \"published\": entry.get(\"published\"),\n",
    "            \"video_id\": video_id,\n",
    "            \"author\": entry.get(\"author\"),\n",
    "            \"summary\": entry.get(\"summary\"),\n",
    "            \"transcript_text\": None,  # No transcript\n",
    "            \"mention_counts\": {\n",
    "                \"stocks\": len(tickers),\n",
    "                \"companies\": len(companies),\n",
    "                \"sectors\": len(sectors),\n",
    "            },\n",
    "        })\n",
    "    return videos\n",
    "\n",
    "youtube_videos_no_transcript = fetch_youtube_videos_no_transcript(YOUTUBE_FEED, max_items=MAX_VIDEOS)\n",
    "youtube_videos_no_transcript[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b9c275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_youtube_video_entities(video, window=1):\n",
    "    \"\"\"Analyze YouTube video for entities with sentiment (similar to analyze_article_entities)\"\"\"\n",
    "    # Combine title, summary, and transcript if available\n",
    "    text_parts = [\n",
    "        video.get('title', ''),\n",
    "        video.get('summary', ''),\n",
    "        video.get('transcript_text', '')\n",
    "    ]\n",
    "    text = ' '.join([part for part in text_parts if part])\n",
    "    \n",
    "    if not text.strip():\n",
    "        return defaultdict(list), defaultdict(list), defaultdict(list)\n",
    "\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    stock_scores = defaultdict(list)\n",
    "    company_scores = defaultdict(list)\n",
    "    sector_scores = defaultdict(list)\n",
    "\n",
    "    cache = {}\n",
    "\n",
    "    for i, sent in enumerate(sentences):\n",
    "        sent_doc = nlp(sent)\n",
    "        tickers = set(get_tickers(sent))\n",
    "        companies = set(get_companies(sent_doc))\n",
    "        sectors = set(get_sectors(sent.lower()))\n",
    "\n",
    "        if not tickers and not companies and not sectors:\n",
    "            continue\n",
    "\n",
    "        window_text = sentence_windows(sentences, i, window=window)\n",
    "        if window_text not in cache:\n",
    "            cache[window_text] = get_text_sentiment_score(window_text)\n",
    "        s = cache[window_text]\n",
    "\n",
    "        for t in tickers:\n",
    "            stock_scores[t].append(s)\n",
    "        for c in companies:\n",
    "            company_scores[c].append(s)\n",
    "        for sec in sectors:\n",
    "            sector_scores[sec].append(s)\n",
    "\n",
    "    return stock_scores, company_scores, sector_scores\n",
    "\n",
    "\n",
    "def aggregate_youtube_entities_with_sentiment(videos, window=1):\n",
    "    \"\"\"Aggregate YouTube video entities with sentiment (similar to aggregate_entities_with_sentiment)\"\"\"\n",
    "    stock_stats = defaultdict(lambda: {\"mentions\": 0, \"scores\": []})\n",
    "    company_stats = defaultdict(lambda: {\"mentions\": 0, \"scores\": []})\n",
    "    sector_stats = defaultdict(lambda: {\"mentions\": 0, \"scores\": []})\n",
    "\n",
    "    for video in videos:\n",
    "        stock_scores, company_scores, sector_scores = analyze_youtube_video_entities(video, window=window)\n",
    "\n",
    "        for name, scores in stock_scores.items():\n",
    "            stock_stats[name][\"mentions\"] += 1\n",
    "            stock_stats[name][\"scores\"].append(sum(scores) / len(scores) if scores else 0.0)\n",
    "\n",
    "        for name, scores in company_scores.items():\n",
    "            company_stats[name][\"mentions\"] += 1\n",
    "            company_stats[name][\"scores\"].append(sum(scores) / len(scores) if scores else 0.0)\n",
    "\n",
    "        for name, scores in sector_scores.items():\n",
    "            sector_stats[name][\"mentions\"] += 1\n",
    "            sector_stats[name][\"scores\"].append(sum(scores) / len(scores) if scores else 0.0)\n",
    "\n",
    "    def finalize(stats):\n",
    "        rows = []\n",
    "        for name, data in stats.items():\n",
    "            avg = sum(data[\"scores\"]) / len(data[\"scores\"]) if data[\"scores\"] else 0.0\n",
    "            rows.append({\n",
    "                \"name\": name,\n",
    "                \"mentions\": data[\"mentions\"],\n",
    "                \"avg_sentiment\": avg,\n",
    "            })\n",
    "        rows.sort(key=lambda x: x[\"mentions\"], reverse=True)\n",
    "        return rows\n",
    "\n",
    "    return {\n",
    "        \"stocks\": finalize(stock_stats),\n",
    "        \"companies\": finalize(company_stats),\n",
    "        \"sectors\": finalize(sector_stats),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "54b9fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube Videos - Most Talked About Stocks:\n",
      "  BA: 1 mentions, avg sentiment: 0.000\n",
      "  SLB: 1 mentions, avg sentiment: 0.000\n",
      "\n",
      "YouTube Videos - Most Talked About Companies:\n",
      "  CNBC: 3 mentions, avg sentiment: 0.000\n",
      "  CNBC News Online: 1 mentions, avg sentiment: 0.000\n",
      "  WhatsApp: 1 mentions, avg sentiment: 0.000\n",
      "  FSLR: 1 mentions, avg sentiment: 0.000\n",
      "  Radiant Logistics: 1 mentions, avg sentiment: 0.000\n",
      "  Talen Energy: 1 mentions, avg sentiment: 0.000\n",
      "  SailPoint: 1 mentions, avg sentiment: 0.000\n",
      "  BA: 1 mentions, avg sentiment: 0.000\n",
      "  SLB: 1 mentions, avg sentiment: 0.000\n",
      "  SPY: 1 mentions, avg sentiment: 0.000\n",
      "\n",
      "YouTube Videos - Most Talked About Sectors:\n",
      "  Finance: 10 mentions, avg sentiment: 0.033\n",
      "  Technology: 9 mentions, avg sentiment: -0.032\n",
      "  Energy: 2 mentions, avg sentiment: 0.000\n",
      "  Automotive: 1 mentions, avg sentiment: 0.000\n",
      "  Healthcare: 1 mentions, avg sentiment: 0.834\n",
      "  Real Estate: 1 mentions, avg sentiment: -0.504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'name': 'BA', 'mentions': 1, 'avg_sentiment': 0.0},\n",
       "  {'name': 'SLB', 'mentions': 1, 'avg_sentiment': 0.0}],\n",
       " [{'name': 'CNBC', 'mentions': 3, 'avg_sentiment': 0.0},\n",
       "  {'name': 'CNBC News Online', 'mentions': 1, 'avg_sentiment': 0.0},\n",
       "  {'name': 'WhatsApp', 'mentions': 1, 'avg_sentiment': 0.0},\n",
       "  {'name': 'FSLR', 'mentions': 1, 'avg_sentiment': 0.0},\n",
       "  {'name': 'Radiant Logistics', 'mentions': 1, 'avg_sentiment': 0.0},\n",
       "  {'name': 'Talen Energy', 'mentions': 1, 'avg_sentiment': 0.0},\n",
       "  {'name': 'SailPoint', 'mentions': 1, 'avg_sentiment': 0.0},\n",
       "  {'name': 'BA', 'mentions': 1, 'avg_sentiment': 0.0},\n",
       "  {'name': 'SLB', 'mentions': 1, 'avg_sentiment': 0.0},\n",
       "  {'name': 'SPY', 'mentions': 1, 'avg_sentiment': 0.0}],\n",
       " [{'name': 'Finance', 'mentions': 10, 'avg_sentiment': 0.03299798965454102},\n",
       "  {'name': 'Technology', 'mentions': 9, 'avg_sentiment': -0.0315802428457472},\n",
       "  {'name': 'Energy', 'mentions': 2, 'avg_sentiment': 0.0},\n",
       "  {'name': 'Automotive', 'mentions': 1, 'avg_sentiment': 0.0},\n",
       "  {'name': 'Healthcare', 'mentions': 1, 'avg_sentiment': 0.8337874412536621},\n",
       "  {'name': 'Real Estate', 'mentions': 1, 'avg_sentiment': -0.503807544708252}])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyze YouTube videos and show stocks/companies/sectors with sentiment\n",
    "youtube_results = aggregate_youtube_entities_with_sentiment(youtube_videos_no_transcript, window=CONTEXT_SENTENCES)\n",
    "\n",
    "print(\"YouTube Videos - Most Talked About Stocks:\")\n",
    "for stock in youtube_results[\"stocks\"][:10]:\n",
    "    print(f\"  {stock['name']}: {stock['mentions']} mentions, avg sentiment: {stock['avg_sentiment']:.3f}\")\n",
    "\n",
    "print(\"\\nYouTube Videos - Most Talked About Companies:\")\n",
    "for company in youtube_results[\"companies\"][:10]:\n",
    "    print(f\"  {company['name']}: {company['mentions']} mentions, avg sentiment: {company['avg_sentiment']:.3f}\")\n",
    "\n",
    "print(\"\\nYouTube Videos - Most Talked About Sectors:\")\n",
    "for sector in youtube_results[\"sectors\"][:10]:\n",
    "    print(f\"  {sector['name']}: {sector['mentions']} mentions, avg sentiment: {sector['avg_sentiment']:.3f}\")\n",
    "\n",
    "# Show top results\n",
    "youtube_results[\"stocks\"][:10], youtube_results[\"companies\"][:10], youtube_results[\"sectors\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93d1703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed provides 15 videos\n",
      "Feed only has 15 videos, but you set MAX_VIDEOS=30\n",
      "   You're already getting all available videos from this RSS feed\n"
     ]
    }
   ],
   "source": [
    "feed = feedparser.parse(YOUTUBE_FEED)\n",
    "print(f\"Feed provides {len(feed.entries)} videos\")\n",
    "\n",
    "# If it's only 15, you're already getting the max from RSS\n",
    "if len(feed.entries) < MAX_VIDEOS:\n",
    "    print(f\"Feed only has {len(feed.entries)} videos, but you set MAX_VIDEOS={MAX_VIDEOS}\")\n",
    "    print(\"   You're already getting all available videos from this RSS feed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875026f1",
   "metadata": {},
   "source": [
    "## Youtube with Google API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ecb0a276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching videos from channel UCrp_UI8XtuYfpiqluWLD7Lw...\n",
      "  Fetched 50 videos so far...\n",
      "  Fetched 100 videos so far...\n",
      "✅ Total videos fetched: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'title': 'Mad Money 01/09/26 | Audio Only',\n",
       "  'video_id': 'Ctwl6H8f9o8',\n",
       "  'url': 'https://www.youtube.com/watch?v=Ctwl6H8f9o8',\n",
       "  'published': '2026-01-10T00:59:12Z',\n",
       "  'published_date': '2026-01-10T00:59:12Z',\n",
       "  'author': 'CNBC Television',\n",
       "  'summary': 'Listen to Jim Cramer’s personal guide through the confusing jungle of Wall Street investing, navigating through opportunities and pitfalls with one goal in mind - to help you make money. \\n\\nFor access to live and exclusive video from CNBC subscribe to CNBC PRO: https://cnb.cx/42d859g\\n\\n» Subscribe to CNBC TV: https://cnb.cx/SubscribeCNBCtelevision\\n» Subscribe to CNBC: https://cnb.cx/SubscribeCNBC\\n» Watch CNBC on the go with CNBC+: https://www.cnbc.com/WatchCNBCPlus\\n\\n\\nTurn to CNBC TV for the latest stock market news and analysis. From market futures to live price updates CNBC is the leader in business news worldwide.\\n\\nConnect with CNBC News Online\\nGet the latest news: http://www.cnbc.com/\\nFollow CNBC on LinkedIn: https://cnb.cx/LinkedInCNBC\\nFollow CNBC News on Instagram: https://cnb.cx/InstagramCNBC\\nFollow CNBC News on Facebook: https://cnb.cx/LikeCNBC\\nFollow CNBC on Threads: https://cnb.cx/threads\\nFollow CNBC News on X: https://cnb.cx/FollowCNBC\\nFollow CNBC on WhatsApp: https://cnb.cx/WhatsAppCNBC\\n\\nhttps://www.cnbc.com/select/best-credit-cards/ \\n\\n#CNBC\\n#CNBCTV',\n",
       "  'transcript_text': None,\n",
       "  'view_count': '4683',\n",
       "  'like_count': '59'},\n",
       " {'title': 'Lightning Round: Buy some Talen Energy here, says Jim Cramer',\n",
       "  'video_id': 'G2RIbBiI_G0',\n",
       "  'url': 'https://www.youtube.com/watch?v=G2RIbBiI_G0',\n",
       "  'published': '2026-01-10T00:35:37Z',\n",
       "  'published_date': '2026-01-10T00:35:37Z',\n",
       "  'author': 'CNBC Television',\n",
       "  'summary': \"'Mad Money' host Jim Cramer weighs in on stocks including: Dynavax Tech, Talen Energy, SailPoint, Radiant Logistics, First Solar, Amprius Tech, and Arm Holdings.\",\n",
       "  'transcript_text': None,\n",
       "  'view_count': '7869',\n",
       "  'like_count': '83'},\n",
       " {'title': \"Jim Cramer looks ahead to next week's market moving moments\",\n",
       "  'video_id': '_ArKhW0jaUI',\n",
       "  'url': 'https://www.youtube.com/watch?v=_ArKhW0jaUI',\n",
       "  'published': '2026-01-10T00:30:27Z',\n",
       "  'published_date': '2026-01-10T00:30:27Z',\n",
       "  'author': 'CNBC Television',\n",
       "  'summary': \"'Mad Money' host Jim Cramer looks ahead to next week's market game plan.\",\n",
       "  'transcript_text': None,\n",
       "  'view_count': '26180',\n",
       "  'like_count': '353'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YouTube Data API Configuration\n",
    "load_dotenv()\n",
    "YOUTUBE_API_KEY = os.getenv(\"API_KEY\")\n",
    "CHANNEL_ID = \"UCrp_UI8XtuYfpiqluWLD7Lw\"  # CNBC channel\n",
    "MAX_VIDEOS = 100\n",
    "\n",
    "def fetch_youtube_videos_with_api(channel_id, api_key, max_results=100):\n",
    "    \"\"\"Fetch YouTube videos using Data API (no transcripts needed)\"\"\"\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    uploads_playlist_id = None\n",
    "    \n",
    "    print(f\"Fetching videos from channel {channel_id}...\")\n",
    "    \n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            # First, get the uploads playlist ID for the channel\n",
    "            if uploads_playlist_id is None:  # Only need to do this once\n",
    "                channel_response = youtube.channels().list(\n",
    "                    part='contentDetails',\n",
    "                    id=channel_id\n",
    "                ).execute()\n",
    "                \n",
    "                if not channel_response.get('items'):\n",
    "                    print(f\"❌ Channel {channel_id} not found\")\n",
    "                    break\n",
    "                \n",
    "                uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "            \n",
    "            # Get videos from uploads playlist\n",
    "            if uploads_playlist_id:\n",
    "                request = youtube.playlistItems().list(\n",
    "                    part='snippet,contentDetails',\n",
    "                    playlistId=uploads_playlist_id,\n",
    "                    maxResults=min(max_results, max_results - len(videos)),\n",
    "                    pageToken=next_page_token\n",
    "                )\n",
    "            else:\n",
    "                # Fallback: search for videos from channel\n",
    "                request = youtube.search().list(\n",
    "                    part='snippet',\n",
    "                    channelId=channel_id,\n",
    "                    type='video',\n",
    "                    maxResults=min(max_results, max_results - len(videos)),\n",
    "                    pageToken=next_page_token,\n",
    "                    order='date'\n",
    "                )\n",
    "            \n",
    "            response = request.execute()\n",
    "            \n",
    "            # Get video IDs\n",
    "            video_ids = []\n",
    "            for item in response['items']:\n",
    "                if 'contentDetails' in item:\n",
    "                    video_ids.append(item['contentDetails']['videoId'])\n",
    "                elif 'id' in item and 'videoId' in item['id']:\n",
    "                    video_ids.append(item['id']['videoId'])\n",
    "            \n",
    "            # Get detailed video information\n",
    "            if video_ids:\n",
    "                video_details = youtube.videos().list(\n",
    "                    part='snippet,statistics',\n",
    "                    id=','.join(video_ids)\n",
    "                ).execute()\n",
    "                \n",
    "                for item in video_details['items']:\n",
    "                    snippet = item['snippet']\n",
    "                    videos.append({\n",
    "                        'title': snippet.get('title', ''),\n",
    "                        'video_id': item['id'],\n",
    "                        'url': f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                        'published': snippet.get('publishedAt', ''),\n",
    "                        'published_date': snippet.get('publishedAt', ''),\n",
    "                        'author': snippet.get('channelTitle', ''),\n",
    "                        'summary': snippet.get('description', ''),  # Full description\n",
    "                        'transcript_text': None,  # No transcript (IP banned)\n",
    "                        'view_count': item['statistics'].get('viewCount', 0),\n",
    "                        'like_count': item['statistics'].get('likeCount', 0),\n",
    "                    })\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "                \n",
    "            print(f\"  Fetched {len(videos)} videos so far...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching videos: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"✅ Total videos fetched: {len(videos)}\")\n",
    "    return videos\n",
    "\n",
    "# Fetch videos using API\n",
    "youtube_videos_api = fetch_youtube_videos_with_api(CHANNEL_ID, YOUTUBE_API_KEY, max_results=MAX_VIDEOS)\n",
    "\n",
    "# Show first few videos\n",
    "youtube_videos_api[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca05a9",
   "metadata": {},
   "source": [
    "# Caching Transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcript fetching with caching and conservative throttling\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "TRANSCRIPT_CACHE_PATH = Path('daily_transcripts.json')\n",
    "\n",
    "def load_transcript_cache(path):\n",
    "    if path.exists():\n",
    "        return json.loads(path.read_text(encoding='utf-8'))\n",
    "    return {}\n",
    "\n",
    "def save_transcript_cache(path, cache):\n",
    "    path.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "\n",
    "def fetch_transcript_with_backoff(video_id, max_retries=3):\n",
    "    delay = 1.0\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "            # Join transcript segments into a single string\n",
    "            return ' '.join([seg.get('text', '') for seg in transcript])\n",
    "        except (TranscriptsDisabled, NoTranscriptFound):\n",
    "            return None\n",
    "        except Exception:\n",
    "            if attempt == max_retries:\n",
    "                return None\n",
    "            time.sleep(delay + random.random())\n",
    "            delay *= 2\n",
    "\n",
    "def attach_transcripts(videos, cache_path=TRANSCRIPT_CACHE_PATH):\n",
    "    # Latest IDs in order; used for pruning\n",
    "    latest_ids = [v.get('video_id') for v in videos if v.get('video_id')]\n",
    "\n",
    "    cache = load_transcript_cache(cache_path)\n",
    "    # Drop anything not in the latest 100\n",
    "    cache = {vid: cache.get(vid) for vid in latest_ids if vid in cache}\n",
    "\n",
    "    for idx, video in enumerate(videos, start=1):\n",
    "        vid = video.get('video_id')\n",
    "        if not vid:\n",
    "            continue\n",
    "        if vid in cache:\n",
    "            video['transcript_text'] = cache[vid]\n",
    "            continue\n",
    "        # Conservative pacing with jitter\n",
    "        time.sleep(0.75 + random.random() * 0.75)\n",
    "        transcript_text = fetch_transcript_with_backoff(vid)\n",
    "        video['transcript_text'] = transcript_text\n",
    "        cache[vid] = transcript_text\n",
    "        if idx % 10 == 0:\n",
    "            save_transcript_cache(cache_path, cache)\n",
    "\n",
    "    # Final prune to ensure cache only has latest 100 IDs\n",
    "    cache = {vid: cache.get(vid) for vid in latest_ids}\n",
    "    save_transcript_cache(cache_path, cache)\n",
    "    return videos\n",
    "\n",
    "# Attach transcripts for the latest 100 videos\n",
    "youtube_videos_api = attach_transcripts(youtube_videos_api)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67404a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SENTENCES = 1\n",
    "result = aggregate_youtube_entities_with_sentiment(youtube_videos_api, window=CONTEXT_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41113c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Technology', 'mentions': 54, 'avg_sentiment': -0.05197525889049342},\n",
       " {'name': 'Finance', 'mentions': 52, 'avg_sentiment': -0.029143351058547314},\n",
       " {'name': 'Automotive', 'mentions': 29, 'avg_sentiment': -0.16226489379488188},\n",
       " {'name': 'Energy', 'mentions': 23, 'avg_sentiment': -0.05071821420089058},\n",
       " {'name': 'Real Estate', 'mentions': 6, 'avg_sentiment': 0.06632298231124878},\n",
       " {'name': 'Healthcare', 'mentions': 5, 'avg_sentiment': -0.026582205295562746},\n",
       " {'name': 'Retail', 'mentions': 3, 'avg_sentiment': 0.13232562939325967},\n",
       " {'name': 'Aerospace', 'mentions': 2, 'avg_sentiment': 0.0},\n",
       " {'name': 'Telecommunications', 'mentions': 1, 'avg_sentiment': 0.0}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['sectors']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ca1fcf",
   "metadata": {},
   "source": [
    "# Check youtube IP ban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2ec1651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=dQw4w9WgXcQ! This is most likely caused by:\n",
      "\n",
      "YouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n",
      "- You have done too many requests and your IP has been blocked by YouTube\n",
      "- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n",
      "\n",
      "Ways to work around this are explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n",
      "\n",
      "\n",
      "If you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!\n",
      "⚠️ Likely IP banned\n"
     ]
    }
   ],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "\n",
    "test_video_id = \"dQw4w9WgXcQ\"  # Rick Roll - should have captions\n",
    "try:\n",
    "    ytt_api = YouTubeTranscriptApi()\n",
    "    transcript = ytt_api.fetch(test_video_id)\n",
    "    print(\"✅ Not banned - can fetch transcripts\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    if \"429\" in str(e) or \"banned\" in str(e).lower() or \"blocked\" in str(e).lower():\n",
    "        print(\"⚠️ Likely IP banned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
