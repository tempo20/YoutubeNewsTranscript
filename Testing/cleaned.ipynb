{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d7d2e4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f6f3f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8632f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\News_Majority\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "import trafilatura\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# import io\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "from youtube_transcript_api.proxies import WebshareProxyConfig, GenericProxyConfig\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbf278",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d868a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Installing spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cd40c",
   "metadata": {},
   "source": [
    "## Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb502e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    tokenizer=\"ProsusAI/finbert\"\n",
    ")\n",
    "\n",
    "def get_text_sentiment_score(text: str, max_chars=512) -> float:\n",
    "    if not text or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    chunks = [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n",
    "    scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        result = sentiment_analyzer(chunk)[0]\n",
    "        label = result[\"label\"].upper()\n",
    "        score = float(result[\"score\"])\n",
    "\n",
    "        if \"POS\" in label:\n",
    "            scores.append(score)\n",
    "        elif \"NEG\" in label:\n",
    "            scores.append(-score)\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return sum(scores) / len(scores) if scores else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b072542",
   "metadata": {},
   "source": [
    "## Entity Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b44139",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SENTENCES = 1\n",
    "TICKER_LIST_PATH = Path(\"tickers.csv\")  # optional: columns ticker,name\n",
    "\n",
    "TICKER_RE = re.compile(r\"(?<![A-Z])\\$?[A-Z]{2,5}(?![A-Z])\")\n",
    "TICKER_STOP = {\n",
    "    \"A\", \"AN\", \"AND\", \"ARE\", \"AS\", \"AT\", \"BE\", \"BUT\", \"BY\", \"CAN\", \"CO\", \"FOR\",\n",
    "    \"FROM\", \"HAS\", \"HAVE\", \"IN\", \"IS\", \"IT\", \"ITS\", \"NOT\", \"OF\", \"ON\", \"OR\",\n",
    "    \"THE\", \"TO\", \"WAS\", \"WERE\", \"WILL\", \"WITH\",\n",
    "}\n",
    "\n",
    "ENTITY_ALIASES = {\n",
    "    # companies\n",
    "    \"meta\": \"META\",\n",
    "    \"facebook\": \"META\",\n",
    "\n",
    "    \"google\": \"GOOGL\",\n",
    "    \"alphabet\": \"GOOGL\",\n",
    "\n",
    "    \"apple\": \"AAPL\",\n",
    "    \"amazon\": \"AMZN\",\n",
    "    \"microsoft\": \"MSFT\",\n",
    "\n",
    "    # institutions\n",
    "    \"fed\": \"Federal Reserve\",\n",
    "    \"federal reserve\": \"Federal Reserve\",\n",
    "    \"doj\": \"Department of Justice\",\n",
    "    \"department of justice\": \"Department of Justice\",\n",
    "    \"supreme court\": \"Supreme Court\",\n",
    "    \"cnn\": \"CNN\",\n",
    "}\n",
    "\n",
    "SECTOR_KEYWORDS = {\n",
    "    \"Technology\": [\"tech\", \"software\", \"technology\", \"cloud\", \"ai\", \"artificial intelligence\",\n",
    "                   \"chip\", \"semiconductor\", \"digital\", \"platform\", \"app\", \"data\", \"cyber\"],\n",
    "    \"Finance\": [\"bank\", \"financial\", \"finance\", \"investment\", \"trading\", \"market\",\n",
    "                \"stock\", \"equity\", \"bond\", \"credit\", \"lending\", \"mortgage\"],\n",
    "    \"Healthcare\": [\"health\", \"medical\", \"pharmaceutical\", \"drug\", \"biotech\", \"hospital\",\n",
    "                    \"treatment\", \"patient\", \"fda\", \"clinical\", \"therapy\"],\n",
    "    \"Energy\": [\"oil\", \"gas\", \"energy\", \"petroleum\", \"renewable\", \"solar\", \"wind\",\n",
    "               \"electric\", \"power\", \"fuel\", \"drilling\", \"crude\"],\n",
    "    \"Retail\": [\"retail\", \"store\", \"shopping\", \"consumer\", \"e-commerce\", \"online shopping\",\n",
    "               \"merchandise\", \"sales\", \"retailer\"],\n",
    "    \"Automotive\": [\"car\", \"automotive\", \"vehicle\", \"auto\", \"truck\", \"electric vehicle\",\n",
    "                   \"ev\", \"manufacturing\", \"tesla\"],\n",
    "    \"Real Estate\": [\"real estate\", \"property\", \"housing\", \"construction\", \"mortgage\",\n",
    "                    \"development\", \"reit\"],\n",
    "    \"Telecommunications\": [\"telecom\", \"communication\", \"wireless\", \"5g\", \"network\", \"internet\"],\n",
    "    \"Aerospace\": [\"aerospace\", \"aircraft\", \"defense\", \"boeing\", \"space\"],\n",
    "    \"Consumer Goods\": [\"consumer goods\", \"packaged goods\", \"cpg\"],\n",
    "}\n",
    "\n",
    "def normalize_company_name(name):\n",
    "    return name.lower().replace(\"inc.\", \"\").replace(\"corp.\", \"\").replace(\"corporation\", \"\").strip()\n",
    "\n",
    "def extract_article_text(url: str) -> str | None:\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    if not downloaded:\n",
    "        return None\n",
    "\n",
    "    text = trafilatura.extract(\n",
    "        downloaded,\n",
    "        include_comments=False,\n",
    "        include_tables=False,\n",
    "        include_formatting=False\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def load_ticker_map(path: Path):\n",
    "    ticker_to_name = {}\n",
    "    name_to_ticker = {}\n",
    "    if not path.exists():\n",
    "        return ticker_to_name, name_to_ticker\n",
    "\n",
    "    with path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            ticker = (row.get(\"ticker\") or \"\").strip().upper()\n",
    "            name = (row.get(\"name\") or \"\").strip()\n",
    "            if not ticker or not name:\n",
    "                continue\n",
    "            ticker_to_name[ticker] = name\n",
    "            name_to_ticker[normalize_company_name(name)] = ticker\n",
    "\n",
    "    return ticker_to_name, name_to_ticker\n",
    "\n",
    "\n",
    "ticker_to_name, name_to_ticker = load_ticker_map(TICKER_LIST_PATH)\n",
    "\n",
    "\n",
    "def fetch_articles(feed_url, max_items=30):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    articles = []\n",
    "    for entry in feed.entries[:max_items]:\n",
    "        text = extract_article_text(entry.link)\n",
    "        if not text:\n",
    "            continue\n",
    "        articles.append({\n",
    "            \"title\": entry.title,\n",
    "            \"url\": entry.link,\n",
    "            \"published\": entry.get(\"published\"),\n",
    "            \"text\": text,\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_tickers(text):\n",
    "    tickers = set()\n",
    "    for m in TICKER_RE.findall(text):\n",
    "        t = m.replace(\"$\", \"\").upper()\n",
    "        if t in TICKER_STOP:\n",
    "            continue\n",
    "        if ticker_to_name and t not in ticker_to_name:\n",
    "            continue\n",
    "        tickers.add(t)\n",
    "\n",
    "    return list(tickers)\n",
    "\n",
    "def get_companies(doc):\n",
    "    mapped = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ != \"ORG\":\n",
    "            continue\n",
    "        key = normalize_company_name(ent.text)\n",
    "        if key in name_to_ticker:\n",
    "            mapped.append(name_to_ticker[key])   # return ticker\n",
    "        else:\n",
    "            mapped.append(ent.text)\n",
    "    return mapped\n",
    "\n",
    "\n",
    "def get_sectors(text_lower):\n",
    "    return [\n",
    "        sector for sector, keywords in SECTOR_KEYWORDS.items()\n",
    "        if any(kw in text_lower for kw in keywords)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ce0b7",
   "metadata": {},
   "source": [
    "## Youtube Data Api Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6f0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "YOUTUBE_API_KEY = os.getenv(\"API_KEY\")\n",
    "CHANNEL_ID = \"UCrp_UI8XtuYfpiqluWLD7Lw\"  # CNBC channel\n",
    "MAX_VIDEOS = 100\n",
    "\n",
    "def fetch_youtube_videos_with_api(channel_id, api_key, max_results=100):\n",
    "    \"\"\"Fetch YouTube videos using Data API (no transcripts needed)\"\"\"\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    uploads_playlist_id = None\n",
    "    \n",
    "    print(f\"Fetching videos from channel {channel_id}...\")\n",
    "    \n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            # First, get the uploads playlist ID for the channel\n",
    "            if uploads_playlist_id is None:  # Only need to do this once\n",
    "                channel_response = youtube.channels().list(\n",
    "                    part='contentDetails',\n",
    "                    id=channel_id\n",
    "                ).execute()\n",
    "                \n",
    "                if not channel_response.get('items'):\n",
    "                    print(f\"❌ Channel {channel_id} not found\")\n",
    "                    break\n",
    "                \n",
    "                uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "            \n",
    "            # Get videos from uploads playlist\n",
    "            if uploads_playlist_id:\n",
    "                request = youtube.playlistItems().list(\n",
    "                    part='snippet,contentDetails',\n",
    "                    playlistId=uploads_playlist_id,\n",
    "                    maxResults=min(max_results, max_results - len(videos)),\n",
    "                    pageToken=next_page_token\n",
    "                )\n",
    "            else:\n",
    "                # Fallback: search for videos from channel\n",
    "                request = youtube.search().list(\n",
    "                    part='snippet',\n",
    "                    channelId=channel_id,\n",
    "                    type='video',\n",
    "                    maxResults=min(max_results, max_results - len(videos)),\n",
    "                    pageToken=next_page_token,\n",
    "                    order='date'\n",
    "                )\n",
    "            \n",
    "            response = request.execute()\n",
    "            \n",
    "            # Get video IDs\n",
    "            video_ids = []\n",
    "            for item in response['items']:\n",
    "                if 'contentDetails' in item:\n",
    "                    video_ids.append(item['contentDetails']['videoId'])\n",
    "                elif 'id' in item and 'videoId' in item['id']:\n",
    "                    video_ids.append(item['id']['videoId'])\n",
    "            \n",
    "            # Get detailed video information\n",
    "            if video_ids:\n",
    "                video_details = youtube.videos().list(\n",
    "                    part='snippet,statistics',\n",
    "                    id=','.join(video_ids)\n",
    "                ).execute()\n",
    "                \n",
    "                for item in video_details['items']:\n",
    "                    snippet = item['snippet']\n",
    "                    videos.append({\n",
    "                        'title': snippet.get('title', ''),\n",
    "                        'video_id': item['id'],\n",
    "                        'url': f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                        'published': snippet.get('publishedAt', ''),\n",
    "                        'published_date': snippet.get('publishedAt', ''),\n",
    "                        'author': snippet.get('channelTitle', ''),\n",
    "                        'summary': snippet.get('description', ''),  # Full description\n",
    "                        'transcript_text': None,  # No transcript (IP banned)\n",
    "                        'view_count': item['statistics'].get('viewCount', 0),\n",
    "                        'like_count': item['statistics'].get('likeCount', 0),\n",
    "                    })\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "                \n",
    "            print(f\"  Fetched {len(videos)} videos so far...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching videos: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"✅ Total videos fetched: {len(videos)}\")\n",
    "    return videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293213d",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfcd3d",
   "metadata": {},
   "source": [
    "## Retrieve latest 100 videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d85a61",
   "metadata": {},
   "source": [
    "## Retrieve and cache Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c27c6d",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "print(\"HTTP_PROXY =\", os.environ.get(\"HTTP_PROXY\"))\n",
    "print(\"HTTPS_PROXY =\", os.environ.get(\"HTTPS_PROXY\"))\n",
    "print(\"Proxies seen by requests:\", requests.utils.get_environ_proxies(\"https://www.youtube.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "# URL-encode credentials\n",
    "proxy_user = quote(os.getenv(\"PROXY_USER\"), safe='')\n",
    "proxy_pass = quote(os.getenv(\"PROXY_PASS\"), safe='')\n",
    "\n",
    "# Set rotating proxy globally\n",
    "proxy_url = f\"http://{proxy_user}:{proxy_pass}@p.webshare.io:80\"\n",
    "os.environ['HTTP_PROXY'] = proxy_url\n",
    "os.environ['HTTPS_PROXY'] = proxy_url\n",
    "\n",
    "# Test rotation by checking your IP multiple times\n",
    "print(\"Testing proxy rotation...\")\n",
    "for i in range(5):\n",
    "    try:\n",
    "        response = requests.get('https://api.ipify.org?format=json', \n",
    "                               proxies={'http': proxy_url, 'https': proxy_url},\n",
    "                               timeout=10)\n",
    "        ip = response.json().get('ip')\n",
    "        print(f\"Request {i+1}: IP = {ip}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Request {i+1}: Error = {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3abb11",
   "metadata": {},
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /sorry/index?continue=https://www.youtube.com/watch%3Fv%3DwN2s5uGh1YQ&q=EgQXX5aRGJj2wMsGIjDgUktWauhYQDmp5GNAVeYpHQU0kM_WB31C4CJx0aoU4SEWWxHDgWoOJjjNLtp6ws4yAnJSWgFD (Caused by ResponseError('too many 429 error responses'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[31mResponseError\u001b[39m: too many 429 error responses",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:942\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    941\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, url)\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:942\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    941\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, url)\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "    \u001b[31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 942 (7 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:942\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    941\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, url)\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:932\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m     retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py:535\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    534\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    537\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /sorry/index?continue=https://www.youtube.com/watch%3Fv%3DwN2s5uGh1YQ&q=EgQXX5aRGJj2wMsGIjDgUktWauhYQDmp5GNAVeYpHQU0kM_WB31C4CJx0aoU4SEWWxHDgWoOJjjNLtp6ws4yAnJSWgFD (Caused by ResponseError('too many 429 error responses'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRetryError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m ytt_api = YouTubeTranscriptApi(\n\u001b[32m      2\u001b[39m     proxy_config=WebshareProxyConfig(\n\u001b[32m      3\u001b[39m         proxy_username = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mPROXY_USER\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      4\u001b[39m         proxy_password = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mPROXY_PASS\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m     )\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m transcript = \u001b[43mytt_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwN2s5uGh1YQ\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m transcript\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_api.py:71\u001b[39m, in \u001b[36mYouTubeTranscriptApi.fetch\u001b[39m\u001b[34m(self, video_id, languages, preserve_formatting)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch\u001b[39m(\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     53\u001b[39m     video_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     54\u001b[39m     languages: Iterable[\u001b[38;5;28mstr\u001b[39m] = (\u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m,),\n\u001b[32m     55\u001b[39m     preserve_formatting: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     56\u001b[39m ) -> FetchedTranscript:\n\u001b[32m     57\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m    Retrieves the transcript for a single video. This is just a shortcut for\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    calling:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \u001b[33;03m    :param preserve_formatting: whether to keep select HTML text formatting\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m         .find_transcript(languages)\n\u001b[32m     73\u001b[39m         .fetch(preserve_formatting=preserve_formatting)\n\u001b[32m     74\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_api.py:127\u001b[39m, in \u001b[36mYouTubeTranscriptApi.list\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist\u001b[39m(\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     78\u001b[39m     video_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     79\u001b[39m ) -> TranscriptList:\n\u001b[32m     80\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m    Retrieves the list of transcripts which are available for a given video. It\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    returns a `TranscriptList` object which is iterable and provides methods to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03m        Make sure that this is the actual ID, NOT the full URL to the video!\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:356\u001b[39m, in \u001b[36mTranscriptListFetcher.fetch\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> TranscriptList:\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TranscriptList.build(\n\u001b[32m    354\u001b[39m         \u001b[38;5;28mself\u001b[39m._http_client,\n\u001b[32m    355\u001b[39m         video_id,\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_captions_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    357\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:372\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_captions_json\u001b[39m\u001b[34m(self, video_id, try_number)\u001b[39m\n\u001b[32m    366\u001b[39m retries = (\n\u001b[32m    367\u001b[39m     \u001b[32m0\u001b[39m\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._proxy_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._proxy_config.retries_when_blocked\n\u001b[32m    370\u001b[39m )\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m try_number + \u001b[32m1\u001b[39m < retries:\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_captions_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_number\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception.with_proxy_config(\u001b[38;5;28mself\u001b[39m._proxy_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:372\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_captions_json\u001b[39m\u001b[34m(self, video_id, try_number)\u001b[39m\n\u001b[32m    366\u001b[39m retries = (\n\u001b[32m    367\u001b[39m     \u001b[32m0\u001b[39m\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._proxy_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._proxy_config.retries_when_blocked\n\u001b[32m    370\u001b[39m )\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m try_number + \u001b[32m1\u001b[39m < retries:\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_captions_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_number\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_number\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception.with_proxy_config(\u001b[38;5;28mself\u001b[39m._proxy_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:361\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_captions_json\u001b[39m\u001b[34m(self, video_id, try_number)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_captions_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m, try_number: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m) -> Dict:\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         html = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_video_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m         api_key = \u001b[38;5;28mself\u001b[39m._extract_innertube_api_key(html, video_id)\n\u001b[32m    363\u001b[39m         innertube_data = \u001b[38;5;28mself\u001b[39m._fetch_innertube_data(video_id, api_key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:433\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_video_html\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_video_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     html = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33maction=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://consent.youtube.com/s\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m html:\n\u001b[32m    435\u001b[39m         \u001b[38;5;28mself\u001b[39m._create_consent_cookie(html, video_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:442\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_html\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_http_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWATCH_URL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m unescape(_raise_http_errors(response, video_id).text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:724\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_redirects:\n\u001b[32m    722\u001b[39m     \u001b[38;5;66;03m# Redirect resolving generator.\u001b[39;00m\n\u001b[32m    723\u001b[39m     gen = \u001b[38;5;28mself\u001b[39m.resolve_redirects(r, request, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m     history = \u001b[43m[\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    726\u001b[39m     history = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:265\u001b[39m, in \u001b[36mSessionRedirectMixin.resolve_redirects\u001b[39m\u001b[34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[39m\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m req\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madapter_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     extract_cookies_to_jar(\u001b[38;5;28mself\u001b[39m.cookies, prepared_request, resp.raw)\n\u001b[32m    278\u001b[39m     \u001b[38;5;66;03m# extract redirect url, if any, for the next loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\adapters.py:668\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    665\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request=request)\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ResponseError):\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request=request)\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _ProxyError):\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request=request)\n",
      "\u001b[31mRetryError\u001b[39m: HTTPSConnectionPool(host='www.google.com', port=443): Max retries exceeded with url: /sorry/index?continue=https://www.youtube.com/watch%3Fv%3DwN2s5uGh1YQ&q=EgQXX5aRGJj2wMsGIjDgUktWauhYQDmp5GNAVeYpHQU0kM_WB31C4CJx0aoU4SEWWxHDgWoOJjjNLtp6ws4yAnJSWgFD (Caused by ResponseError('too many 429 error responses'))"
     ]
    }
   ],
   "source": [
    "ytt_api = YouTubeTranscriptApi(\n",
    "    proxy_config=WebshareProxyConfig(\n",
    "        proxy_username = os.getenv(\"PROXY_USER\"),\n",
    "        proxy_password = os.getenv(\"PROXY_PASS\"),\n",
    "    )\n",
    ")\n",
    "transcript = ytt_api.fetch('wN2s5uGh1YQ')\n",
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c0f03",
   "metadata": {},
   "source": [
    "### Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24c444da",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPT_CACHE_PATH = Path('daily_transcripts.json')\n",
    "MAX_CACHE_SIZE = 100  # Maximum number of videos to keep in cache\n",
    "\n",
    "def load_transcript_cache(path):\n",
    "    if path.exists():\n",
    "        try:\n",
    "            content = path.read_text(encoding='utf-8')\n",
    "            if content.strip():\n",
    "                return json.loads(content)\n",
    "            else:\n",
    "                print(\"⚠️  Cache file is empty, starting fresh\")\n",
    "                return {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"⚠️  Cache file is corrupted: {e}\")\n",
    "            print(\"   Starting with fresh cache\")\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_transcript_cache(path, cache):\n",
    "    try:\n",
    "        path.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Failed to save cache: {e}\")\n",
    "\n",
    "def fetch_transcript_with_backoff(video_id, max_retries=10):\n",
    "    \"\"\"\n",
    "    Fetch transcript with exponential backoff and jitter.\n",
    "    No proxy - relies on longer delays to avoid rate limits.\n",
    "    \"\"\"\n",
    "    base_delay = 5.0  # Longer initial delay without proxy\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            # Add random delay before each request (rate limit avoidance)\n",
    "            jitter = random.uniform(2, 5)\n",
    "            if attempt > 1:\n",
    "                time.sleep(jitter)\n",
    "            ytt_api = YouTubeTranscriptApi(\n",
    "                proxy_config=WebshareProxyConfig(\n",
    "                    proxy_username = os.getenv(\"PROXY_USER\"),\n",
    "                    proxy_password = os.getenv(\"PROXY_PASS\"),\n",
    "                )\n",
    "            )\n",
    "            transcript = ytt_api.fetch(video_id)\n",
    "            return ' '.join([seg.text for seg in transcript])\n",
    "            \n",
    "        except (TranscriptsDisabled, NoTranscriptFound):\n",
    "            # These are not rate limits, just unavailable transcripts\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            \n",
    "            # Check for rate limit indicators\n",
    "            if any(indicator in error_msg for indicator in ['429', 'too many requests', 'rate limit', 'forbidden', '403']):\n",
    "                wait_time = base_delay * (2 ** (attempt - 1)) + random.uniform(5, 15)\n",
    "                print(f\"⚠️  Rate limit detected (attempt {attempt}/{max_retries})\")\n",
    "                print(f\"   Waiting {wait_time:.1f}s before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            # Other errors\n",
    "            error_type = type(e).__name__\n",
    "            print(f\"Attempt {attempt} failed for {video_id}: {error_type}: {str(e)[:100]}\")\n",
    "            # Skip immediately for unavailable/unplayable videos (no point retrying)\n",
    "            if any(skip in error_msg or skip in error_type.lower() for skip in \n",
    "                   ['unavailable', 'unplayable', 'private', 'deleted']):\n",
    "                print(f\"  ⏭️  Skipping (video unavailable)\")\n",
    "                return None\n",
    "            if attempt == max_retries:\n",
    "                return None\n",
    "            \n",
    "            # Exponential backoff for other errors\n",
    "            wait_time = base_delay * (1.5 ** attempt) + random.uniform(1, 3)\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def attach_transcripts(videos, cache_path=TRANSCRIPT_CACHE_PATH, max_cache_size=MAX_CACHE_SIZE, delay_between_requests=3.0):\n",
    "    \"\"\"\n",
    "    Attach transcripts to videos with aggressive rate limit avoidance.\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video dictionaries\n",
    "        cache_path: Path to cache file\n",
    "        max_cache_size: Maximum number of videos to keep in cache\n",
    "        delay_between_requests: Base delay between requests in seconds (default: 3.0)\n",
    "    \"\"\"\n",
    "    latest_ids = [v.get('video_id') for v in videos if v.get('video_id')]\n",
    "    total_videos = len(latest_ids)\n",
    "    print(f\"\\n📝 Processing {total_videos} videos for transcripts...\\n\")\n",
    "    print(f\"⏱️  Using delays to avoid rate limits (no proxy)\\n\")\n",
    "\n",
    "    # Load cache\n",
    "    cache = load_transcript_cache(cache_path)\n",
    "    old_cache_size = len(cache)\n",
    "    \n",
    "    # Create ordered list: newest videos first\n",
    "    all_video_ids = latest_ids.copy()\n",
    "    \n",
    "    # Add old cached videos that aren't in the new list\n",
    "    for old_vid in cache.keys():\n",
    "        if old_vid not in all_video_ids:\n",
    "            all_video_ids.append(old_vid)\n",
    "    \n",
    "    # Keep only the newest MAX_CACHE_SIZE videos\n",
    "    videos_to_keep = all_video_ids[:max_cache_size]\n",
    "    \n",
    "    # Filter cache\n",
    "    filtered_cache = {vid: cache[vid] for vid in videos_to_keep if vid in cache}\n",
    "    removed_count = old_cache_size - len(filtered_cache)\n",
    "    \n",
    "    print(f\"📦 Cache status: {old_cache_size} total → keeping {len(filtered_cache)} (removed {removed_count} oldest)\\n\")\n",
    "    \n",
    "    cache = filtered_cache\n",
    "\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    cached_count = 0\n",
    "    actual_idx = 0\n",
    "\n",
    "    for idx, video in enumerate(videos, start=1):\n",
    "        vid = video.get('video_id')\n",
    "        if not vid:\n",
    "            continue\n",
    "        \n",
    "        actual_idx = idx\n",
    "        \n",
    "        # Check cache first\n",
    "        if vid in cache:\n",
    "            video['transcript_text'] = cache[vid]\n",
    "            cached_count += 1\n",
    "            print(f\"[{idx}/{total_videos}] ✓ Cached: {vid} - {video.get('title', 'N/A')[:50]}\")\n",
    "            if cache[vid]:\n",
    "                print(f\"  Preview: {cache[vid][:150]}...\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Add delay between requests to avoid rate limits\n",
    "        delay = delay_between_requests + random.uniform(1, 3)\n",
    "        print(f\"[{idx}/{total_videos}] Fetching: {vid} (waiting {delay:.1f}s)...\")\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        # Fetch transcript\n",
    "        try:\n",
    "            transcript_text = fetch_transcript_with_backoff(vid)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Unexpected error: {e}\")\n",
    "            transcript_text = None\n",
    "        \n",
    "        video['transcript_text'] = transcript_text\n",
    "        cache[vid] = transcript_text\n",
    "        \n",
    "        if transcript_text:\n",
    "            success_count += 1\n",
    "            print(f\"✓ Success: {video.get('title', 'N/A')[:50]}\")\n",
    "            print(f\"  Preview: {transcript_text[:150]}...\\n\")\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            print(f\"✗ Failed/No transcript: {video.get('title', 'N/A')[:50]}\\n\")\n",
    "        \n",
    "        # Save cache periodically\n",
    "        if idx % 10 == 0:\n",
    "            save_transcript_cache(cache_path, cache)\n",
    "            print(f\"  💾 Cache saved at {idx} videos\\n\")\n",
    "\n",
    "    # Final save\n",
    "    save_transcript_cache(cache_path, cache)\n",
    "    \n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"  ✓ Successfully fetched: {success_count}\")\n",
    "    print(f\"  ✓ From cache: {cached_count}\")\n",
    "    print(f\"  ✗ Failed/No transcript: {failed_count}\")\n",
    "    print(f\"  Total processed: {actual_idx}/{total_videos}\")\n",
    "    print(f\"  📦 Final cache size: {len(cache)}/{max_cache_size}\")\n",
    "    print()\n",
    "    \n",
    "    return videos\n",
    "\n",
    "def refresh_transcripts_in_dict(videos, cache_path=Path('daily_transcripts.json')):\n",
    "    \"\"\"Refresh transcript data from cache file\"\"\"\n",
    "    if not cache_path.exists():\n",
    "        return videos\n",
    "    cache = json.loads(cache_path.read_text(encoding='utf-8'))\n",
    "    updated = 0\n",
    "    for video in videos:\n",
    "        vid = video.get('video_id')\n",
    "        if not vid:\n",
    "            continue\n",
    "        cached_value = cache.get(vid)\n",
    "        if cached_value is not None:\n",
    "            if video.get('transcript_text') != cached_value:\n",
    "                video['transcript_text'] = cached_value\n",
    "                updated += 1\n",
    "    print(f'Overwrote {updated} transcripts from cache')\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8575db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching videos from channel UCrp_UI8XtuYfpiqluWLD7Lw...\n",
      "  Fetched 50 videos so far...\n",
      "  Fetched 100 videos so far...\n",
      "✅ Total videos fetched: 100\n"
     ]
    }
   ],
   "source": [
    "youtube_videos_api = fetch_youtube_videos_with_api(CHANNEL_ID, YOUTUBE_API_KEY, max_results=MAX_VIDEOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d570f82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Processing 100 videos for transcripts...\n",
      "\n",
      "⏱️  Using delays to avoid rate limits (no proxy)\n",
      "\n",
      "📦 Cache status: 40 total → keeping 40 (removed 0 oldest)\n",
      "\n",
      "[1/100] ✓ Cached: UIEMxGany2k - EXCLUSIVE: CNBC's Joe Kernen interviews President \n",
      "[2/100] ✓ Cached: Ia0Ev2Xuj0g - LIVE: Trump speaks at the World Economic Forum in \n",
      "[3/100] ✓ Cached: 4YJmAZY8V4w - Lightning Round: Shell is 'just an ok oil company'\n",
      "  Preview: >> OF COURSE MY STEP PLAY. ITSELF AND THEN THE LIGHTNING ROUND IS OVER. ARE YOU READY, SKI DADDY? THE LIGHTNING ROUND. LET'S START WITH SKIP IN CALIFO...\n",
      "\n",
      "[4/100] ✓ Cached: tKgYSl5KSq0 - Mad Money 01/20/26 | Audio Only\n",
      "  Preview: Hey, I'm Kramer. Welcome to Mad Money. Welcome to Crayer. Other friends, I'm just trying to save a little bit of money. My job is not just to educate,...\n",
      "\n",
      "[5/100] ✓ Cached: _LVNB2Hc5t0 - Jim Cramer sounds the alarm on speculation and see\n",
      "  Preview: Money. Wel MONEY. WELCOME TO CRAMER PICKER FRIENDS I'M JUST TRYING TO SAVE YOU A LITTLE BIT OF MONEY. MY JOB IS NOT JUST TO EDUCATE BUT TO EXPLAIN HOW...\n",
      "\n",
      "[6/100] ✓ Cached: -hK__qbJBYQ - Jim Cramer urges investors to take profits in spec\n",
      "  Preview: REBOUND ONCE THE PRESIDENT ROLLED BACK MOST OF THEM. ALL DAY, AS THE MARKET ADJUSTED TO THE POSSIBILITY THAT SOMEONE BLINKS ON GREENLAND NEXT 72 HOURS...\n",
      "\n",
      "[7/100] ✓ Cached: 0qZLuz25_w4 - Final Trade: DAL, NFLX, INTC, RIG\n",
      "  Preview: THIS TUESDAY. LET'S GO AROUND THE HORN TIM SEYMOUR. >> YEAH, WE FLEW AROUND A LOT OF TOPICS TONIGHT ON THIS DESK. BUT THE TOPIC OF AIRLINES, I THINK I...\n",
      "\n",
      "[8/100] ✓ Cached: ImeJSU0_MWU - Watch CNBC's full interview with United Airlines C\n",
      "  Preview: SAYING 2026 IS OFF TO A STRONG START. FOR MORE ON THE RESULTS, CNBC'S PHIL LEBEAU JOINS US WITH UNITED CEO SCOTT KIRBY. >> PHIL. >> THANK YOU, MELISSA...\n",
      "\n",
      "[9/100] ✓ Cached: 00RnQASrPzs - LightShed's Greenfield: Paramount not likely to be\n",
      "  Preview: >> BE CAREFUL. I LIKE NEW YEAR'S. RESOLUTIONS ARE OUT. I THINK THERE'S PROBABLY MORE. MORE SQUATS IN 26. >> ESPECIALLY IN JANUARY. >> IT'S SQUATS. >> ...\n",
      "\n",
      "[10/100] ✓ Cached: bqGNtghu6E0 - Citi's Kaiser: Global bond yields are a risk that \n",
      "  Preview: DAY. ONE DAY DOES NOT MAKE A TREND. WE DID HAVE, YOU KNOW, A 6% OR SO SELLOFF THAT CAME I THINK IN NOVEMBER. AND THEN WHERE WE WERE A FEW WEEKS LATER,...\n",
      "\n",
      "[11/100] ✓ Cached: nda8spdTEX0 - 'Fast Money' traders talk stock and bond markets' \n",
      "  Preview: AND DO NOT MISS JOE KERNAN'S INTERVIEW WITH PRESIDENT TRUMP THAT'S LIVE FROM THE WORLD ECONOMIC FORUM IN DAVOS TOMORROW 1 P.M. EASTERN TIME RIGHT HERE...\n",
      "\n",
      "[12/100] ✓ Cached: yPhU0qHVEi0 - Political headlines unlikely to change the positiv\n",
      "  Preview: EAGER TO GET AN UPDATE ON WHETHER OUR NATION IS GETTING CLOSER TO MANUFACTURING HIGH PERFORMING CHIPS. MIKE. >> ALL RIGHT. THANK YOU. DESPITE TODAY'S ...\n",
      "\n",
      "[13/100] ✓ Cached: 8gcrpS3JHf0 - Netflix is still overvalued at these levels, says \n",
      "  Preview: NORMALIZED. AND I'M WONDERING IF THAT TAKES SOME OF THE ENERGY OUT OF THE KIND OF SPECULATIVE PART. >> OF IT. >> THERE'S NO FRICTION TO GETTING IN AND...\n",
      "\n",
      "[14/100] ✓ Cached: 4FGlogzC0R4 - Interactive Brokers CEO Thomas Peterffy talks quar\n",
      "  Preview: THAT TO THE ESTIMATE, BUT IT IS AN IMPROVEMENT FROM LAST YEAR. ITS COMMISSION REVENUE GREW ON HIGHER TRADING VOLUMES, AND NET INTEREST INCOME ALSO GRE...\n",
      "\n",
      "[15/100] ✓ Cached: ZZOEe5Iimcc - 'Sobering' the U.S. Treasury is not playing 'safe \n",
      "  Preview: Our next guest says today's market moves indicate that the sell America trade is back as investors try to minimize exposure to what he calls a volatil...\n",
      "\n",
      "[16/100] ✓ Cached: 4co7lXldTZQ - Lutnick: Interest rates are 'hundreds of basis poi\n",
      "  Preview: The fact is our rates are much, much too high. Hundreds of basis points too high. What used to happen in America is when we would grow, they'd raise r...\n",
      "\n",
      "[17/100] ✓ Cached: fa7mIBJQZkM - Pres. Trump on how far he is willing to go to acqu\n",
      "  Preview: HOUR. NETFLIX AND UNITED AIRLINES. >> OUR REPORTERS ALL OVER TODAY'S MOVE. STOCKS, BONDS AND COMMODITIES. AND ON THOSE AFTER HOURS EARNINGS REPORTS. B...\n",
      "\n",
      "[18/100] ✓ Cached: hKpE5bIZvCI - Netflix shares drop despite better-than-expected Q\n",
      "[19/100] ✓ Cached: yJtQcfzTxE4 - Pfizer CEO Albert Bourla: U.S. drug prices are com\n",
      "  Preview: The US price is going down is happening now. The US price is going up in Europe. We will see. >> Why? Because >> because we don't have an agreement wi...\n",
      "\n",
      "[20/100] ✓ Cached: lIJk9D1LLOM - UBS CEO on market volatility: Don't see any path o\n",
      "  Preview: The real issues that that's in my point of view is that there is a limit on how many things you can put on the table without and eventually one day on...\n",
      "\n",
      "[21/100] ✓ Cached: G1xcvba3xq0 - Trump speaks at White House press briefing — 1/20/\n",
      "  Preview: personal truck. >> I'm sure. Yes. 100% polyester%. That was good. Oh yeah. Basically Sorry. Hey, how are you? Yeah. >> Yes. You're These are off the r...\n",
      "\n",
      "[22/100] ✓ Cached: 5CsydJqO7yI - Consumer is really strong after the best holiday s\n",
      "  Preview: THE DOW STILL DOWN ABOUT 900 POINTS. DISCRETIONARY IS THE WORST SECTOR OF THE DAY. BIG QUESTION. NOW HOW LONG CAN THE CONSUMER KEEP SPENDING. IT MIGHT...\n",
      "\n",
      "[23/100] ✓ Cached: g0bULp8GW14 - Gap CEO: We have to do 'a lot of work' to mitigate\n",
      "[24/100] ✓ Cached: ro-K_oZIDLo - Netflix is already the clear winner in streaming, \n",
      "[25/100] ✓ Cached: 1PlPZ9BWcQU - Micron CEO on navigating a tight memory market\n",
      "  Preview: Let me just point out that it's not just about the shortage of memory. It is also about the value of memory. You know AI is now everywhere from data c...\n",
      "\n",
      "[26/100] ✓ Cached: n3GW0g1aAZQ - Battery stocks have more room to run despite recen\n",
      "[27/100] ✓ Cached: FP9fgRCAvl8 - Stocks will present a buying opportunity amid sell\n",
      "  Preview: LIKE FOR MOST OF THE COUNTRIES THAT AGREED TO INVEST IN US AND FULFILL THE PURPOSE OF THE ADMINISTRATION. >> SO WHAT'S THE RIGHT MOVE FOR INVESTORS HE...\n",
      "\n",
      "[28/100] ✓ Cached: y_17SL9VShU - Europe would respond vigorously if U.S. went throu\n",
      "  Preview: LEGALITY OF THE PRESIDENT'S TARIFFS DIDN'T COME TODAY. THE PRESIDENT THOUGH IS VOWING TO FOLLOW THROUGH ON HIS THREAT TO IMPOSE TARIFFS ON EUROPEAN CO...\n",
      "\n",
      "[29/100] ✓ Cached: T06z_aATEM4 - Global bonds sell off\n",
      "[30/100] ✓ Cached: RzAFNFUopeo - Citi CEO Jane Fraser: Credit card caps would have \n",
      "  Preview: I think the the president is right to focus on affordability because there are some of the consumers out there that it is tough at the moment and that...\n",
      "\n",
      "[31/100] ✓ Cached: EAzJ_bokqWc - NYSE President Lynn Martin discusses the 2026 IPO \n",
      "[32/100] ✓ Cached: rZlMOIQFgwk - Salesforce CEO Marc Benioff on AI regulation: 'It \n",
      "[33/100] ✓ Cached: JaZ5cKEYkDs - Carlyle's David Rubenstein on Trump's threats to t\n",
      "  Preview: But I suspect there'll be some negotiations on really things relating to Greenland. Right now we have the right to do militarily, I think, anything we...\n",
      "\n",
      "[34/100] ✓ Cached: NBuaO-HB42M - Japan's economic issues won't become a major globa\n",
      "  Preview: HONOR HIS ONE YEAR ANNIVERSARY SINCE INAUGURATION AT THE WHITE HOUSE PRESS BRIEFING. THE MOMENT IT VEERS BACK INTO ANYTHING HAVING TO DO WITH TARIFFS,...\n",
      "\n",
      "[35/100] ✓ Cached: UGD3UhxdtvI - Greenland pursuit has to be on shortlist of really\n",
      "  Preview: ACHIEVEMENTS, PERHAPS LESS SO ABOUT THE NEWS THAT IS SHAKING THE MARKETS TODAY, WHICH IS HIS THREATS OF TARIFFS AGAINST EUROPE IF THEY DON'T COMPLY WI...\n",
      "\n",
      "[36/100] ✓ Cached: Ef_BM1B_ZGA - Volatility index shows market woes around tariffs \n",
      "  Preview: DOWN AROUND A PERCENT AND A HALF OR MORE. BIG MOVE IN YIELDS. BIG MOVE IN THE VIX. ALSO THE VOLATILITY INDEX WHICH OUR NEXT GUEST SAYS IS OVERDONE. LE...\n",
      "\n",
      "[37/100] ✓ Cached: uGFV91BX0ho - Market doesn't believe Trump has moved away from t\n",
      "[38/100] ✓ Cached: vnxxHDcpUEM - Coca-Cola CEO on the strength of the consumer\n",
      "  Preview: There's pretty strong resilience out there globally. There are a few emerging markets that have been uh that have been weaker in the back half of last...\n",
      "\n",
      "[39/100] ✓ Cached: Nb5SWU6HL8g - Ray Dalio warns 'capital wars' could follow Trump'\n",
      "  Preview: On the other side of trade deficits and trade wars are capital and capital wars. If you take the conflicts, you can't ignore the possibility that capi...\n",
      "\n",
      "[40/100] ✓ Cached: 8es9nwuMWiw - How to trade around the market volatility\n",
      "  Preview: THAT SEEMED POISED TO KEEP RALLYING. JOINING ME FOR THE HOUR TODAY JOE TERRANOVA LIZ THOMAS BRIAN BELSKY, JOSH BROWN. WE WILL GO TO THE MARKETS GET YO...\n",
      "\n",
      "[41/100] Fetching: 1qQiYGa_RQA (waiting 7.8s)...\n",
      "Attempt 1 failed for 1qQiYGa_RQA: RequestBlocked: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=1qQiYGa_RQA! This is \n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 24.8s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 26.4s before retry...\n",
      "⚠️  Rate limit detected (attempt 4/10)\n",
      "   Waiting 53.0s before retry...\n",
      "✓ Success: Calls of the Day: ServiceTitan, Shake Shack, Toast\n",
      "  Preview: >> LET'S RUN THROUGH SOME CALLS OF THE DAY SERVICE TITAN. BIG UPGRADE TODAY AT MORGAN STANLEY TO OVERWEIGHT FROM EQUAL WEIGHT. NAMED A TOP PICK, THE T...\n",
      "\n",
      "[42/100] Fetching: 6tibOd_S97A (waiting 7.6s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 17.2s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 17.3s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 27.1s before retry...\n",
      "⚠️  Rate limit detected (attempt 4/10)\n",
      "   Waiting 51.3s before retry...\n",
      "⚠️  Rate limit detected (attempt 5/10)\n",
      "   Waiting 89.4s before retry...\n",
      "⚠️  Rate limit detected (attempt 6/10)\n",
      "   Waiting 166.8s before retry...\n",
      "✓ Success: Josh Brown's \"best stocks in the market:\" Utilitie\n",
      "  Preview: the collar. Stay stiff all day. Get >> IT IS TIME FOR BEST STOCKS IN THE MARKET. ACCORDING TO JOSH BROWN, THE SPOTLIGHT TODAY SHINING BRIGHTLY ON THE ...\n",
      "\n",
      "[43/100] Fetching: Tt6pv6CqRR4 (waiting 7.1s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 15.8s before retry...\n",
      "✓ Success: Final Trades: Toast, Southern Company, the IGV and\n",
      "  Preview:  And this is the power of us. >> WE'LL WRAP UP THE TRADING SESSION. ADAM. PARKER, CAMERON. DAWSON. YOUNG, YUMA. JASON. SNIPE FOR NETFLIX, ALEX KANTROW...\n",
      "\n",
      "[44/100] Fetching: eLeSTWJ1XzI (waiting 7.0s)...\n",
      "✓ Success: Trade Tracker: Joe Terranova buys Twilio\n",
      "  Preview: IS TOMORROW. AND YOU WILL HEAR SOME OF THAT INTERVIEW. OF COURSE, THROUGHOUT MOST OF THE AFTERNOON TOMORROW. LIZ MENTIONED, LOOK AT SOFTWARE. I'M TRYI...\n",
      "\n",
      "[45/100] Fetching: cmPvRfK1JKg (waiting 7.4s)...\n",
      "✓ Success: Squawk Pod: Davos 2026: David Beckham & Brian Moyn\n",
      "  Preview: Would you put this on par with what we heard last April with the tariffs? No. >> That was a a bigger shock to the system. >> Bank of America CEO Brian...\n",
      "\n",
      "[46/100] Fetching: cFVWGmcxWXU (waiting 7.1s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 12.8s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 15.8s before retry...\n",
      "Attempt 3 failed for cFVWGmcxWXU: VideoUnplayable: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=cFVWGmcxWXU! This is \n",
      "  ⏭️  Skipping (video unavailable)\n",
      "✗ Failed/No transcript: Pfizer CEO Albert Bourla: U.S. drug prices are com\n",
      "\n",
      "[47/100] Fetching: 2-gaGY90W0A (waiting 7.4s)...\n",
      "✓ Success: AI adoption in focus at Davos\n",
      "  Preview: NUMBERS OVER THE WEEKEND THAT TRY TO PUT SOME AI BUBBLE CONCERNS TO REST. DEIRDRE BOSA HAS MORE IN TODAY'S TECH CHECK D. >> HEY GOOD MORNING DAVID. SO...\n",
      "\n",
      "[48/100] Fetching: yYuezNl5FOM (waiting 6.7s)...\n",
      "Attempt 1 failed for yYuezNl5FOM: VideoUnplayable: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=yYuezNl5FOM! This is \n",
      "  ⏭️  Skipping (video unavailable)\n",
      "✗ Failed/No transcript: Water.org CEO on launch of 'Get Blue' initiative: \n",
      "\n",
      "[49/100] Fetching: dr-Y8vE420A (waiting 6.6s)...\n",
      "✓ Success: Watch CNBC’s full interview with U.S. Commerce Sec\n",
      "  Preview: THE MARKET ON EDGE AS INVESTORS TRY TO MAKE SENSE OF RE ESCALATING TARIFF THREATS. JOINING ME HERE FIRST ON CNBC AT THE WORLD ECONOMIC FORUM IS COMMER...\n",
      "\n",
      "[50/100] Fetching: 96bFH1de38c (waiting 6.0s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 11.0s before retry...\n",
      "✓ Success: Amazon CEO Jassy says Trump's tariffs have started\n",
      "  Preview: We'll have to see what happens on tariffs. You know, we we did a lot of pre- buying in the early part of 2025 um to enable us to try and keep prices a...\n",
      "\n",
      "  💾 Cache saved at 50 videos\n",
      "\n",
      "[51/100] Fetching: CrKDG-ztY30 (waiting 7.6s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 10.6s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 21.9s before retry...\n",
      "✓ Success: Gap CEO Richard Dickson on tariffs: Don't see any \n",
      "  Preview: >> But worse things. FOR A FREE QUOTE. >> WELCOME BACK TO MONEY MOVERS. ONE OF THE THEMES HERE IN DAVOS THIS YEAR IS BUILDING PROSPERITY AROUND THE WO...\n",
      "\n",
      "[52/100] Fetching: Fu5hvbpYagw (waiting 6.5s)...\n",
      "✓ Success: Market still in 'pretty good standing' despite geo\n",
      "  Preview: COULD COULD BE OVER SEVERAL DAYS. THEY KIND OF SPRING THIS ON US WITH A COUPLE OF DAYS NOTICE AND SAY, YOU KNOW, WATCH THIS SPACE. AND THEN WE WATCH T...\n",
      "\n",
      "[53/100] Fetching: EFkkwD-ZwFQ (waiting 6.5s)...\n",
      "✓ Success: CoreWeave CEO on when AI investment will pay off\n",
      "  Preview: So I I I think in five or 10 years, you're going to be in a world where artificial intelligence is embedded into absolutely everything we do. And it w...\n",
      "\n",
      "[54/100] Fetching: h_6usBEUwWg (waiting 6.7s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 16.5s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 15.4s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 27.3s before retry...\n",
      "⚠️  Rate limit detected (attempt 4/10)\n",
      "   Waiting 49.4s before retry...\n",
      "✓ Success: Micron CEO Sanjay Mehrotra: Focused on increasing \n",
      "  Preview: THE SELL OFF TODAY, ADDING TO A SOLID START TO THE YEAR AFTER A STRONG 2025. THE COMPANY NAVIGATING A TIGHT MEMORY MARKET AND BREAKING GROUND ON A MAS...\n",
      "\n",
      "[55/100] Fetching: IGGQzcYmDX0 (waiting 7.2s)...\n",
      "✓ Success: Arm CEO Rene Haas on AI's energy scaling challenge\n",
      "  Preview: One of the problems that we have particularly in the US is not so much energy because in the US we actually have a fair bit of energy >> but it's actu...\n",
      "\n",
      "[56/100] Fetching: pmKTgEur6LU (waiting 6.5s)...\n",
      "✓ Success: Salesforce CEO Marc Benioff on AI regulation: It c\n",
      "  Preview: WHAT IS THE NEXT BEST INFRASTRUCTURE, WHAT IS THE NEXT BEST, YOU KNOW, COMPONENT. AND OUR JOB IS TO BRING IT ALL TOGETHER AND THEN DELIVER VALUE TO TH...\n",
      "\n",
      "[57/100] Fetching: i2D5G8emoDM (waiting 7.9s)...\n",
      "✓ Success: Citigroup CEO on market sell-off: Confident that a\n",
      "  Preview: PRESIDENT. SARAH. >> OKAY. DAVID. WELL, THE FINANCIALS ARE JOINING THE SELL OFF THIS MORNING AS WELL AS PRESIDENT TRUMP'S GREENLAND TARIFF THREATS RAT...\n",
      "\n",
      "[58/100] Fetching: G5ZK8ZYLINc (waiting 6.5s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 14.6s before retry...\n",
      "✓ Success: Jefferies' David Zervos: Trump's negotiating tacti\n",
      "  Preview: >> WELCOME BACK. STOCKS GETTING HIT HARD THIS MORNING AS PRESIDENT TRUMP THREATENS NEW TARIFFS ON EUROPEAN COUNTRIES OPPOSED TO A SALE OF GREENLAND TO...\n",
      "\n",
      "[59/100] Fetching: Ji8BqZ-KQ2k (waiting 6.8s)...\n",
      "✓ Success: BofA CEO: Greenland discussions have brought volat\n",
      "  Preview: If you think about the travel last year, the market loves certainty when it feels that the business plans can generate and people can go and so I thin...\n",
      "\n",
      "[60/100] Fetching: b-P0xe3ixdc (waiting 7.0s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 14.2s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 18.8s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 32.1s before retry...\n",
      "✓ Success: President Trump details new tariff threats on NATO\n",
      "  Preview: ROUND OF LEVIES BECAUSE OF GREENLAND. ESSENTIALLY THE FACT THAT THE PRESIDENT WANTS THE U.S. TO OWN IT, SO TO SPEAK. WE AWAIT A POTENTIAL GAME CHANGIN...\n",
      "\n",
      "  💾 Cache saved at 60 videos\n",
      "\n",
      "[61/100] Fetching: 2mAe-PfYjgo (waiting 6.8s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 10.0s before retry...\n",
      "✓ Success: Cramer's Stop Trading: D.R. Horton\n",
      "  Preview: >> ALL RIGHT, LET'S LET'S FIND OUT WHAT YOU GOT FOR STOP TRADING. >> I'M NOT CALLING YOU A BOTTOM HERE, BUT IT'S IMPORTANT TO POINT OUT THAT D.R. HORT...\n",
      "\n",
      "[62/100] Fetching: DCI6Uh1Aw_Y (waiting 6.9s)...\n",
      "✓ Success: The World Economic Forum kicks off in Davos — here\n",
      "  Preview: Hello everybody and welcome to Davos, Switzerland. We are here for the beginning of the World Economic Forum and it is early but things are already bu...\n",
      "\n",
      "[63/100] Fetching: OZrCTsileaU (waiting 7.9s)...\n",
      "✓ Success: Opening Bell: January 20, 2026\n",
      "  Preview: >> AMAZON CEO ANDY JASSY THAT WAS EARLIER ON SQUAWK BOX. HE'S IN DAVOS OFFERING HIS TAKES OF COURSE ON TARIFFS AND THE CONSUMER. SOME REPORTS OVER THE...\n",
      "\n",
      "[64/100] Fetching: VS3OkVon59k (waiting 7.6s)...\n",
      "Attempt 1 failed for VS3OkVon59k: ProxyError: HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: /watch?v=VS3Ok\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 23.8s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 30.6s before retry...\n",
      "⚠️  Rate limit detected (attempt 4/10)\n",
      "   Waiting 54.1s before retry...\n",
      "⚠️  Rate limit detected (attempt 5/10)\n",
      "   Waiting 89.2s before retry...\n",
      "⚠️  Rate limit detected (attempt 6/10)\n",
      "   Waiting 167.7s before retry...\n",
      "✓ Success: Cramer's Mad Dash: Oracle\n",
      "  Preview: WE GET TO AN OPENING BELL. AND OF COURSE, AS WE POINTED OUT, WE ARE GOING TO BE SHARPLY LOWER AT THE OPEN. LET'S TALK A LITTLE ORACLE FOR THE MAD DASH...\n",
      "\n",
      "[65/100] Fetching: _aC_rhv26zk (waiting 7.6s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 13.4s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 19.9s before retry...\n",
      "✓ Success: Coinbase CEO on opposition to U.S. crypto bill\n",
      "  Preview: Number one is really around payment of rewards on stable coins. I think Americans should be able to earn more money on their money. Uh banks should ha...\n",
      "\n",
      "[66/100] Fetching: qocwxpymo5U (waiting 6.5s)...\n",
      "✓ Success: Faber Report: Netflix & Warner Bros. Discovery ame\n",
      "  Preview: NETFLIX. >> THANK YOU. WELL, LET'S DO THAT, BECAUSE OBVIOUSLY IT'S A STORY WE'VE BEEN FOLLOWING, AS YOU KNOW SO CLOSELY FOR QUITE SOME TIME AND WILL B...\n",
      "\n",
      "[67/100] Fetching: 0SExSZBIEuk (waiting 7.6s)...\n",
      "✓ Success: Coca-Cola CEO James Quincey: The U.S. consumer con\n",
      "  Preview: NEXT GUEST HAS A UNIQUE VIEWPOINT ON THE CONSUMER, ON TARIFFS, ON THE GLOBAL ECONOMY AND MUCH MORE. JAMES QUINCEY IS THE CHAIRMAN AND CEO OF COCA-COLA...\n",
      "\n",
      "[68/100] Fetching: MhdavqwbYd4 (waiting 6.4s)...\n",
      "✓ Success: Bessent: Trump is showing the world the U.S. is 'b\n",
      "  Preview: the the US is back and this is what US leadership looks like. Greenland is strategically important. It's strategically important for his Golden Dome p...\n",
      "\n",
      "[69/100] Fetching: tfp1r07_4rg (waiting 6.2s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 19.2s before retry...\n",
      "✓ Success: Carlyle's David Rubenstein: The U.S. economy is do\n",
      "  Preview: NOW DAVID RUBENSTEIN IS HERE. HE'S CO-FOUNDER OF CARLYLE, AND WE'RE SUPPOSED TO INTERVIEW HIM. BUT YOU'RE SUCH A GREAT INTERVIEWER. >> YOU ARE. >> CAN...\n",
      "\n",
      "[70/100] Fetching: HT64yR4d3Tk (waiting 7.4s)...\n",
      "✓ Success: Meta's Joel Kaplan on AI investments: Our ambition\n",
      "  Preview: TRAINIUM IS ALREADY MULTI BILLIONS OF DOLLARS. AND THEY'VE GOT LEO, WHICH IS THE SATELLITE BUSINESS, A LOT OF OTHER THINGS THAT THEY'RE TRYING TO PUT ...\n",
      "\n",
      "  💾 Cache saved at 70 videos\n",
      "\n",
      "[71/100] Fetching: YqURc32k7Ms (waiting 7.1s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 11.7s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 24.8s before retry...\n",
      "✓ Success: CoreWeave CEO Mike Intrator: AI will continue to p\n",
      "  Preview: THAT MEANS FOR ELECTRICITY DEMAND ON TOP OF THAT. BUT A LOT TO TALK ABOUT WITH AMAZON CEO ANDY JASSY. MORE TO COME. RIGHT. >> SO AS THE CORE, WE'VE SO...\n",
      "\n",
      "[72/100] Fetching: OQcDSxbw2cQ (waiting 7.8s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 15.9s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 18.2s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 32.2s before retry...\n",
      "✓ Success: Amazon CEO Andy Jassy: 'Unprecedented' how much AI\n",
      "  Preview: >> EARLIER TODAY, I SAT DOWN WITH AMAZON CEO ANDY JASSY AND ASKED HIM ABOUT THE IMMENSE DEMANDS OF ARTIFICIAL INTELLIGENCE LIKE COMPUTING POWER AND GR...\n",
      "\n",
      "[73/100] Fetching: Gda9T9gZSe4 (waiting 6.7s)...\n",
      "✓ Success: Bridgewater's Ray Dalio on 'capital war' fears: Th\n",
      "  Preview: 100. WE'RE LOOKING AT ALL OF THIS THIS MORNING KIND OF WATCHING HOW THIS PLAYS OUT. PERCENTAGE TERMS. THE NASDAQ IS DOWN THE MOST. IT'S DOWN BY ABOUT ...\n",
      "\n",
      "[74/100] Fetching: Ku57xeJledw (waiting 6.9s)...\n",
      "✓ Success: Amazon CEO Andy Jassy on AI: The cost of inference\n",
      "  Preview: WHENEVER YOU GO THROUGH ANY BIG TECHNOLOGY TRANSFORMATION, YOU FIND NEW JOBS. I THINK THE SAME IS GOING TO BE TRUE HERE, MEDIUM TO LONG TERM. >> WE AL...\n",
      "\n",
      "[75/100] Fetching: lw3FbOspa-o (waiting 7.9s)...\n",
      "✓ Success: Amazon CEO Andy Jassy: Consumers are trying trade \n",
      "  Preview: >> EARLIER TODAY, I SAT DOWN WITH AMAZON CEO ANDY JASSY AS HE SETTLES INTO THE WORLD ECONOMIC FORUM IN DAVOS THIS YEAR. THIS YEAR, PEOPLE ARE THINKING...\n",
      "\n",
      "[76/100] Fetching: ZeXiBERHXC4 (waiting 6.1s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 15.9s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 23.1s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 28.0s before retry...\n",
      "⚠️  Rate limit detected (attempt 4/10)\n",
      "   Waiting 53.5s before retry...\n",
      "⚠️  Rate limit detected (attempt 5/10)\n",
      "   Waiting 89.6s before retry...\n",
      "⚠️  Rate limit detected (attempt 6/10)\n",
      "   Waiting 173.4s before retry...\n",
      "✓ Success: Sen. Tillis on Pres. Trump's desire for Greenland:\n",
      "  Preview: your tea >> WELCOME BACK EVERYBODY. PRESIDENT TRUMP WILL BE ADDRESSING THE WORLD ECONOMIC FORUM IN DAVOS TOMORROW. OUR NEXT GUEST GUEST HAS BEEN CRITI...\n",
      "\n",
      "[77/100] Fetching: qcXxU5iunik (waiting 7.1s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 16.1s before retry...\n",
      "✓ Success: UBS CEO Sergio Ermotti on market volatility: Don't\n",
      "  Preview: INVESTORS WAVE PRESIDENT TRUMP'S THREAT TO HIT A EUROPEAN THE EUROPEAN COUNTRIES I SHOULD SAY WITH ESCALATING TARIFFS IF THE U.S. ISN'T ALLOWED TO BUY...\n",
      "\n",
      "[78/100] Fetching: GgC6XOjgzrY (waiting 6.2s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 17.4s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 23.6s before retry...\n",
      "✓ Success: Arm CEO Rene Haas: AI models will become 'very dom\n",
      "  Preview: >> WELCOME BACK EVERYBODY. JOINING US RIGHT NOW IS A MAJOR PLAYER IN THE AI RACE. ARM HOLDINGS CEO RENEE HAAS. AND RENEE, THANK YOU FOR BEING WITH US ...\n",
      "\n",
      "[79/100] Fetching: ivhU_ArfaA4 (waiting 7.4s)...\n",
      "✓ Success: Bianchi: There certainly are offramps here for the\n",
      "  Preview: 2025. And also with me, Ben Emons, founder at Fedwatch Advisors. Good morning to both of you. Sarah. I'm going to start with you. How serious is this ...\n",
      "\n",
      "[80/100] Fetching: Ju3liEhWJxs (waiting 7.0s)...\n",
      "✓ Success: Lee: This latest spike in volatility is an opportu\n",
      "  Preview: demand with us, Sarah Koontz, managing director at Cleo Capital. Also, we want to bring in Jimmy Lee, CEO of the Wealth Consulting Group. Jimmy, good ...\n",
      "\n",
      "  💾 Cache saved at 80 videos\n",
      "\n",
      "[81/100] Fetching: bB7SduHbVek (waiting 7.1s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 13.4s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 22.2s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 30.2s before retry...\n",
      "✓ Success: Kunst: Tech has done the best on the way up, which\n",
      "  Preview: the premarket. For much more, let's bring in Sara Coombs, managing director at Melio capital. Sara, good morning. Good to see you. >> Good to see you....\n",
      "\n",
      "[82/100] Fetching: f3Byb-2PKOg (waiting 7.4s)...\n",
      "✓ Success: Nasdaq CEO Adena Friedman on 2026 outlook, impact \n",
      "  Preview: IF I GOT TO COME UP WITH THOSE QUESTIONS FOR HIM. >> I WOULD JUST DO THEM OFF THE CUFF THE WAY YOU DO EVERYTHING RIGHT. >> BUT I GOT TO WAIT TILL AFTE...\n",
      "\n",
      "[83/100] Fetching: GWVPVEJahYk (waiting 6.4s)...\n",
      "✓ Success: Watch CNBC's full interview with Treasury Secretar\n",
      "  Preview: GOOD AFTERNOON, GOOD DAY. DEPENDING ON THE TIME ZONE, JOINED BY US TREASURY SECRETARY SCOTT, IT'S OUR GREAT PLEASURE TO HAVE YOU HERE, JOE. >> GOOD TO...\n",
      "\n",
      "[84/100] Fetching: gKTuR5S2M6g (waiting 6.1s)...\n",
      "✓ Success: Bank of America CEO Brian Moynihan and David Beckh\n",
      "  Preview: >> YEAH OKAY. DOWN 655. RIGHT NOW FOR THE DOW S&P FUTURES OFF BY 100. THE NASDAQ INDICATED OFF BY ABOUT 483 POINTS. JOINING US RIGHT NOW BANK OF AMERI...\n",
      "\n",
      "[85/100] Fetching: 0wRFByeZG8k (waiting 7.3s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 17.1s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 22.6s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 32.9s before retry...\n",
      "Attempt 4 failed for 0wRFByeZG8k: RequestBlocked: \n",
      "Could not retrieve a transcript for the video https://www.youtube.com/watch?v=0wRFByeZG8k! This is \n",
      "✓ Success: Straehl: Trade threats are infusing new volatility\n",
      "  Preview: money. Joining me now, Philipp Strahl, chief investment officer for the Americas at Morningstar Wealth. Philipp, good morning. Good to see you. >> Goo...\n",
      "\n",
      "[86/100] Fetching: VfPTMwaJU_4 (waiting 6.7s)...\n",
      "✓ Success: Treasury Secretary Bessent: Pres. Trump could deci\n",
      "  Preview: THE END OF. >> WELL, AT THIS PACE, WE MAY BE THERE BEFORE THE BEFORE THE END OF THAT. >> LET'S SWITCH TO THE FED, BECAUSE WE'VE HAD A LOT OF COMMENTS ...\n",
      "\n",
      "[87/100] Fetching: ljFJYbHbf9Y (waiting 7.8s)...\n",
      "✓ Success: Treasury Secretary Bessent: Pursuing control of Gr\n",
      "  Preview: GOOD AFTERNOON, GOOD DAY. DEPENDING ON THE TIME ZONE, JOINED BY US TREASURY SECRETARY SCOTT, IT'S OUR GREAT PLEASURE TO HAVE YOU HERE, JOE. >> GOOD TO...\n",
      "\n",
      "[88/100] Fetching: Vq_7hZW5jc4 (waiting 7.1s)...\n",
      "✓ Success: Coinbase CEO Brian Armstrong on crypto regulation:\n",
      "  Preview: >> WELCOME BACK TO SQUAWK BOX. WE ARE HERE IN DAVOS, SWITZERLAND. ONE OF THE BIGGEST ISSUES ON CAPITOL HILL IS CRYPTO REGULATION. OUR NEXT GUEST FINDS...\n",
      "\n",
      "[89/100] Fetching: jaYK04ruAM8 (waiting 6.9s)...\n",
      "✓ Success: Tax tip: No tax on tips\n",
      "  Preview: Get a big break on tips this tax season. [music] Due to a new federal law, you can claim a deduction of up to $25,000 for qualified [music] tips on yo...\n",
      "\n",
      "[90/100] Fetching: OlUDNP7DpYI (waiting 7.4s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 15.6s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 24.6s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 30.9s before retry...\n",
      "✓ Success: Why NBCU is all in on sports\n",
      "  Preview: I'm not sure there's ever been a month in media [music] and sports quite like what we're going to see in February in terms of the amount of content on...\n",
      "\n",
      "  💾 Cache saved at 90 videos\n",
      "\n",
      "[91/100] Fetching: pQVp5vzuOHo (waiting 6.9s)...\n",
      "✓ Success: Tax tip: 2025 HSA deadline\n",
      "  Preview: Contribute to a health savings account to pay for medical expenses [music] with tax-free money. If you're enrolled in a highdeductible health plan, pu...\n",
      "\n",
      "[92/100] Fetching: 3qf_h8DXLyY (waiting 7.5s)...\n",
      "✓ Success: Mad Money 01/16/26 | Audio Only\n",
      "  Preview: Hey, I'm Kramer. Welcome to Mad Money. Welcome to Craig America. Other people, my friends, I'm just trying to make a little bit of money. My job is no...\n",
      "\n",
      "[93/100] Fetching: Wylo89xyL6E (waiting 6.5s)...\n",
      "✓ Success: There's no quick fix for chip shortage despite new\n",
      "  Preview: HAPPEN. IT'S ALMOST LIKE NOBODY SAW IT COMING. I'M TALKING ABOUT THE SEVERE SHORTAGES OF ALL SORTS OF LOWER END SEMICONDUCTORS THAT GO INTO A DATA CEN...\n",
      "\n",
      "[94/100] Fetching: lT-Q4jGjK_g (waiting 6.6s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 11.6s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 24.7s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/10)\n",
      "   Waiting 29.1s before retry...\n",
      "⚠️  Rate limit detected (attempt 4/10)\n",
      "   Waiting 52.6s before retry...\n",
      "✓ Success: Lightning Round: Medline is 'unbelievable', it's a\n",
      "  Preview: ALSO SOUTHERN. PLAYING ITSELF. AND THEN THE LIGHTNING ROUND IS OVER. ARE YOU READY? SKI DANCE. ALL RIGHT, LET'S START WITH MATTHEW IN GEORGIA. MATTHEW...\n",
      "\n",
      "[95/100] Fetching: F4JAvRl6Ns4 (waiting 7.8s)...\n",
      "✓ Success: Jim Cramer talk Micron breaking ground on New York\n",
      "  Preview: HAPPEN. IT'S ALMOST LIKE NOBODY SAW IT COMING. I'M TALKING ABOUT THE SEVERE SHORTAGES OF ALL SORTS OF LOWER END SEMICONDUCTORS THAT GO INTO A DATA CEN...\n",
      "\n",
      "[96/100] Fetching: EsvCNf6CEYY (waiting 7.0s)...\n",
      "✓ Success: Jim Cramer looks ahead to next week's market game \n",
      "  Preview: HEY I'M TO MAD MONEY. WELCOME TO CRAMERICA UP WITH MY FRIENDS. I'M JUST TRYING TO MAKE A LITTLE BIT OF MONEY. MY JOB IS NOT JUST ENTERTAINING BUT IT'S...\n",
      "\n",
      "[97/100] Fetching: TD5YGHvr8ik (waiting 6.9s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 16.2s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 16.8s before retry...\n",
      "✓ Success: We are starting to see green shoots in the housing\n",
      "  Preview: TEACH. SO CALL ME AT ONE 800 743 CNBC OR TWEET ME AT JIM CRAMER. THIS MARKET IS KIND OF JUST MEANDERING. IT'S ERRATIC. IT HATES THE BANKS. ONE DAY LIK...\n",
      "\n",
      "[98/100] Fetching: e6zVyv7q40Y (waiting 6.8s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 13.8s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 22.4s before retry...\n",
      "✓ Success: Final Trade: IONQ, URA, KRE, GM\n",
      "  Preview: TRADE TIME. LET'S START WITH MIKE. >> YEAH. ONE PART OF AUTO AFFORDABILITY, OF COURSE, IS GASOLINE. THEY'VE DROPPED 40%. GAS PRICES ARE DOWN 40% SINCE...\n",
      "\n",
      "[99/100] Fetching: 7XXA-QpRXAY (waiting 7.6s)...\n",
      "✓ Success: 'Fast Money' traders Bonawyn Eison and Mike Khouw \n",
      "  Preview: Try for free today. WE'RE REVEALING OUR LAST TWO TRADER ACRONYMS TONIGHT. BRONWYN AND MIKE ARE ROUNDING OUT THE GROUP. LET'S START WITH BONOAN, WHO EN...\n",
      "\n",
      "[100/100] Fetching: TNDfNdn_dVc (waiting 6.5s)...\n",
      "⚠️  Rate limit detected (attempt 1/10)\n",
      "   Waiting 16.7s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/10)\n",
      "   Waiting 20.7s before retry...\n",
      "✓ Success: FedWatch's Ben Emons reacts to shake up in Fed Cha\n",
      "  Preview: DRIVING A WEDGE BETWEEN HIM AND HASSETT, WHOSE ODDS PLUNGED TO 16%, PUTTING HIM IN THIRD PLACE. FOR MORE, LET'S BRING IN BEN EVANS, FOUNDER AND CHIEF ...\n",
      "\n",
      "  💾 Cache saved at 100 videos\n",
      "\n",
      "\n",
      "📊 Summary:\n",
      "  ✓ Successfully fetched: 58\n",
      "  ✓ From cache: 40\n",
      "  ✗ Failed/No transcript: 2\n",
      "  Total processed: 100/100\n",
      "  📦 Final cache size: 100/100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "youtube_videos_api = attach_transcripts(youtube_videos_api, delay_between_requests = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a08c10",
   "metadata": {},
   "source": [
    "## Add transcripts to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite in-memory transcripts with cache values when available\n",
    "\n",
    "youtube_videos_api = refresh_transcripts_in_dict(youtube_videos_api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d0e87",
   "metadata": {},
   "source": [
    "## Check rotating proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"PROXY_USER\")\n",
    "password = os.getenv(\"PROXY_PASS\")\n",
    "endpoint = os.getenv(\"PROXY_HOST\") +  \":\" + str(os.getenv(\"PROXY_PORT\"))\n",
    "\n",
    "proxy_url = f\"http://{username}:{password}@{endpoint}\"\n",
    "\n",
    "def check_ip_rotation(num_requests=10):\n",
    "    \"\"\"Check if proxy IPs are rotating\"\"\"\n",
    "    proxies = {\n",
    "        'http': proxy_url,\n",
    "        'https': proxy_url\n",
    "    }\n",
    "    \n",
    "    ips = []\n",
    "    for i in range(num_requests):\n",
    "        try:\n",
    "            # Using http instead of https for simpler testing\n",
    "            response = requests.get('http://ipinfo.io/json', \n",
    "                                   proxies=proxies, \n",
    "                                   timeout=10)\n",
    "            ip = response.json().get('ip')\n",
    "            ips.append(ip)\n",
    "            print(f\"Request {i+1}: IP = {ip}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Request {i+1} failed: {e}\")\n",
    "    \n",
    "    unique_ips = set(ips)\n",
    "    print(f\"\\nTotal requests: {len(ips)}\")\n",
    "    print(f\"Unique IPs: {len(unique_ips)}\")\n",
    "    print(f\"IPs are {'ROTATING ✓' if len(unique_ips) > 1 else 'NOT ROTATING ✗'}\")\n",
    "    return ips\n",
    "\n",
    "# Test rotation\n",
    "print(\"Testing IP rotation with Webshare:\")\n",
    "check_ip_rotation(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff8479",
   "metadata": {},
   "source": [
    "# After Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17316fe",
   "metadata": {},
   "source": [
    "## Sentiment and Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "def chunk_text_words(text, chunk_words=500):\n",
    "    words = text.split()\n",
    "    return [\n",
    "        \" \".join(words[i:i+chunk_words])\n",
    "        for i in range(0, len(words), chunk_words)\n",
    "    ]\n",
    "\n",
    "def summarize_long_text(text):\n",
    "    chunks = chunk_text_words(text, chunk_words=500)\n",
    "\n",
    "    partial_summaries = []\n",
    "    for chunk in chunks:\n",
    "        result = summarizer(\n",
    "            chunk,\n",
    "            max_length=120,\n",
    "            min_length=40,\n",
    "            do_sample=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        partial_summaries.append(result[0][\"summary_text\"])\n",
    "\n",
    "    combined = \" \".join(partial_summaries)\n",
    "\n",
    "    final = summarizer(\n",
    "        combined,\n",
    "        max_length=180,\n",
    "        min_length=60,\n",
    "        do_sample=False,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return final[0][\"summary_text\"]\n",
    "\n",
    "\n",
    "def analyze_video_sentiment(video, debug=False):\n",
    "    \"\"\"Analyze sentiment of title and transcript separately\"\"\"\n",
    "    \n",
    "    # Title: Direct sentiment (no summarization)\n",
    "    title = video.get('title', '')\n",
    "    if title:\n",
    "        try:\n",
    "            title_sentiment = sentiment_analyzer(title[:512])[0]\n",
    "            video['title_sentiment'] = title_sentiment\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"    Title sentiment failed: {e}\")\n",
    "            video['title_sentiment'] = None\n",
    "    else:\n",
    "        video['title_sentiment'] = None\n",
    "    \n",
    "    # Transcript: Summarize → Sentiment\n",
    "    transcript_text = video.get('transcript_text', '')\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   Transcript length: {len(transcript_text)} chars, {len(transcript_text.split())} words\")\n",
    "    \n",
    "    # Check if transcript exists and is long enough\n",
    "    if not transcript_text or len(transcript_text.strip()) < 200:\n",
    "        if debug:\n",
    "            print(f\"    Transcript too short or missing\")\n",
    "        video['transcript_summary'] = None\n",
    "        video['transcript_sentiment'] = None\n",
    "        return video\n",
    "    \n",
    "    try:\n",
    "        # Clean and truncate transcript\n",
    "        transcript_text = transcript_text.strip()\n",
    "        words = transcript_text.split()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Word count: {len(words)}\")\n",
    "        \n",
    "        # BART works best with 100-1024 tokens\n",
    "        if len(words) < 100:\n",
    "            if debug:\n",
    "                print(f\"    Too few words: {len(words)}\")\n",
    "            video['transcript_summary'] = None\n",
    "            video['transcript_sentiment'] = None\n",
    "            return video\n",
    "        \n",
    "        if len(words) > 1000:\n",
    "            transcript_text = ' '.join(words[:1000])\n",
    "            if debug:\n",
    "                print(f\"    Truncated to 1000 words\")\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Generating summary...\")\n",
    "            print(f\"   First 200 chars: {transcript_text[:200]}\")\n",
    "        \n",
    "        # Generate summary with better parameters\n",
    "\n",
    "        summary = summarize_long_text(transcript_text)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Summary: {summary}\")\n",
    "        \n",
    "        # Sentiment of summary\n",
    "        transcript_sentiment = sentiment_analyzer(summary[:512])[0]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Sentiment: {transcript_sentiment}\")\n",
    "        \n",
    "        video['transcript_summary'] = summary\n",
    "        video['transcript_sentiment'] = transcript_sentiment\n",
    "        \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"   Error: {type(e).__name__}: {str(e)}\")\n",
    "        video['transcript_summary'] = None\n",
    "        try:\n",
    "            video['transcript_sentiment'] = sentiment_analyzer(transcript_text[:512])[0]\n",
    "        except Exception:\n",
    "            video['transcript_sentiment'] = None\n",
    "    \n",
    "    return video\n",
    "\n",
    "# Test on first video with debug output\n",
    "print(\"\\nTesting first video with debug output:\\n\")\n",
    "if youtube_videos_api:\n",
    "    test_video = youtube_videos_api[0].copy()\n",
    "    print(f\"Title: {test_video.get('title')}\")\n",
    "    analyze_video_sentiment(test_video, debug=True)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Ask user if they want to continue\n",
    "response = input(\"Continue with all videos? (y/n): \")\n",
    "\n",
    "if response.lower() == 'y':\n",
    "    print(\"\\n Analyzing all videos...\")\n",
    "    \n",
    "    for video in tqdm(youtube_videos_api, desc=\"Processing videos\", unit=\"video\"):\n",
    "        if video.get('transcript_text'):\n",
    "            analyze_video_sentiment(video, debug=False)\n",
    "        else:\n",
    "            video['title_sentiment'] = None\n",
    "            video['transcript_summary'] = None\n",
    "            video['transcript_sentiment'] = None\n",
    "    \n",
    "    print(\"Analysis complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(videos, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(videos, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# usage\n",
    "save_to_json(youtube_videos_api, \"youtube_analysis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac76c1",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_entity(name: str) -> str:\n",
    "    if not name:\n",
    "        return name\n",
    "\n",
    "    n = name.strip().lower()\n",
    "\n",
    "    n = re.sub(r\"^(the|a|an)\\s+\", \"\", n)\n",
    "    n = re.sub(r\"[^\\w\\s]\", \"\", n)\n",
    "    n = re.sub(r\"\\s+\", \" \", n)\n",
    "\n",
    "    if n in ENTITY_ALIASES:\n",
    "        return ENTITY_ALIASES[n]\n",
    "\n",
    "    return n.upper() if n.isupper() else n.title()\n",
    "\n",
    "def sentiment_to_score(sentiment):\n",
    "    if not sentiment:\n",
    "        return None\n",
    "    label = str(sentiment.get('label', '')).upper()\n",
    "    score = float(sentiment.get('score', 0))\n",
    "    if 'POS' in label:\n",
    "        return score\n",
    "    if 'NEG' in label:\n",
    "        return -score\n",
    "    return 0.0\n",
    "\n",
    "def extract_video_text(video, prefer_summary=True):\n",
    "    title = video.get('title', '')\n",
    "    transcript = ''\n",
    "    if prefer_summary and video.get('transcript_summary'):\n",
    "        transcript = video['transcript_summary']\n",
    "    elif video.get('transcript_text'):\n",
    "        transcript = video['transcript_text']\n",
    "    combined = f\"{title} {transcript}\".strip()\n",
    "    return combined\n",
    "\n",
    "def analyze_video_entities_split(video):\n",
    "    title = video.get('title', '') or ''\n",
    "\n",
    "    # Prefer summary, but fall back to full transcript_text if no summary\n",
    "    raw_summary = video.get('transcript_summary') or video.get('transcript_text') or ''\n",
    "    summary = raw_summary\n",
    "\n",
    "    title_doc = nlp(title) if title else None\n",
    "    summary_doc = nlp(summary) if summary else None\n",
    "\n",
    "    title_tickers = set(get_tickers(title)) if title else set()\n",
    "    title_companies = set(get_companies(title_doc)) if title_doc else set()\n",
    "    title_sectors = set(get_sectors(title.lower())) if title else set()\n",
    "\n",
    "    summary_tickers = set(get_tickers(summary)) if summary else set()\n",
    "    summary_companies = set(get_companies(summary_doc)) if summary_doc else set()\n",
    "    summary_sectors = set(get_sectors(summary.lower())) if summary else set()\n",
    "\n",
    "    title_score = sentiment_to_score(video.get('title_sentiment'))\n",
    "    summary_score = sentiment_to_score(video.get('transcript_sentiment'))\n",
    "\n",
    "    return {\n",
    "        \"title\": (title_tickers, title_companies, title_sectors, title_score),\n",
    "        \"summary\": (summary_tickers, summary_companies, summary_sectors, summary_score),\n",
    "    }\n",
    "\n",
    "def aggregate_youtube_entities(videos):\n",
    "\n",
    "    def new_bucket():\n",
    "        return {\n",
    "            \"title_mentions\": 0,\n",
    "            \"title_scores\": [],  # One score per video where entity appears in title\n",
    "            \"summary_mentions\": 0,\n",
    "            \"summary_scores\": [],  # One score per video where entity appears in summary\n",
    "        }\n",
    "\n",
    "    stock_stats = defaultdict(new_bucket)\n",
    "    company_stats = defaultdict(new_bucket)\n",
    "    sector_stats = defaultdict(new_bucket)\n",
    "\n",
    "    for video in videos:\n",
    "        parts = analyze_video_entities_split(video)\n",
    "\n",
    "        # Track which entities we've already counted for this video (per part)\n",
    "        # to avoid adding the same score multiple times\n",
    "        title_entities_seen = set()\n",
    "        summary_entities_seen = set()\n",
    "\n",
    "        for part_name, (tickers, companies, sectors, score) in parts.items():\n",
    "            is_title = (part_name == \"title\")\n",
    "            seen_set = title_entities_seen if is_title else summary_entities_seen\n",
    "\n",
    "            for t in tickers:\n",
    "                t = normalize_entity(t)\n",
    "                stock_stats[t][f\"{part_name}_mentions\"] += 1\n",
    "                # Only add score once per video per part\n",
    "                if t not in seen_set and score is not None:\n",
    "                    stock_stats[t][f\"{part_name}_scores\"].append(score)\n",
    "                    seen_set.add(t)\n",
    "\n",
    "            for c in companies:\n",
    "                c = normalize_entity(c)\n",
    "                company_stats[c][f\"{part_name}_mentions\"] += 1\n",
    "                # Only add score once per video per part\n",
    "                if c not in seen_set and score is not None:\n",
    "                    company_stats[c][f\"{part_name}_scores\"].append(score)\n",
    "                    seen_set.add(c)\n",
    "\n",
    "            for s in sectors:\n",
    "                s = normalize_entity(s)\n",
    "                sector_stats[s][f\"{part_name}_mentions\"] += 1\n",
    "                # Only add score once per video per part\n",
    "                if s not in seen_set and score is not None:\n",
    "                    sector_stats[s][f\"{part_name}_scores\"].append(score)\n",
    "                    seen_set.add(s)\n",
    "\n",
    "    def finalize(stats):\n",
    "        rows = []\n",
    "        for name, data in stats.items():\n",
    "            rows.append({\n",
    "                \"name\": name,\n",
    "\n",
    "                \"title_mentions\": data[\"title_mentions\"],\n",
    "                \"avg_title_sentiment\": (\n",
    "                    sum(data[\"title_scores\"]) / len(data[\"title_scores\"])\n",
    "                    if data[\"title_scores\"] else None\n",
    "                ),\n",
    "\n",
    "                \"summary_mentions\": data[\"summary_mentions\"],\n",
    "                \"avg_summary_sentiment\": (\n",
    "                    sum(data[\"summary_scores\"]) / len(data[\"summary_scores\"])\n",
    "                    if data[\"summary_scores\"] else None\n",
    "                ),\n",
    "            })\n",
    "\n",
    "        rows.sort(key=lambda x: (x[\"title_mentions\"] + x[\"summary_mentions\"]), reverse=True)\n",
    "        return rows\n",
    "\n",
    "    return {\n",
    "        \"stocks\": finalize(stock_stats),\n",
    "        \"companies\": finalize(company_stats),\n",
    "        \"sectors\": finalize(sector_stats),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = aggregate_youtube_entities(youtube_videos_api)\n",
    "save_to_json(result, \"entity_mentions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['companies']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665887a",
   "metadata": {},
   "source": [
    "## Turn Mentions into readable txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sentiment(score):\n",
    "    \"\"\"Format sentiment score for display.\"\"\"\n",
    "    if score is None:\n",
    "        return \"N/A\"\n",
    "    return f\"{score:+.4f}\"\n",
    "\n",
    "def format_mentions(item):\n",
    "    \"\"\"Format a single item's mention data.\"\"\"\n",
    "    total_mentions = item.get(\"title_mentions\", 0) + item.get(\"summary_mentions\", 0)\n",
    "    title_sent = format_sentiment(item.get(\"avg_title_sentiment\"))\n",
    "    summary_sent = format_sentiment(item.get(\"avg_summary_sentiment\"))\n",
    "    \n",
    "    lines = [\n",
    "        f\"  Name: {item['name']}\",\n",
    "        f\"  Total Mentions: {total_mentions}\",\n",
    "        f\"    - Title Mentions: {item.get('title_mentions', 0)} (Sentiment: {title_sent})\",\n",
    "        f\"    - Summary Mentions: {item.get('summary_mentions', 0)} (Sentiment: {summary_sent})\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = Path(\"entity_mentions.json\")\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build the output text\n",
    "output_lines = []\n",
    "output_lines.append(\"=\" * 80)\n",
    "output_lines.append(\"ENTITY MENTIONS REPORT\")\n",
    "output_lines.append(\"=\" * 80)\n",
    "output_lines.append(\"\")\n",
    "\n",
    "# Stocks Section\n",
    "output_lines.append(\"STOCKS\")\n",
    "output_lines.append(\"-\" * 80)\n",
    "if data.get(\"stocks\"):\n",
    "    for i, stock in enumerate(data[\"stocks\"], 1):\n",
    "        output_lines.append(f\"\\n{i}. {format_mentions(stock)}\")\n",
    "else:\n",
    "    output_lines.append(\"  No stocks found.\")\n",
    "output_lines.append(\"\")\n",
    "output_lines.append(\"\")\n",
    "\n",
    "# Companies Section\n",
    "output_lines.append(\"COMPANIES\")\n",
    "output_lines.append(\"-\" * 80)\n",
    "if data.get(\"companies\"):\n",
    "    for i, company in enumerate(data[\"companies\"], 1):\n",
    "        output_lines.append(f\"\\n{i}. {format_mentions(company)}\")\n",
    "else:\n",
    "    output_lines.append(\"  No companies found.\")\n",
    "output_lines.append(\"\")\n",
    "output_lines.append(\"\")\n",
    "\n",
    "# Sectors Section\n",
    "output_lines.append(\"SECTORS\")\n",
    "output_lines.append(\"-\" * 80)\n",
    "if data.get(\"sectors\"):\n",
    "    for i, sector in enumerate(data[\"sectors\"], 1):\n",
    "        output_lines.append(f\"\\n{i}. {format_mentions(sector)}\")\n",
    "else:\n",
    "    output_lines.append(\"  No sectors found.\")\n",
    "\n",
    "output_lines.append(\"\")\n",
    "output_lines.append(\"=\" * 80)\n",
    "\n",
    "# Join all lines\n",
    "output_text = \"\\n\".join(output_lines)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"entity_mentions.txt\")\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(output_text)\n",
    "\n",
    "print(f\"Successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def28e16",
   "metadata": {},
   "source": [
    "# Filter by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('100vids.json', 'r', encoding='utf-8') as file:\n",
    "    # Use json.load() to convert the file content to a Python object\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def filter_by_date_range(videos, chosen_date_str):\n",
    "    \"\"\"\n",
    "    chosen_date_str format: 'YYYY-MM-DD'\n",
    "    \"\"\"\n",
    "    chosen_date = datetime.strptime(chosen_date_str, \"%Y-%m-%d\")\n",
    "    start_date = chosen_date - timedelta(days=7)\n",
    "\n",
    "    filtered = []\n",
    "\n",
    "    for v in videos:\n",
    "        published_str = v.get(\"published\") or v.get(\"published_date\")\n",
    "        if not published_str:\n",
    "            continue\n",
    "\n",
    "        published_dt = datetime.fromisoformat(published_str.replace(\"Z\", \"\"))\n",
    "\n",
    "        if start_date <= published_dt <= chosen_date:\n",
    "            filtered.append(v)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c1bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a705a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_date = \"2026-01-15\"\n",
    "\n",
    "filtered_videos = filter_by_date_range(data, chosen_date)\n",
    "filtered_videos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
