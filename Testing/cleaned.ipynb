{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d7d2e4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f6f3f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8632f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\News_Majority\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "import trafilatura\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# import io\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "from youtube_transcript_api.proxies import WebshareProxyConfig, GenericProxyConfig\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbf278",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d868a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Installing spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cd40c",
   "metadata": {},
   "source": [
    "## Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb502e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    tokenizer=\"ProsusAI/finbert\"\n",
    ")\n",
    "\n",
    "def get_text_sentiment_score(text: str, max_chars=512) -> float:\n",
    "    if not text or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    chunks = [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n",
    "    scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        result = sentiment_analyzer(chunk)[0]\n",
    "        label = result[\"label\"].upper()\n",
    "        score = float(result[\"score\"])\n",
    "\n",
    "        if \"POS\" in label:\n",
    "            scores.append(score)\n",
    "        elif \"NEG\" in label:\n",
    "            scores.append(-score)\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return sum(scores) / len(scores) if scores else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b072542",
   "metadata": {},
   "source": [
    "## Entity Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b44139",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SENTENCES = 1\n",
    "TICKER_LIST_PATH = Path(\"tickers.csv\")  # optional: columns ticker,name\n",
    "\n",
    "TICKER_RE = re.compile(r\"(?<![A-Z])\\$?[A-Z]{2,5}(?![A-Z])\")\n",
    "TICKER_STOP = {\n",
    "    \"A\", \"AN\", \"AND\", \"ARE\", \"AS\", \"AT\", \"BE\", \"BUT\", \"BY\", \"CAN\", \"CO\", \"FOR\",\n",
    "    \"FROM\", \"HAS\", \"HAVE\", \"IN\", \"IS\", \"IT\", \"ITS\", \"NOT\", \"OF\", \"ON\", \"OR\",\n",
    "    \"THE\", \"TO\", \"WAS\", \"WERE\", \"WILL\", \"WITH\",\n",
    "}\n",
    "\n",
    "ENTITY_ALIASES = {\n",
    "    # companies\n",
    "    \"meta\": \"META\",\n",
    "    \"facebook\": \"META\",\n",
    "\n",
    "    \"google\": \"GOOGL\",\n",
    "    \"alphabet\": \"GOOGL\",\n",
    "\n",
    "    \"apple\": \"AAPL\",\n",
    "    \"amazon\": \"AMZN\",\n",
    "    \"microsoft\": \"MSFT\",\n",
    "\n",
    "    # institutions\n",
    "    \"fed\": \"Federal Reserve\",\n",
    "    \"federal reserve\": \"Federal Reserve\",\n",
    "    \"doj\": \"Department of Justice\",\n",
    "    \"department of justice\": \"Department of Justice\",\n",
    "    \"supreme court\": \"Supreme Court\",\n",
    "    \"cnn\": \"CNN\",\n",
    "}\n",
    "\n",
    "SECTOR_KEYWORDS = {\n",
    "    \"Technology\": [\"tech\", \"software\", \"technology\", \"cloud\", \"ai\", \"artificial intelligence\",\n",
    "                   \"chip\", \"semiconductor\", \"digital\", \"platform\", \"app\", \"data\", \"cyber\"],\n",
    "    \"Finance\": [\"bank\", \"financial\", \"finance\", \"investment\", \"trading\", \"market\",\n",
    "                \"stock\", \"equity\", \"bond\", \"credit\", \"lending\", \"mortgage\"],\n",
    "    \"Healthcare\": [\"health\", \"medical\", \"pharmaceutical\", \"drug\", \"biotech\", \"hospital\",\n",
    "                    \"treatment\", \"patient\", \"fda\", \"clinical\", \"therapy\"],\n",
    "    \"Energy\": [\"oil\", \"gas\", \"energy\", \"petroleum\", \"renewable\", \"solar\", \"wind\",\n",
    "               \"electric\", \"power\", \"fuel\", \"drilling\", \"crude\"],\n",
    "    \"Retail\": [\"retail\", \"store\", \"shopping\", \"consumer\", \"e-commerce\", \"online shopping\",\n",
    "               \"merchandise\", \"sales\", \"retailer\"],\n",
    "    \"Automotive\": [\"car\", \"automotive\", \"vehicle\", \"auto\", \"truck\", \"electric vehicle\",\n",
    "                   \"ev\", \"manufacturing\", \"tesla\"],\n",
    "    \"Real Estate\": [\"real estate\", \"property\", \"housing\", \"construction\", \"mortgage\",\n",
    "                    \"development\", \"reit\"],\n",
    "    \"Telecommunications\": [\"telecom\", \"communication\", \"wireless\", \"5g\", \"network\", \"internet\"],\n",
    "    \"Aerospace\": [\"aerospace\", \"aircraft\", \"defense\", \"boeing\", \"space\"],\n",
    "    \"Consumer Goods\": [\"consumer goods\", \"packaged goods\", \"cpg\"],\n",
    "}\n",
    "\n",
    "def normalize_company_name(name):\n",
    "    return name.lower().replace(\"inc.\", \"\").replace(\"corp.\", \"\").replace(\"corporation\", \"\").strip()\n",
    "\n",
    "def extract_article_text(url: str) -> str | None:\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    if not downloaded:\n",
    "        return None\n",
    "\n",
    "    text = trafilatura.extract(\n",
    "        downloaded,\n",
    "        include_comments=False,\n",
    "        include_tables=False,\n",
    "        include_formatting=False\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def load_ticker_map(path: Path):\n",
    "    ticker_to_name = {}\n",
    "    name_to_ticker = {}\n",
    "    if not path.exists():\n",
    "        return ticker_to_name, name_to_ticker\n",
    "\n",
    "    with path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            ticker = (row.get(\"ticker\") or \"\").strip().upper()\n",
    "            name = (row.get(\"name\") or \"\").strip()\n",
    "            if not ticker or not name:\n",
    "                continue\n",
    "            ticker_to_name[ticker] = name\n",
    "            name_to_ticker[normalize_company_name(name)] = ticker\n",
    "\n",
    "    return ticker_to_name, name_to_ticker\n",
    "\n",
    "\n",
    "ticker_to_name, name_to_ticker = load_ticker_map(TICKER_LIST_PATH)\n",
    "\n",
    "\n",
    "def fetch_articles(feed_url, max_items=30):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    articles = []\n",
    "    for entry in feed.entries[:max_items]:\n",
    "        text = extract_article_text(entry.link)\n",
    "        if not text:\n",
    "            continue\n",
    "        articles.append({\n",
    "            \"title\": entry.title,\n",
    "            \"url\": entry.link,\n",
    "            \"published\": entry.get(\"published\"),\n",
    "            \"text\": text,\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_tickers(text):\n",
    "    tickers = set()\n",
    "    for m in TICKER_RE.findall(text):\n",
    "        t = m.replace(\"$\", \"\").upper()\n",
    "        if t in TICKER_STOP:\n",
    "            continue\n",
    "        if ticker_to_name and t not in ticker_to_name:\n",
    "            continue\n",
    "        tickers.add(t)\n",
    "\n",
    "    return list(tickers)\n",
    "\n",
    "def get_companies(doc):\n",
    "    mapped = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ != \"ORG\":\n",
    "            continue\n",
    "        key = normalize_company_name(ent.text)\n",
    "        if key in name_to_ticker:\n",
    "            mapped.append(name_to_ticker[key])   # return ticker\n",
    "        else:\n",
    "            mapped.append(ent.text)\n",
    "    return mapped\n",
    "\n",
    "\n",
    "def get_sectors(text_lower):\n",
    "    return [\n",
    "        sector for sector, keywords in SECTOR_KEYWORDS.items()\n",
    "        if any(kw in text_lower for kw in keywords)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ce0b7",
   "metadata": {},
   "source": [
    "## Youtube Data Api Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6f0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "YOUTUBE_API_KEY = os.getenv(\"API_KEY\")\n",
    "CHANNEL_ID = \"UCrp_UI8XtuYfpiqluWLD7Lw\"  # CNBC channel\n",
    "MAX_VIDEOS = 100\n",
    "\n",
    "def fetch_youtube_videos_with_api(channel_id, api_key, max_results=100):\n",
    "    \"\"\"Fetch YouTube videos using Data API (no transcripts needed)\"\"\"\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    uploads_playlist_id = None\n",
    "    \n",
    "    print(f\"Fetching videos from channel {channel_id}...\")\n",
    "    \n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            # First, get the uploads playlist ID for the channel\n",
    "            if uploads_playlist_id is None:  # Only need to do this once\n",
    "                channel_response = youtube.channels().list(\n",
    "                    part='contentDetails',\n",
    "                    id=channel_id\n",
    "                ).execute()\n",
    "                \n",
    "                if not channel_response.get('items'):\n",
    "                    print(f\"‚ùå Channel {channel_id} not found\")\n",
    "                    break\n",
    "                \n",
    "                uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "            \n",
    "            # Get videos from uploads playlist\n",
    "            if uploads_playlist_id:\n",
    "                request = youtube.playlistItems().list(\n",
    "                    part='snippet,contentDetails',\n",
    "                    playlistId=uploads_playlist_id,\n",
    "                    maxResults=min(max_results, max_results - len(videos)),\n",
    "                    pageToken=next_page_token\n",
    "                )\n",
    "            else:\n",
    "                # Fallback: search for videos from channel\n",
    "                request = youtube.search().list(\n",
    "                    part='snippet',\n",
    "                    channelId=channel_id,\n",
    "                    type='video',\n",
    "                    maxResults=min(max_results, max_results - len(videos)),\n",
    "                    pageToken=next_page_token,\n",
    "                    order='date'\n",
    "                )\n",
    "            \n",
    "            response = request.execute()\n",
    "            \n",
    "            # Get video IDs\n",
    "            video_ids = []\n",
    "            for item in response['items']:\n",
    "                if 'contentDetails' in item:\n",
    "                    video_ids.append(item['contentDetails']['videoId'])\n",
    "                elif 'id' in item and 'videoId' in item['id']:\n",
    "                    video_ids.append(item['id']['videoId'])\n",
    "            \n",
    "            # Get detailed video information\n",
    "            if video_ids:\n",
    "                video_details = youtube.videos().list(\n",
    "                    part='snippet,statistics',\n",
    "                    id=','.join(video_ids)\n",
    "                ).execute()\n",
    "                \n",
    "                for item in video_details['items']:\n",
    "                    snippet = item['snippet']\n",
    "                    videos.append({\n",
    "                        'title': snippet.get('title', ''),\n",
    "                        'video_id': item['id'],\n",
    "                        'url': f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                        'published': snippet.get('publishedAt', ''),\n",
    "                        'published_date': snippet.get('publishedAt', ''),\n",
    "                        'author': snippet.get('channelTitle', ''),\n",
    "                        'summary': snippet.get('description', ''),  # Full description\n",
    "                        'transcript_text': None,  # No transcript (IP banned)\n",
    "                        'view_count': item['statistics'].get('viewCount', 0),\n",
    "                        'like_count': item['statistics'].get('likeCount', 0),\n",
    "                    })\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "                \n",
    "            print(f\"  Fetched {len(videos)} videos so far...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error fetching videos: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"‚úÖ Total videos fetched: {len(videos)}\")\n",
    "    return videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293213d",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfcd3d",
   "metadata": {},
   "source": [
    "## Retrieve latest 100 videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d85a61",
   "metadata": {},
   "source": [
    "## Retrieve and cache Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c27c6d",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "print(\"HTTP_PROXY =\", os.environ.get(\"HTTP_PROXY\"))\n",
    "print(\"HTTPS_PROXY =\", os.environ.get(\"HTTPS_PROXY\"))\n",
    "print(\"Proxies seen by requests:\", requests.utils.get_environ_proxies(\"https://www.youtube.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "# URL-encode credentials\n",
    "proxy_user = quote(os.getenv(\"PROXY_USER\"), safe='')\n",
    "proxy_pass = quote(os.getenv(\"PROXY_PASS\"), safe='')\n",
    "\n",
    "# Set rotating proxy globally\n",
    "proxy_url = f\"http://{proxy_user}:{proxy_pass}@p.webshare.io:80\"\n",
    "os.environ['HTTP_PROXY'] = proxy_url\n",
    "os.environ['HTTPS_PROXY'] = proxy_url\n",
    "\n",
    "# Test rotation by checking your IP multiple times\n",
    "print(\"Testing proxy rotation...\")\n",
    "for i in range(5):\n",
    "    try:\n",
    "        response = requests.get('https://api.ipify.org?format=json', \n",
    "                               proxies={'http': proxy_url, 'https': proxy_url},\n",
    "                               timeout=10)\n",
    "        ip = response.json().get('ip')\n",
    "        print(f\"Request {i+1}: IP = {ip}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Request {i+1}: Error = {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3abb11",
   "metadata": {},
   "outputs": [
    {
     "ename": "RetryError",
     "evalue": "HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: /api/timedtext?v=wN2s5uGh1YQ&ei=NxxvaenoEKSWsfIP3eO44QE&caps=asr&opi=112496729&xoaf=5&xowf=1&xospf=1&hl=en&ip=0.0.0.0&ipbits=0&expire=1768914599&sparams=ip,ipbits,expire,v,ei,caps,opi,xoaf&signature=E93C13F2E512B61142D85E105FA51C9C455748B8.3C6A19EDE1D97C47948DDA4225367CAFA3399623&key=yt8&lang=en&name=DTVCC1 (Caused by ResponseError('too many 429 error responses'))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[31mResponseError\u001b[39m: too many 429 error responses",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:942\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    941\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, url)\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:942\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    941\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, url)\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "    \u001b[31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 942 (7 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:942\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    941\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, url)\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:932\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    931\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m     retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py:535\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    534\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    537\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: /api/timedtext?v=wN2s5uGh1YQ&ei=NxxvaenoEKSWsfIP3eO44QE&caps=asr&opi=112496729&xoaf=5&xowf=1&xospf=1&hl=en&ip=0.0.0.0&ipbits=0&expire=1768914599&sparams=ip,ipbits,expire,v,ei,caps,opi,xoaf&signature=E93C13F2E512B61142D85E105FA51C9C455748B8.3C6A19EDE1D97C47948DDA4225367CAFA3399623&key=yt8&lang=en&name=DTVCC1 (Caused by ResponseError('too many 429 error responses'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRetryError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m ytt_api = YouTubeTranscriptApi(\n\u001b[32m      2\u001b[39m     proxy_config=WebshareProxyConfig(\n\u001b[32m      3\u001b[39m         proxy_username = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mPROXY_USER\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      4\u001b[39m         proxy_password = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mPROXY_PASS\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m     )\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m transcript = \u001b[43mytt_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwN2s5uGh1YQ\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m transcript\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_api.py:73\u001b[39m, in \u001b[36mYouTubeTranscriptApi.fetch\u001b[39m\u001b[34m(self, video_id, languages, preserve_formatting)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch\u001b[39m(\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     53\u001b[39m     video_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     54\u001b[39m     languages: Iterable[\u001b[38;5;28mstr\u001b[39m] = (\u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m,),\n\u001b[32m     55\u001b[39m     preserve_formatting: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     56\u001b[39m ) -> FetchedTranscript:\n\u001b[32m     57\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m    Retrieves the transcript for a single video. This is just a shortcut for\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    calling:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \u001b[33;03m    :param preserve_formatting: whether to keep select HTML text formatting\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_transcript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreserve_formatting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_formatting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:137\u001b[39m, in \u001b[36mTranscript.fetch\u001b[39m\u001b[34m(self, preserve_formatting)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m&exp=xpe\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._url:\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PoTokenRequired(\u001b[38;5;28mself\u001b[39m.video_id)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_http_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m snippets = _TranscriptParser(preserve_formatting=preserve_formatting).parse(\n\u001b[32m    139\u001b[39m     _raise_http_errors(response, \u001b[38;5;28mself\u001b[39m.video_id).text,\n\u001b[32m    140\u001b[39m )\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m FetchedTranscript(\n\u001b[32m    142\u001b[39m     snippets=snippets,\n\u001b[32m    143\u001b[39m     video_id=\u001b[38;5;28mself\u001b[39m.video_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    146\u001b[39m     is_generated=\u001b[38;5;28mself\u001b[39m.is_generated,\n\u001b[32m    147\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\adapters.py:668\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    665\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeout(e, request=request)\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, ResponseError):\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request=request)\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _ProxyError):\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request=request)\n",
      "\u001b[31mRetryError\u001b[39m: HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: /api/timedtext?v=wN2s5uGh1YQ&ei=NxxvaenoEKSWsfIP3eO44QE&caps=asr&opi=112496729&xoaf=5&xowf=1&xospf=1&hl=en&ip=0.0.0.0&ipbits=0&expire=1768914599&sparams=ip,ipbits,expire,v,ei,caps,opi,xoaf&signature=E93C13F2E512B61142D85E105FA51C9C455748B8.3C6A19EDE1D97C47948DDA4225367CAFA3399623&key=yt8&lang=en&name=DTVCC1 (Caused by ResponseError('too many 429 error responses'))"
     ]
    }
   ],
   "source": [
    "ytt_api = YouTubeTranscriptApi(\n",
    "    proxy_config=WebshareProxyConfig(\n",
    "        proxy_username = os.getenv(\"PROXY_USER\"),\n",
    "        proxy_password = os.getenv(\"PROXY_PASS\"),\n",
    "    )\n",
    ")\n",
    "transcript = ytt_api.fetch('wN2s5uGh1YQ')\n",
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c0f03",
   "metadata": {},
   "source": [
    "### Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c444da",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPT_CACHE_PATH = Path('daily_transcripts.json')\n",
    "MAX_CACHE_SIZE = 100  # Maximum number of videos to keep in cache\n",
    "\n",
    "def load_transcript_cache(path):\n",
    "    if path.exists():\n",
    "        try:\n",
    "            content = path.read_text(encoding='utf-8')\n",
    "            if content.strip():\n",
    "                return json.loads(content)\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  Cache file is empty, starting fresh\")\n",
    "                return {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è  Cache file is corrupted: {e}\")\n",
    "            print(\"   Starting with fresh cache\")\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_transcript_cache(path, cache):\n",
    "    try:\n",
    "        path.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Failed to save cache: {e}\")\n",
    "\n",
    "def fetch_transcript_with_backoff(video_id, max_retries=3):\n",
    "    \"\"\"\n",
    "    Fetch transcript with exponential backoff and jitter.\n",
    "    No proxy - relies on longer delays to avoid rate limits.\n",
    "    \"\"\"\n",
    "    base_delay = 5.0  # Longer initial delay without proxy\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            # Add random delay before each request (rate limit avoidance)\n",
    "            jitter = random.uniform(2, 5)\n",
    "            if attempt > 1:\n",
    "                time.sleep(jitter)\n",
    "            ytt_api = YouTubeTranscriptApi(\n",
    "                proxy_config=WebshareProxyConfig(\n",
    "                    proxy_username = os.getenv(\"PROXY_USER\"),\n",
    "                    proxy_password = os.getenv(\"PROXY_PASS\"),\n",
    "                )\n",
    "            )\n",
    "            transcript = ytt_api.fetch(video_id)\n",
    "            return ' '.join([seg.text for seg in transcript])\n",
    "            \n",
    "        except (TranscriptsDisabled, NoTranscriptFound):\n",
    "            # These are not rate limits, just unavailable transcripts\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            \n",
    "            # Check for rate limit indicators\n",
    "            if any(indicator in error_msg for indicator in ['429', 'too many requests', 'rate limit', 'forbidden', '403']):\n",
    "                wait_time = base_delay * (2 ** (attempt - 1)) + random.uniform(5, 15)\n",
    "                print(f\"‚ö†Ô∏è  Rate limit detected (attempt {attempt}/{max_retries})\")\n",
    "                print(f\"   Waiting {wait_time:.1f}s before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            # Other errors\n",
    "            print(f\"Attempt {attempt} failed for {video_id}: {type(e).__name__}: {str(e)[:100]}\")\n",
    "            if attempt == max_retries:\n",
    "                return None\n",
    "            \n",
    "            # Exponential backoff for other errors\n",
    "            wait_time = base_delay * (1.5 ** attempt) + random.uniform(1, 3)\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def attach_transcripts(videos, cache_path=TRANSCRIPT_CACHE_PATH, max_cache_size=MAX_CACHE_SIZE, delay_between_requests=3.0):\n",
    "    \"\"\"\n",
    "    Attach transcripts to videos with aggressive rate limit avoidance.\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video dictionaries\n",
    "        cache_path: Path to cache file\n",
    "        max_cache_size: Maximum number of videos to keep in cache\n",
    "        delay_between_requests: Base delay between requests in seconds (default: 3.0)\n",
    "    \"\"\"\n",
    "    latest_ids = [v.get('video_id') for v in videos if v.get('video_id')]\n",
    "    total_videos = len(latest_ids)\n",
    "    print(f\"\\nüìù Processing {total_videos} videos for transcripts...\\n\")\n",
    "    print(f\"‚è±Ô∏è  Using delays to avoid rate limits (no proxy)\\n\")\n",
    "\n",
    "    # Load cache\n",
    "    cache = load_transcript_cache(cache_path)\n",
    "    old_cache_size = len(cache)\n",
    "    \n",
    "    # Create ordered list: newest videos first\n",
    "    all_video_ids = latest_ids.copy()\n",
    "    \n",
    "    # Add old cached videos that aren't in the new list\n",
    "    for old_vid in cache.keys():\n",
    "        if old_vid not in all_video_ids:\n",
    "            all_video_ids.append(old_vid)\n",
    "    \n",
    "    # Keep only the newest MAX_CACHE_SIZE videos\n",
    "    videos_to_keep = all_video_ids[:max_cache_size]\n",
    "    \n",
    "    # Filter cache\n",
    "    filtered_cache = {vid: cache[vid] for vid in videos_to_keep if vid in cache}\n",
    "    removed_count = old_cache_size - len(filtered_cache)\n",
    "    \n",
    "    print(f\"üì¶ Cache status: {old_cache_size} total ‚Üí keeping {len(filtered_cache)} (removed {removed_count} oldest)\\n\")\n",
    "    \n",
    "    cache = filtered_cache\n",
    "\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    cached_count = 0\n",
    "    actual_idx = 0\n",
    "\n",
    "    for idx, video in enumerate(videos, start=1):\n",
    "        vid = video.get('video_id')\n",
    "        if not vid:\n",
    "            continue\n",
    "        \n",
    "        actual_idx = idx\n",
    "        \n",
    "        # Check cache first\n",
    "        if vid in cache:\n",
    "            video['transcript_text'] = cache[vid]\n",
    "            cached_count += 1\n",
    "            print(f\"[{idx}/{total_videos}] ‚úì Cached: {vid} - {video.get('title', 'N/A')[:50]}\")\n",
    "            if cache[vid]:\n",
    "                print(f\"  Preview: {cache[vid][:150]}...\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Add delay between requests to avoid rate limits\n",
    "        delay = delay_between_requests + random.uniform(1, 3)\n",
    "        print(f\"[{idx}/{total_videos}] Fetching: {vid} (waiting {delay:.1f}s)...\")\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        # Fetch transcript\n",
    "        try:\n",
    "            transcript_text = fetch_transcript_with_backoff(vid)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Unexpected error: {e}\")\n",
    "            transcript_text = None\n",
    "        \n",
    "        video['transcript_text'] = transcript_text\n",
    "        cache[vid] = transcript_text\n",
    "        \n",
    "        if transcript_text:\n",
    "            success_count += 1\n",
    "            print(f\"‚úì Success: {video.get('title', 'N/A')[:50]}\")\n",
    "            print(f\"  Preview: {transcript_text[:150]}...\\n\")\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            print(f\"‚úó Failed/No transcript: {video.get('title', 'N/A')[:50]}\\n\")\n",
    "        \n",
    "        # Save cache periodically\n",
    "        if idx % 10 == 0:\n",
    "            save_transcript_cache(cache_path, cache)\n",
    "            print(f\"  üíæ Cache saved at {idx} videos\\n\")\n",
    "\n",
    "    # Final save\n",
    "    save_transcript_cache(cache_path, cache)\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  ‚úì Successfully fetched: {success_count}\")\n",
    "    print(f\"  ‚úì From cache: {cached_count}\")\n",
    "    print(f\"  ‚úó Failed/No transcript: {failed_count}\")\n",
    "    print(f\"  Total processed: {actual_idx}/{total_videos}\")\n",
    "    print(f\"  üì¶ Final cache size: {len(cache)}/{max_cache_size}\")\n",
    "    print()\n",
    "    \n",
    "    return videos\n",
    "\n",
    "def refresh_transcripts_in_dict(videos, cache_path=Path('daily_transcripts.json')):\n",
    "    \"\"\"Refresh transcript data from cache file\"\"\"\n",
    "    if not cache_path.exists():\n",
    "        return videos\n",
    "    cache = json.loads(cache_path.read_text(encoding='utf-8'))\n",
    "    updated = 0\n",
    "    for video in videos:\n",
    "        vid = video.get('video_id')\n",
    "        if not vid:\n",
    "            continue\n",
    "        cached_value = cache.get(vid)\n",
    "        if cached_value is not None:\n",
    "            if video.get('transcript_text') != cached_value:\n",
    "                video['transcript_text'] = cached_value\n",
    "                updated += 1\n",
    "    print(f'Overwrote {updated} transcripts from cache')\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8575db6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_videos_api = fetch_youtube_videos_with_api(CHANNEL_ID, YOUTUBE_API_KEY, max_results=MAX_VIDEOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_videos_api = attach_transcripts(youtube_videos_api, delay_between_requests = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a08c10",
   "metadata": {},
   "source": [
    "## Add transcripts to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite in-memory transcripts with cache values when available\n",
    "\n",
    "youtube_videos_api = refresh_transcripts_in_dict(youtube_videos_api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d0e87",
   "metadata": {},
   "source": [
    "## Check rotating proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"PROXY_USER\")\n",
    "password = os.getenv(\"PROXY_PASS\")\n",
    "endpoint = os.getenv(\"PROXY_HOST\") +  \":\" + str(os.getenv(\"PROXY_PORT\"))\n",
    "\n",
    "proxy_url = f\"http://{username}:{password}@{endpoint}\"\n",
    "\n",
    "def check_ip_rotation(num_requests=10):\n",
    "    \"\"\"Check if proxy IPs are rotating\"\"\"\n",
    "    proxies = {\n",
    "        'http': proxy_url,\n",
    "        'https': proxy_url\n",
    "    }\n",
    "    \n",
    "    ips = []\n",
    "    for i in range(num_requests):\n",
    "        try:\n",
    "            # Using http instead of https for simpler testing\n",
    "            response = requests.get('http://ipinfo.io/json', \n",
    "                                   proxies=proxies, \n",
    "                                   timeout=10)\n",
    "            ip = response.json().get('ip')\n",
    "            ips.append(ip)\n",
    "            print(f\"Request {i+1}: IP = {ip}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Request {i+1} failed: {e}\")\n",
    "    \n",
    "    unique_ips = set(ips)\n",
    "    print(f\"\\nTotal requests: {len(ips)}\")\n",
    "    print(f\"Unique IPs: {len(unique_ips)}\")\n",
    "    print(f\"IPs are {'ROTATING ‚úì' if len(unique_ips) > 1 else 'NOT ROTATING ‚úó'}\")\n",
    "    return ips\n",
    "\n",
    "# Test rotation\n",
    "print(\"Testing IP rotation with Webshare:\")\n",
    "check_ip_rotation(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff8479",
   "metadata": {},
   "source": [
    "# After Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17316fe",
   "metadata": {},
   "source": [
    "## Sentiment and Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "def chunk_text_words(text, chunk_words=500):\n",
    "    words = text.split()\n",
    "    return [\n",
    "        \" \".join(words[i:i+chunk_words])\n",
    "        for i in range(0, len(words), chunk_words)\n",
    "    ]\n",
    "\n",
    "def summarize_long_text(text):\n",
    "    chunks = chunk_text_words(text, chunk_words=500)\n",
    "\n",
    "    partial_summaries = []\n",
    "    for chunk in chunks:\n",
    "        result = summarizer(\n",
    "            chunk,\n",
    "            max_length=120,\n",
    "            min_length=40,\n",
    "            do_sample=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        partial_summaries.append(result[0][\"summary_text\"])\n",
    "\n",
    "    combined = \" \".join(partial_summaries)\n",
    "\n",
    "    final = summarizer(\n",
    "        combined,\n",
    "        max_length=180,\n",
    "        min_length=60,\n",
    "        do_sample=False,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return final[0][\"summary_text\"]\n",
    "\n",
    "\n",
    "def analyze_video_sentiment(video, debug=False):\n",
    "    \"\"\"Analyze sentiment of title and transcript separately\"\"\"\n",
    "    \n",
    "    # Title: Direct sentiment (no summarization)\n",
    "    title = video.get('title', '')\n",
    "    if title:\n",
    "        try:\n",
    "            title_sentiment = sentiment_analyzer(title[:512])[0]\n",
    "            video['title_sentiment'] = title_sentiment\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"    Title sentiment failed: {e}\")\n",
    "            video['title_sentiment'] = None\n",
    "    else:\n",
    "        video['title_sentiment'] = None\n",
    "    \n",
    "    # Transcript: Summarize ‚Üí Sentiment\n",
    "    transcript_text = video.get('transcript_text', '')\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   Transcript length: {len(transcript_text)} chars, {len(transcript_text.split())} words\")\n",
    "    \n",
    "    # Check if transcript exists and is long enough\n",
    "    if not transcript_text or len(transcript_text.strip()) < 200:\n",
    "        if debug:\n",
    "            print(f\"    Transcript too short or missing\")\n",
    "        video['transcript_summary'] = None\n",
    "        video['transcript_sentiment'] = None\n",
    "        return video\n",
    "    \n",
    "    try:\n",
    "        # Clean and truncate transcript\n",
    "        transcript_text = transcript_text.strip()\n",
    "        words = transcript_text.split()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Word count: {len(words)}\")\n",
    "        \n",
    "        # BART works best with 100-1024 tokens\n",
    "        if len(words) < 100:\n",
    "            if debug:\n",
    "                print(f\"    Too few words: {len(words)}\")\n",
    "            video['transcript_summary'] = None\n",
    "            video['transcript_sentiment'] = None\n",
    "            return video\n",
    "        \n",
    "        if len(words) > 1000:\n",
    "            transcript_text = ' '.join(words[:1000])\n",
    "            if debug:\n",
    "                print(f\"    Truncated to 1000 words\")\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Generating summary...\")\n",
    "            print(f\"   First 200 chars: {transcript_text[:200]}\")\n",
    "        \n",
    "        # Generate summary with better parameters\n",
    "\n",
    "        summary = summarize_long_text(transcript_text)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Summary: {summary}\")\n",
    "        \n",
    "        # Sentiment of summary\n",
    "        transcript_sentiment = sentiment_analyzer(summary[:512])[0]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Sentiment: {transcript_sentiment}\")\n",
    "        \n",
    "        video['transcript_summary'] = summary\n",
    "        video['transcript_sentiment'] = transcript_sentiment\n",
    "        \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"   Error: {type(e).__name__}: {str(e)}\")\n",
    "        video['transcript_summary'] = None\n",
    "        try:\n",
    "            video['transcript_sentiment'] = sentiment_analyzer(transcript_text[:512])[0]\n",
    "        except Exception:\n",
    "            video['transcript_sentiment'] = None\n",
    "    \n",
    "    return video\n",
    "\n",
    "# Test on first video with debug output\n",
    "print(\"\\nTesting first video with debug output:\\n\")\n",
    "if youtube_videos_api:\n",
    "    test_video = youtube_videos_api[0].copy()\n",
    "    print(f\"Title: {test_video.get('title')}\")\n",
    "    analyze_video_sentiment(test_video, debug=True)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Ask user if they want to continue\n",
    "response = input(\"Continue with all videos? (y/n): \")\n",
    "\n",
    "if response.lower() == 'y':\n",
    "    print(\"\\n Analyzing all videos...\")\n",
    "    \n",
    "    for video in tqdm(youtube_videos_api, desc=\"Processing videos\", unit=\"video\"):\n",
    "        if video.get('transcript_text'):\n",
    "            analyze_video_sentiment(video, debug=False)\n",
    "        else:\n",
    "            video['title_sentiment'] = None\n",
    "            video['transcript_summary'] = None\n",
    "            video['transcript_sentiment'] = None\n",
    "    \n",
    "    print(\"Analysis complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(videos, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(videos, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# usage\n",
    "save_to_json(youtube_videos_api, \"youtube_analysis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac76c1",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_entity(name: str) -> str:\n",
    "    if not name:\n",
    "        return name\n",
    "\n",
    "    n = name.strip().lower()\n",
    "\n",
    "    n = re.sub(r\"^(the|a|an)\\s+\", \"\", n)\n",
    "    n = re.sub(r\"[^\\w\\s]\", \"\", n)\n",
    "    n = re.sub(r\"\\s+\", \" \", n)\n",
    "\n",
    "    if n in ENTITY_ALIASES:\n",
    "        return ENTITY_ALIASES[n]\n",
    "\n",
    "    return n.upper() if n.isupper() else n.title()\n",
    "\n",
    "def sentiment_to_score(sentiment):\n",
    "    if not sentiment:\n",
    "        return None\n",
    "    label = str(sentiment.get('label', '')).upper()\n",
    "    score = float(sentiment.get('score', 0))\n",
    "    if 'POS' in label:\n",
    "        return score\n",
    "    if 'NEG' in label:\n",
    "        return -score\n",
    "    return 0.0\n",
    "\n",
    "def extract_video_text(video, prefer_summary=True):\n",
    "    title = video.get('title', '')\n",
    "    transcript = ''\n",
    "    if prefer_summary and video.get('transcript_summary'):\n",
    "        transcript = video['transcript_summary']\n",
    "    elif video.get('transcript_text'):\n",
    "        transcript = video['transcript_text']\n",
    "    combined = f\"{title} {transcript}\".strip()\n",
    "    return combined\n",
    "\n",
    "def analyze_video_entities_split(video):\n",
    "    title = video.get('title', '') or ''\n",
    "\n",
    "    # Prefer summary, but fall back to full transcript_text if no summary\n",
    "    raw_summary = video.get('transcript_summary') or video.get('transcript_text') or ''\n",
    "    summary = raw_summary\n",
    "\n",
    "    title_doc = nlp(title) if title else None\n",
    "    summary_doc = nlp(summary) if summary else None\n",
    "\n",
    "    title_tickers = set(get_tickers(title)) if title else set()\n",
    "    title_companies = set(get_companies(title_doc)) if title_doc else set()\n",
    "    title_sectors = set(get_sectors(title.lower())) if title else set()\n",
    "\n",
    "    summary_tickers = set(get_tickers(summary)) if summary else set()\n",
    "    summary_companies = set(get_companies(summary_doc)) if summary_doc else set()\n",
    "    summary_sectors = set(get_sectors(summary.lower())) if summary else set()\n",
    "\n",
    "    title_score = sentiment_to_score(video.get('title_sentiment'))\n",
    "    summary_score = sentiment_to_score(video.get('transcript_sentiment'))\n",
    "\n",
    "    return {\n",
    "        \"title\": (title_tickers, title_companies, title_sectors, title_score),\n",
    "        \"summary\": (summary_tickers, summary_companies, summary_sectors, summary_score),\n",
    "    }\n",
    "\n",
    "def aggregate_youtube_entities(videos):\n",
    "\n",
    "    def new_bucket():\n",
    "        return {\n",
    "            \"title_mentions\": 0,\n",
    "            \"title_scores\": [],  # One score per video where entity appears in title\n",
    "            \"summary_mentions\": 0,\n",
    "            \"summary_scores\": [],  # One score per video where entity appears in summary\n",
    "        }\n",
    "\n",
    "    stock_stats = defaultdict(new_bucket)\n",
    "    company_stats = defaultdict(new_bucket)\n",
    "    sector_stats = defaultdict(new_bucket)\n",
    "\n",
    "    for video in videos:\n",
    "        parts = analyze_video_entities_split(video)\n",
    "\n",
    "        # Track which entities we've already counted for this video (per part)\n",
    "        # to avoid adding the same score multiple times\n",
    "        title_entities_seen = set()\n",
    "        summary_entities_seen = set()\n",
    "\n",
    "        for part_name, (tickers, companies, sectors, score) in parts.items():\n",
    "            is_title = (part_name == \"title\")\n",
    "            seen_set = title_entities_seen if is_title else summary_entities_seen\n",
    "\n",
    "            for t in tickers:\n",
    "                t = normalize_entity(t)\n",
    "                stock_stats[t][f\"{part_name}_mentions\"] += 1\n",
    "                # Only add score once per video per part\n",
    "                if t not in seen_set and score is not None:\n",
    "                    stock_stats[t][f\"{part_name}_scores\"].append(score)\n",
    "                    seen_set.add(t)\n",
    "\n",
    "            for c in companies:\n",
    "                c = normalize_entity(c)\n",
    "                company_stats[c][f\"{part_name}_mentions\"] += 1\n",
    "                # Only add score once per video per part\n",
    "                if c not in seen_set and score is not None:\n",
    "                    company_stats[c][f\"{part_name}_scores\"].append(score)\n",
    "                    seen_set.add(c)\n",
    "\n",
    "            for s in sectors:\n",
    "                s = normalize_entity(s)\n",
    "                sector_stats[s][f\"{part_name}_mentions\"] += 1\n",
    "                # Only add score once per video per part\n",
    "                if s not in seen_set and score is not None:\n",
    "                    sector_stats[s][f\"{part_name}_scores\"].append(score)\n",
    "                    seen_set.add(s)\n",
    "\n",
    "    def finalize(stats):\n",
    "        rows = []\n",
    "        for name, data in stats.items():\n",
    "            rows.append({\n",
    "                \"name\": name,\n",
    "\n",
    "                \"title_mentions\": data[\"title_mentions\"],\n",
    "                \"avg_title_sentiment\": (\n",
    "                    sum(data[\"title_scores\"]) / len(data[\"title_scores\"])\n",
    "                    if data[\"title_scores\"] else None\n",
    "                ),\n",
    "\n",
    "                \"summary_mentions\": data[\"summary_mentions\"],\n",
    "                \"avg_summary_sentiment\": (\n",
    "                    sum(data[\"summary_scores\"]) / len(data[\"summary_scores\"])\n",
    "                    if data[\"summary_scores\"] else None\n",
    "                ),\n",
    "            })\n",
    "\n",
    "        rows.sort(key=lambda x: (x[\"title_mentions\"] + x[\"summary_mentions\"]), reverse=True)\n",
    "        return rows\n",
    "\n",
    "    return {\n",
    "        \"stocks\": finalize(stock_stats),\n",
    "        \"companies\": finalize(company_stats),\n",
    "        \"sectors\": finalize(sector_stats),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = aggregate_youtube_entities(youtube_videos_api)\n",
    "save_to_json(result, \"entity_mentions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['companies']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665887a",
   "metadata": {},
   "source": [
    "## Turn Mentions into readable txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sentiment(score):\n",
    "    \"\"\"Format sentiment score for display.\"\"\"\n",
    "    if score is None:\n",
    "        return \"N/A\"\n",
    "    return f\"{score:+.4f}\"\n",
    "\n",
    "def format_mentions(item):\n",
    "    \"\"\"Format a single item's mention data.\"\"\"\n",
    "    total_mentions = item.get(\"title_mentions\", 0) + item.get(\"summary_mentions\", 0)\n",
    "    title_sent = format_sentiment(item.get(\"avg_title_sentiment\"))\n",
    "    summary_sent = format_sentiment(item.get(\"avg_summary_sentiment\"))\n",
    "    \n",
    "    lines = [\n",
    "        f\"  Name: {item['name']}\",\n",
    "        f\"  Total Mentions: {total_mentions}\",\n",
    "        f\"    - Title Mentions: {item.get('title_mentions', 0)} (Sentiment: {title_sent})\",\n",
    "        f\"    - Summary Mentions: {item.get('summary_mentions', 0)} (Sentiment: {summary_sent})\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = Path(\"entity_mentions.json\")\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build the output text\n",
    "output_lines = []\n",
    "output_lines.append(\"=\" * 80)\n",
    "output_lines.append(\"ENTITY MENTIONS REPORT\")\n",
    "output_lines.append(\"=\" * 80)\n",
    "output_lines.append(\"\")\n",
    "\n",
    "# Stocks Section\n",
    "output_lines.append(\"STOCKS\")\n",
    "output_lines.append(\"-\" * 80)\n",
    "if data.get(\"stocks\"):\n",
    "    for i, stock in enumerate(data[\"stocks\"], 1):\n",
    "        output_lines.append(f\"\\n{i}. {format_mentions(stock)}\")\n",
    "else:\n",
    "    output_lines.append(\"  No stocks found.\")\n",
    "output_lines.append(\"\")\n",
    "output_lines.append(\"\")\n",
    "\n",
    "# Companies Section\n",
    "output_lines.append(\"COMPANIES\")\n",
    "output_lines.append(\"-\" * 80)\n",
    "if data.get(\"companies\"):\n",
    "    for i, company in enumerate(data[\"companies\"], 1):\n",
    "        output_lines.append(f\"\\n{i}. {format_mentions(company)}\")\n",
    "else:\n",
    "    output_lines.append(\"  No companies found.\")\n",
    "output_lines.append(\"\")\n",
    "output_lines.append(\"\")\n",
    "\n",
    "# Sectors Section\n",
    "output_lines.append(\"SECTORS\")\n",
    "output_lines.append(\"-\" * 80)\n",
    "if data.get(\"sectors\"):\n",
    "    for i, sector in enumerate(data[\"sectors\"], 1):\n",
    "        output_lines.append(f\"\\n{i}. {format_mentions(sector)}\")\n",
    "else:\n",
    "    output_lines.append(\"  No sectors found.\")\n",
    "\n",
    "output_lines.append(\"\")\n",
    "output_lines.append(\"=\" * 80)\n",
    "\n",
    "# Join all lines\n",
    "output_text = \"\\n\".join(output_lines)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"entity_mentions.txt\")\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(output_text)\n",
    "\n",
    "print(f\"Successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def28e16",
   "metadata": {},
   "source": [
    "# Filter by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('100vids.json', 'r', encoding='utf-8') as file:\n",
    "    # Use json.load() to convert the file content to a Python object\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def filter_by_date_range(videos, chosen_date_str):\n",
    "    \"\"\"\n",
    "    chosen_date_str format: 'YYYY-MM-DD'\n",
    "    \"\"\"\n",
    "    chosen_date = datetime.strptime(chosen_date_str, \"%Y-%m-%d\")\n",
    "    start_date = chosen_date - timedelta(days=7)\n",
    "\n",
    "    filtered = []\n",
    "\n",
    "    for v in videos:\n",
    "        published_str = v.get(\"published\") or v.get(\"published_date\")\n",
    "        if not published_str:\n",
    "            continue\n",
    "\n",
    "        published_dt = datetime.fromisoformat(published_str.replace(\"Z\", \"\"))\n",
    "\n",
    "        if start_date <= published_dt <= chosen_date:\n",
    "            filtered.append(v)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c1bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a705a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_date = \"2026-01-15\"\n",
    "\n",
    "filtered_videos = filter_by_date_range(data, chosen_date)\n",
    "filtered_videos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
