{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d7d2e4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f6f3f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8632f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\News_Majority\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "import trafilatura\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# import io\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "from youtube_transcript_api.proxies import WebshareProxyConfig\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbf278",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d868a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Installing spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cd40c",
   "metadata": {},
   "source": [
    "## Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb502e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    tokenizer=\"ProsusAI/finbert\"\n",
    ")\n",
    "\n",
    "def get_text_sentiment_score(text: str, max_chars=512) -> float:\n",
    "    if not text or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    chunks = [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n",
    "    scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        result = sentiment_analyzer(chunk)[0]\n",
    "        label = result[\"label\"].upper()\n",
    "        score = float(result[\"score\"])\n",
    "\n",
    "        if \"POS\" in label:\n",
    "            scores.append(score)\n",
    "        elif \"NEG\" in label:\n",
    "            scores.append(-score)\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return sum(scores) / len(scores) if scores else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b072542",
   "metadata": {},
   "source": [
    "## Entity Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b44139",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SENTENCES = 1\n",
    "TICKER_LIST_PATH = Path(\"tickers.csv\")  # optional: columns ticker,name\n",
    "\n",
    "TICKER_RE = re.compile(r\"(?<![A-Z])\\$?[A-Z]{2,5}(?![A-Z])\")\n",
    "TICKER_STOP = {\n",
    "    \"A\", \"AN\", \"AND\", \"ARE\", \"AS\", \"AT\", \"BE\", \"BUT\", \"BY\", \"CAN\", \"CO\", \"FOR\",\n",
    "    \"FROM\", \"HAS\", \"HAVE\", \"IN\", \"IS\", \"IT\", \"ITS\", \"NOT\", \"OF\", \"ON\", \"OR\",\n",
    "    \"THE\", \"TO\", \"WAS\", \"WERE\", \"WILL\", \"WITH\",\n",
    "}\n",
    "\n",
    "ENTITY_ALIASES = {\n",
    "    # companies\n",
    "    \"meta\": \"META\",\n",
    "    \"facebook\": \"META\",\n",
    "\n",
    "    \"google\": \"GOOGL\",\n",
    "    \"alphabet\": \"GOOGL\",\n",
    "\n",
    "    \"apple\": \"AAPL\",\n",
    "    \"amazon\": \"AMZN\",\n",
    "    \"microsoft\": \"MSFT\",\n",
    "\n",
    "    # institutions\n",
    "    \"fed\": \"Federal Reserve\",\n",
    "    \"federal reserve\": \"Federal Reserve\",\n",
    "    \"doj\": \"Department of Justice\",\n",
    "    \"department of justice\": \"Department of Justice\",\n",
    "    \"supreme court\": \"Supreme Court\",\n",
    "    \"cnn\": \"CNN\",\n",
    "}\n",
    "\n",
    "SECTOR_KEYWORDS = {\n",
    "    \"Technology\": [\"tech\", \"software\", \"technology\", \"cloud\", \"ai\", \"artificial intelligence\",\n",
    "                   \"chip\", \"semiconductor\", \"digital\", \"platform\", \"app\", \"data\", \"cyber\"],\n",
    "    \"Finance\": [\"bank\", \"financial\", \"finance\", \"investment\", \"trading\", \"market\",\n",
    "                \"stock\", \"equity\", \"bond\", \"credit\", \"lending\", \"mortgage\"],\n",
    "    \"Healthcare\": [\"health\", \"medical\", \"pharmaceutical\", \"drug\", \"biotech\", \"hospital\",\n",
    "                    \"treatment\", \"patient\", \"fda\", \"clinical\", \"therapy\"],\n",
    "    \"Energy\": [\"oil\", \"gas\", \"energy\", \"petroleum\", \"renewable\", \"solar\", \"wind\",\n",
    "               \"electric\", \"power\", \"fuel\", \"drilling\", \"crude\"],\n",
    "    \"Retail\": [\"retail\", \"store\", \"shopping\", \"consumer\", \"e-commerce\", \"online shopping\",\n",
    "               \"merchandise\", \"sales\", \"retailer\"],\n",
    "    \"Automotive\": [\"car\", \"automotive\", \"vehicle\", \"auto\", \"truck\", \"electric vehicle\",\n",
    "                   \"ev\", \"manufacturing\", \"tesla\"],\n",
    "    \"Real Estate\": [\"real estate\", \"property\", \"housing\", \"construction\", \"mortgage\",\n",
    "                    \"development\", \"reit\"],\n",
    "    \"Telecommunications\": [\"telecom\", \"communication\", \"wireless\", \"5g\", \"network\", \"internet\"],\n",
    "    \"Aerospace\": [\"aerospace\", \"aircraft\", \"defense\", \"boeing\", \"space\"],\n",
    "    \"Consumer Goods\": [\"consumer goods\", \"packaged goods\", \"cpg\"],\n",
    "}\n",
    "\n",
    "def normalize_company_name(name):\n",
    "    return name.lower().replace(\"inc.\", \"\").replace(\"corp.\", \"\").replace(\"corporation\", \"\").strip()\n",
    "\n",
    "def extract_article_text(url: str) -> str | None:\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    if not downloaded:\n",
    "        return None\n",
    "\n",
    "    text = trafilatura.extract(\n",
    "        downloaded,\n",
    "        include_comments=False,\n",
    "        include_tables=False,\n",
    "        include_formatting=False\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def load_ticker_map(path: Path):\n",
    "    ticker_to_name = {}\n",
    "    name_to_ticker = {}\n",
    "    if not path.exists():\n",
    "        return ticker_to_name, name_to_ticker\n",
    "\n",
    "    with path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            ticker = (row.get(\"ticker\") or \"\").strip().upper()\n",
    "            name = (row.get(\"name\") or \"\").strip()\n",
    "            if not ticker or not name:\n",
    "                continue\n",
    "            ticker_to_name[ticker] = name\n",
    "            name_to_ticker[normalize_company_name(name)] = ticker\n",
    "\n",
    "    return ticker_to_name, name_to_ticker\n",
    "\n",
    "\n",
    "ticker_to_name, name_to_ticker = load_ticker_map(TICKER_LIST_PATH)\n",
    "\n",
    "\n",
    "def fetch_articles(feed_url, max_items=30):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    articles = []\n",
    "    for entry in feed.entries[:max_items]:\n",
    "        text = extract_article_text(entry.link)\n",
    "        if not text:\n",
    "            continue\n",
    "        articles.append({\n",
    "            \"title\": entry.title,\n",
    "            \"url\": entry.link,\n",
    "            \"published\": entry.get(\"published\"),\n",
    "            \"text\": text,\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_tickers(text):\n",
    "    tickers = set()\n",
    "    for m in TICKER_RE.findall(text):\n",
    "        t = m.replace(\"$\", \"\").upper()\n",
    "        if t in TICKER_STOP:\n",
    "            continue\n",
    "        if ticker_to_name and t not in ticker_to_name:\n",
    "            continue\n",
    "        tickers.add(t)\n",
    "\n",
    "    return list(tickers)\n",
    "\n",
    "def get_companies(doc):\n",
    "    mapped = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ != \"ORG\":\n",
    "            continue\n",
    "        key = normalize_company_name(ent.text)\n",
    "        if key in name_to_ticker:\n",
    "            mapped.append(name_to_ticker[key])   # return ticker\n",
    "        else:\n",
    "            mapped.append(ent.text)\n",
    "    return mapped\n",
    "\n",
    "\n",
    "def get_sectors(text_lower):\n",
    "    return [\n",
    "        sector for sector, keywords in SECTOR_KEYWORDS.items()\n",
    "        if any(kw in text_lower for kw in keywords)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ce0b7",
   "metadata": {},
   "source": [
    "## Youtube Data Api Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6f0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "YOUTUBE_API_KEY = os.getenv(\"API_KEY\")\n",
    "CHANNEL_ID = \"UCrp_UI8XtuYfpiqluWLD7Lw\"  # CNBC channel\n",
    "MAX_VIDEOS = 100\n",
    "\n",
    "def fetch_youtube_videos_with_api(channel_id, api_key, max_results=100):\n",
    "    \"\"\"Fetch YouTube videos using Data API (no transcripts needed)\"\"\"\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    uploads_playlist_id = None\n",
    "    \n",
    "    print(f\"Fetching videos from channel {channel_id}...\")\n",
    "    \n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            # First, get the uploads playlist ID for the channel\n",
    "            if uploads_playlist_id is None:  # Only need to do this once\n",
    "                channel_response = youtube.channels().list(\n",
    "                    part='contentDetails',\n",
    "                    id=channel_id\n",
    "                ).execute()\n",
    "                \n",
    "                if not channel_response.get('items'):\n",
    "                    print(f\"❌ Channel {channel_id} not found\")\n",
    "                    break\n",
    "                \n",
    "                uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "            \n",
    "            # Get videos from uploads playlist\n",
    "            if uploads_playlist_id:\n",
    "                request = youtube.playlistItems().list(\n",
    "                    part='snippet,contentDetails',\n",
    "                    playlistId=uploads_playlist_id,\n",
    "                    maxResults=min(max_results, max_results - len(videos)),\n",
    "                    pageToken=next_page_token\n",
    "                )\n",
    "            else:\n",
    "                # Fallback: search for videos from channel\n",
    "                request = youtube.search().list(\n",
    "                    part='snippet',\n",
    "                    channelId=channel_id,\n",
    "                    type='video',\n",
    "                    maxResults=min(max_results, max_results - len(videos)),\n",
    "                    pageToken=next_page_token,\n",
    "                    order='date'\n",
    "                )\n",
    "            \n",
    "            response = request.execute()\n",
    "            \n",
    "            # Get video IDs\n",
    "            video_ids = []\n",
    "            for item in response['items']:\n",
    "                if 'contentDetails' in item:\n",
    "                    video_ids.append(item['contentDetails']['videoId'])\n",
    "                elif 'id' in item and 'videoId' in item['id']:\n",
    "                    video_ids.append(item['id']['videoId'])\n",
    "            \n",
    "            # Get detailed video information\n",
    "            if video_ids:\n",
    "                video_details = youtube.videos().list(\n",
    "                    part='snippet,statistics',\n",
    "                    id=','.join(video_ids)\n",
    "                ).execute()\n",
    "                \n",
    "                for item in video_details['items']:\n",
    "                    snippet = item['snippet']\n",
    "                    videos.append({\n",
    "                        'title': snippet.get('title', ''),\n",
    "                        'video_id': item['id'],\n",
    "                        'url': f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                        'published': snippet.get('publishedAt', ''),\n",
    "                        'published_date': snippet.get('publishedAt', ''),\n",
    "                        'author': snippet.get('channelTitle', ''),\n",
    "                        'summary': snippet.get('description', ''),  # Full description\n",
    "                        'transcript_text': None,  # No transcript (IP banned)\n",
    "                        'view_count': item['statistics'].get('viewCount', 0),\n",
    "                        'like_count': item['statistics'].get('likeCount', 0),\n",
    "                    })\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "                \n",
    "            print(f\"  Fetched {len(videos)} videos so far...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching videos: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"✅ Total videos fetched: {len(videos)}\")\n",
    "    return videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293213d",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfcd3d",
   "metadata": {},
   "source": [
    "## Retrieve latest 100 videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d85a61",
   "metadata": {},
   "source": [
    "## Retrieve and cache Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de69bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: /watch?v=TnXhn4Io1gs (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 407 Proxy Authentication Required')))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:773\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m, SocketTimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:1042\u001b[39m, in \u001b[36mHTTPSConnectionPool._prepare_proxy\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1036\u001b[39m conn.set_tunnel(\n\u001b[32m   1037\u001b[39m     scheme=tunnel_scheme,\n\u001b[32m   1038\u001b[39m     host=\u001b[38;5;28mself\u001b[39m._tunnel_host,\n\u001b[32m   1039\u001b[39m     port=\u001b[38;5;28mself\u001b[39m.port,\n\u001b[32m   1040\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m.proxy_headers,\n\u001b[32m   1041\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1042\u001b[39m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connection.py:776\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    774\u001b[39m \u001b[38;5;28mself\u001b[39m._has_connected_to_proxy = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tunnel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[38;5;66;03m# Override the host with the one we're requesting data from.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:1001\u001b[39m, in \u001b[36mHTTPConnection._tunnel\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1000\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTunnel connection failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage.strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1003\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mOSError\u001b[39m: Tunnel connection failed: 407 Proxy Authentication Required",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mProxyError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[31mProxyError\u001b[39m: ('Unable to connect to proxy', OSError('Tunnel connection failed: 407 Proxy Authentication Required'))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:871\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    868\u001b[39m     log.warning(\n\u001b[32m    869\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) after connection broken by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, retries, err, url\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:871\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    868\u001b[39m     log.warning(\n\u001b[32m    869\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) after connection broken by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, retries, err, url\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 871 (7 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:871\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    868\u001b[39m     log.warning(\n\u001b[32m    869\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m) after connection broken by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, retries, err, url\n\u001b[32m    870\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\urllib3\\util\\retry.py:535\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    534\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    537\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: /watch?v=TnXhn4Io1gs (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 407 Proxy Authentication Required')))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mProxyError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m ytt_api = YouTubeTranscriptApi(\n\u001b[32m      2\u001b[39m     proxy_config=WebshareProxyConfig(\n\u001b[32m      3\u001b[39m         proxy_username = \u001b[33m'\u001b[39m\u001b[33mkjfpdvk\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m         proxy_password = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mPROXY_PASS\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m     )\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mytt_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTnXhn4Io1gs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_api.py:71\u001b[39m, in \u001b[36mYouTubeTranscriptApi.fetch\u001b[39m\u001b[34m(self, video_id, languages, preserve_formatting)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch\u001b[39m(\n\u001b[32m     52\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     53\u001b[39m     video_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     54\u001b[39m     languages: Iterable[\u001b[38;5;28mstr\u001b[39m] = (\u001b[33m\"\u001b[39m\u001b[33men\u001b[39m\u001b[33m\"\u001b[39m,),\n\u001b[32m     55\u001b[39m     preserve_formatting: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     56\u001b[39m ) -> FetchedTranscript:\n\u001b[32m     57\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m    Retrieves the transcript for a single video. This is just a shortcut for\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    calling:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \u001b[33;03m    :param preserve_formatting: whether to keep select HTML text formatting\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m         .find_transcript(languages)\n\u001b[32m     73\u001b[39m         .fetch(preserve_formatting=preserve_formatting)\n\u001b[32m     74\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_api.py:127\u001b[39m, in \u001b[36mYouTubeTranscriptApi.list\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlist\u001b[39m(\n\u001b[32m     77\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     78\u001b[39m     video_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m     79\u001b[39m ) -> TranscriptList:\n\u001b[32m     80\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[33;03m    Retrieves the list of transcripts which are available for a given video. It\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03m    returns a `TranscriptList` object which is iterable and provides methods to\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03m        Make sure that this is the actual ID, NOT the full URL to the video!\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:356\u001b[39m, in \u001b[36mTranscriptListFetcher.fetch\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> TranscriptList:\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m TranscriptList.build(\n\u001b[32m    354\u001b[39m         \u001b[38;5;28mself\u001b[39m._http_client,\n\u001b[32m    355\u001b[39m         video_id,\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_captions_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    357\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:361\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_captions_json\u001b[39m\u001b[34m(self, video_id, try_number)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_captions_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m, try_number: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m) -> Dict:\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m         html = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_video_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m         api_key = \u001b[38;5;28mself\u001b[39m._extract_innertube_api_key(html, video_id)\n\u001b[32m    363\u001b[39m         innertube_data = \u001b[38;5;28mself\u001b[39m._fetch_innertube_data(video_id, api_key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:433\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_video_html\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_video_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     html = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33maction=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://consent.youtube.com/s\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m html:\n\u001b[32m    435\u001b[39m         \u001b[38;5;28mself\u001b[39m._create_consent_cookie(html, video_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:442\u001b[39m, in \u001b[36mTranscriptListFetcher._fetch_html\u001b[39m\u001b[34m(self, video_id)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_fetch_html\u001b[39m(\u001b[38;5;28mself\u001b[39m, video_id: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_http_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWATCH_URL\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m unescape(_raise_http_errors(response, video_id).text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\requests\\adapters.py:671\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    668\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RetryError(e, request=request)\n\u001b[32m    670\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _ProxyError):\n\u001b[32m--> \u001b[39m\u001b[32m671\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProxyError(e, request=request)\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    674\u001b[39m     \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    675\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n",
      "\u001b[31mProxyError\u001b[39m: HTTPSConnectionPool(host='www.youtube.com', port=443): Max retries exceeded with url: /watch?v=TnXhn4Io1gs (Caused by ProxyError('Unable to connect to proxy', OSError('Tunnel connection failed: 407 Proxy Authentication Required')))"
     ]
    }
   ],
   "source": [
    "ytt_api = YouTubeTranscriptApi(\n",
    "    proxy_config=WebshareProxyConfig(\n",
    "        proxy_username = os.getenv(\"PROXY_USER\"),\n",
    "        proxy_password = os.getenv(\"PROXY_PASS\"),\n",
    "    )\n",
    ")\n",
    "ytt_api.fetch('TnXhn4Io1gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b93d55d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing proxy rotation...\n",
      "Request 1: IP = 64.137.96.74\n",
      "Request 2: IP = 23.26.71.145\n",
      "Request 3: IP = 23.95.150.145\n",
      "Request 4: IP = 107.172.163.27\n",
      "Request 5: IP = 64.137.96.74\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "# URL-encode credentials\n",
    "proxy_user = quote(os.getenv(\"PROXY_USER\"), safe='')\n",
    "proxy_pass = quote(os.getenv(\"PROXY_PASS\"), safe='')\n",
    "\n",
    "# Set rotating proxy globally\n",
    "proxy_url = f\"http://{proxy_user}:{proxy_pass}@p.webshare.io:80\"\n",
    "os.environ['HTTP_PROXY'] = proxy_url\n",
    "os.environ['HTTPS_PROXY'] = proxy_url\n",
    "\n",
    "# Test rotation by checking your IP multiple times\n",
    "print(\"Testing proxy rotation...\")\n",
    "for i in range(5):\n",
    "    try:\n",
    "        response = requests.get('https://api.ipify.org?format=json', \n",
    "                               proxies={'http': proxy_url, 'https': proxy_url},\n",
    "                               timeout=10)\n",
    "        ip = response.json().get('ip')\n",
    "        print(f\"Request {i+1}: IP = {ip}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Request {i+1}: Error = {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24c444da",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPT_CACHE_PATH = Path('daily_transcripts.json')\n",
    "MAX_CACHE_SIZE = 100  # Maximum number of videos to keep in cache\n",
    "\n",
    "ytt_api = YouTubeTranscriptApi()\n",
    "\n",
    "def load_transcript_cache(path):\n",
    "    if path.exists():\n",
    "        try:\n",
    "            content = path.read_text(encoding='utf-8')\n",
    "            if content.strip():\n",
    "                return json.loads(content)\n",
    "            else:\n",
    "                print(\"⚠️  Cache file is empty, starting fresh\")\n",
    "                return {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"⚠️  Cache file is corrupted: {e}\")\n",
    "            print(\"   Starting with fresh cache\")\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_transcript_cache(path, cache):\n",
    "    try:\n",
    "        path.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Failed to save cache: {e}\")\n",
    "\n",
    "def fetch_transcript_with_backoff(video_id, max_retries=3):\n",
    "    \"\"\"\n",
    "    Fetch transcript with exponential backoff and jitter.\n",
    "    No proxy - relies on longer delays to avoid rate limits.\n",
    "    \"\"\"\n",
    "    base_delay = 5.0  # Longer initial delay without proxy\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            # Add random delay before each request (rate limit avoidance)\n",
    "            jitter = random.uniform(2, 5)\n",
    "            if attempt > 1:\n",
    "                time.sleep(jitter)\n",
    "            \n",
    "            transcript = ytt_api.fetch(video_id)\n",
    "            return ' '.join([seg.text for seg in transcript])\n",
    "            \n",
    "        except (TranscriptsDisabled, NoTranscriptFound):\n",
    "            # These are not rate limits, just unavailable transcripts\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            \n",
    "            # Check for rate limit indicators\n",
    "            if any(indicator in error_msg for indicator in ['429', 'too many requests', 'rate limit', 'forbidden', '403']):\n",
    "                wait_time = base_delay * (2 ** (attempt - 1)) + random.uniform(5, 15)\n",
    "                print(f\"⚠️  Rate limit detected (attempt {attempt}/{max_retries})\")\n",
    "                print(f\"   Waiting {wait_time:.1f}s before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            # Other errors\n",
    "            print(f\"Attempt {attempt} failed for {video_id}: {type(e).__name__}: {str(e)[:100]}\")\n",
    "            if attempt == max_retries:\n",
    "                return None\n",
    "            \n",
    "            # Exponential backoff for other errors\n",
    "            wait_time = base_delay * (1.5 ** attempt) + random.uniform(1, 3)\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def attach_transcripts(videos, cache_path=TRANSCRIPT_CACHE_PATH, max_cache_size=MAX_CACHE_SIZE, delay_between_requests=3.0):\n",
    "    \"\"\"\n",
    "    Attach transcripts to videos with aggressive rate limit avoidance.\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video dictionaries\n",
    "        cache_path: Path to cache file\n",
    "        max_cache_size: Maximum number of videos to keep in cache\n",
    "        delay_between_requests: Base delay between requests in seconds (default: 3.0)\n",
    "    \"\"\"\n",
    "    latest_ids = [v.get('video_id') for v in videos if v.get('video_id')]\n",
    "    total_videos = len(latest_ids)\n",
    "    print(f\"\\n📝 Processing {total_videos} videos for transcripts...\\n\")\n",
    "    print(f\"⏱️  Using delays to avoid rate limits (no proxy)\\n\")\n",
    "\n",
    "    # Load cache\n",
    "    cache = load_transcript_cache(cache_path)\n",
    "    old_cache_size = len(cache)\n",
    "    \n",
    "    # Create ordered list: newest videos first\n",
    "    all_video_ids = latest_ids.copy()\n",
    "    \n",
    "    # Add old cached videos that aren't in the new list\n",
    "    for old_vid in cache.keys():\n",
    "        if old_vid not in all_video_ids:\n",
    "            all_video_ids.append(old_vid)\n",
    "    \n",
    "    # Keep only the newest MAX_CACHE_SIZE videos\n",
    "    videos_to_keep = all_video_ids[:max_cache_size]\n",
    "    \n",
    "    # Filter cache\n",
    "    filtered_cache = {vid: cache[vid] for vid in videos_to_keep if vid in cache}\n",
    "    removed_count = old_cache_size - len(filtered_cache)\n",
    "    \n",
    "    print(f\"📦 Cache status: {old_cache_size} total → keeping {len(filtered_cache)} (removed {removed_count} oldest)\\n\")\n",
    "    \n",
    "    cache = filtered_cache\n",
    "\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    cached_count = 0\n",
    "    actual_idx = 0\n",
    "\n",
    "    for idx, video in enumerate(videos, start=1):\n",
    "        vid = video.get('video_id')\n",
    "        if not vid:\n",
    "            continue\n",
    "        \n",
    "        actual_idx = idx\n",
    "        \n",
    "        # Check cache first\n",
    "        if vid in cache:\n",
    "            video['transcript_text'] = cache[vid]\n",
    "            cached_count += 1\n",
    "            print(f\"[{idx}/{total_videos}] ✓ Cached: {vid} - {video.get('title', 'N/A')[:50]}\")\n",
    "            if cache[vid]:\n",
    "                print(f\"  Preview: {cache[vid][:150]}...\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Add delay between requests to avoid rate limits\n",
    "        delay = delay_between_requests + random.uniform(1, 3)\n",
    "        print(f\"[{idx}/{total_videos}] Fetching: {vid} (waiting {delay:.1f}s)...\")\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        # Fetch transcript\n",
    "        try:\n",
    "            transcript_text = fetch_transcript_with_backoff(vid)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Unexpected error: {e}\")\n",
    "            transcript_text = None\n",
    "        \n",
    "        video['transcript_text'] = transcript_text\n",
    "        cache[vid] = transcript_text\n",
    "        \n",
    "        if transcript_text:\n",
    "            success_count += 1\n",
    "            print(f\"✓ Success: {video.get('title', 'N/A')[:50]}\")\n",
    "            print(f\"  Preview: {transcript_text[:150]}...\\n\")\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            print(f\"✗ Failed/No transcript: {video.get('title', 'N/A')[:50]}\\n\")\n",
    "        \n",
    "        # Save cache periodically\n",
    "        if idx % 10 == 0:\n",
    "            save_transcript_cache(cache_path, cache)\n",
    "            print(f\"  💾 Cache saved at {idx} videos\\n\")\n",
    "\n",
    "    # Final save\n",
    "    save_transcript_cache(cache_path, cache)\n",
    "    \n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"  ✓ Successfully fetched: {success_count}\")\n",
    "    print(f\"  ✓ From cache: {cached_count}\")\n",
    "    print(f\"  ✗ Failed/No transcript: {failed_count}\")\n",
    "    print(f\"  Total processed: {actual_idx}/{total_videos}\")\n",
    "    print(f\"  📦 Final cache size: {len(cache)}/{max_cache_size}\")\n",
    "    print()\n",
    "    \n",
    "    return videos\n",
    "\n",
    "def refresh_transcripts_in_dict(videos, cache_path=Path('daily_transcripts.json')):\n",
    "    \"\"\"Refresh transcript data from cache file\"\"\"\n",
    "    if not cache_path.exists():\n",
    "        return videos\n",
    "    cache = json.loads(cache_path.read_text(encoding='utf-8'))\n",
    "    updated = 0\n",
    "    for video in videos:\n",
    "        vid = video.get('video_id')\n",
    "        if not vid:\n",
    "            continue\n",
    "        cached_value = cache.get(vid)\n",
    "        if cached_value is not None:\n",
    "            if video.get('transcript_text') != cached_value:\n",
    "                video['transcript_text'] = cached_value\n",
    "                updated += 1\n",
    "    print(f'Overwrote {updated} transcripts from cache')\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8575db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching videos from channel UCrp_UI8XtuYfpiqluWLD7Lw...\n",
      "  Fetched 50 videos so far...\n",
      "  Fetched 100 videos so far...\n",
      "✅ Total videos fetched: 100\n"
     ]
    }
   ],
   "source": [
    "youtube_videos_api = fetch_youtube_videos_with_api(CHANNEL_ID, YOUTUBE_API_KEY, max_results=MAX_VIDEOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570f82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Processing 100 videos for transcripts...\n",
      "\n",
      "⏱️  Using delays to avoid rate limits (no proxy)\n",
      "\n",
      "📦 Cache status: 0 total → keeping 0 (removed 0 oldest)\n",
      "\n",
      "[1/100] Fetching: OM9w1pyVOgc (waiting 7.3s)...\n",
      "✓ Success: Lightning Round: Hold on to Texas Instruments, say\n",
      "  Preview: LET'S TALK ABOUT I JUST GOT ANOTHER CALL ON MY PLAN THIS OUT AND THEN THE LIGHTNING ROUND IS OVER. ARE YOU READY, SKI DADDY? START WITH RON GALELLA. R...\n",
      "\n",
      "[2/100] Fetching: gtOCsnNUHiw (waiting 7.4s)...\n",
      "✓ Success: Amgen CEO Bob Bradway goes one-on-one with Jim Cra\n",
      "  Preview: AMGEN AMERITRADE. THEY'RE LATE STAGE ONCE A MONTH. GLP ONE WEIGHT LOSS INJECTION NOW. WALL STREET DIDN'T REACT TO THE NEWS, BUT THIS IS A BIG BIOTECH ...\n",
      "\n",
      "[3/100] Fetching: iXKXFyMt7Gg (waiting 7.2s)...\n",
      "✓ Success: Mad Money 01/13/26 | Audio Only\n",
      "  Preview: Hey, I'm Kramer. Welcome to a special West Coast edition of Bad Money. Welcome to Cray America from One Market in San Francisco. Other people make fri...\n",
      "\n",
      "[4/100] Fetching: TnXhn4Io1gs (waiting 7.9s)...\n",
      "✓ Success: Amgen CEO: There are a bunch of ways to win for pa\n",
      "  Preview: GOING ON HERE. MR. BRADWAY, WELCOME BACK TO MAD MONEY. >> THANK YOU, JIM. GLAD TO BE WITH YOU. >> ALL RIGHT. NOW, FIRST WE JUST GOT TO ADDRESS WHAT PE...\n",
      "\n",
      "[5/100] Fetching: h7o6R-a0esI (waiting 7.2s)...\n",
      "✓ Success: Markets decided if you make software, AI will make\n",
      "  Preview: SPECIAL WEST COAST EDITION OF MAD MONEY. WELCOME TO CRAMERICA FROM ONE MARKET IN SAN FRANCISCO. OTHER PEOPLE MAKE FRIENDS. I'M JUST TRYING TO MAKE A L...\n",
      "\n",
      "[6/100] Fetching: aopBLDFjmVw (waiting 8.0s)...\n",
      "✓ Success: Jim Cramer: When JPMorgan reports, Dimon tends to \n",
      "  Preview: IT ALL IN CONTEXT. SO CALL ME AT ONE 807 43 CNBC OR TWEET ME AT JIM CRAMER. THE MARKET GETS SO ANGRY OVER NOTHING. IT'S JUST PLAIN IRRITABLE. VERY HAR...\n",
      "\n",
      "[7/100] Fetching: XwkMf9kmeIA (waiting 6.9s)...\n",
      "✓ Success: Final Trade: MLCO, JPM, TMUS, FCX\n",
      "  Preview: >> I SHOULD ASK SOME GUY WHAT'S THE M IN TIM GO? >> MELCO. >> WHAT'S MY FINAL TRADE? THANK YOU GUY. >> THANK YOU KAREN. >> YES SO J.P. MORGAN I THOUGH...\n",
      "\n",
      "[8/100] Fetching: LKLwix8huFI (waiting 7.2s)...\n",
      "✓ Success: 'Fast Money' traders Karen Finerman and Tim Seymou\n",
      "  Preview: IT IS TIME TO UNVEIL TWO MORE 2026 TRADER ACRONYMS. TODAY, KAREN FINERMAN AND TIM SEYMOUR LAY OUT THEIR PICKS. BUT BEFORE WE BEGIN, WE THOUGHT WE WOUL...\n",
      "\n",
      "[9/100] Fetching: d_wMXjwttiw (waiting 7.6s)...\n",
      "✓ Success: Incyte CEO Bill Meury: Blood cancer therapy develo\n",
      "  Preview: EXCLUSIVITY OF ITS BEST SELLING BLOOD CANCER DRUG JAKAFI IN 2028 AND IS INCREASING ITS FOCUS ON COMMERCIALIZATION AND R&D TO OFFSET THAT PATENT CLIFF....\n",
      "\n",
      "[10/100] Fetching: hdc6kIFP3kA (waiting 7.6s)...\n",
      "✓ Success: Golub Capital CEO: U.S. middle market earnings and\n",
      "  Preview: BUT I THINK THERE'S SOMETHING ELSE THAT'S ABOUT TO HAPPEN THAT I THINK IS GOING TO CAPTIVATE OUR AUDIENCE. >> WELL, THEN MAYBE. >> MAYBE NOT. >> WELL,...\n",
      "\n",
      "  💾 Cache saved at 10 videos\n",
      "\n",
      "[11/100] Fetching: Z2VT995nlTk (waiting 6.3s)...\n",
      "✓ Success: Pres. Trump says JPMorgan's Dimon is wrong on the \n",
      "  Preview: WITH THAT, THAT THERE'S A LOT OF HEALTH THERE. AND WE'RE GOING TO SEE SOME OF THIS IN THE S&P. >> MEANTIME, PRESIDENT TRUMP JUST MAKING SOME COMMENTS ...\n",
      "\n",
      "[12/100] Fetching: Vj6fwbjRca8 (waiting 7.7s)...\n",
      "✓ Success: 'Fast Money' traders react to Microsoft's response\n",
      "  Preview: SO PROFITABLE, THEY CAN'T BE SEEN AS PUSHING THESE COSTS OFF ONTO INDIVIDUAL COMMUNITIES. >> ALL RIGHT. THANK YOU. EAMON JAVERS FROM WASHINGTON FOR US...\n",
      "\n",
      "[13/100] Fetching: yTO2QSpG9AM (waiting 6.5s)...\n",
      "✓ Success: Gabelli Funds rolls out sports ETF under ticker sy\n",
      "  Preview: POINTS FOR PRO SPORTS. GOOD RATINGS FOR THE NBA TO START ITS NEW MEDIA RIGHTS DEAL, AND THE NFL WITH ITS BEST REGULAR SEASON VIEWERSHIP SINCE 1989. TH...\n",
      "\n",
      "[14/100] Fetching: vepMd0kJSBA (waiting 6.6s)...\n",
      "✓ Success: Piper Sandler's Scott Siefers: Credit card caps wo\n",
      "  Preview: H 200 CHIP EXPORTS TO CHINA. WE DID KNOW THIS WAS COMING. THE PRESIDENT HAS TALKED ABOUT IT BUT IS NOW OFFICIALLY BEEN ENTERED INTO THE FEDERAL REGIST...\n",
      "\n",
      "[15/100] Fetching: ntH7BciW8JE (waiting 7.1s)...\n",
      "⚠️  Rate limit detected (attempt 1/3)\n",
      "   Waiting 16.9s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/3)\n",
      "   Waiting 24.1s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/3)\n",
      "   Waiting 28.1s before retry...\n",
      "✗ Failed/No transcript: Watch CNBC's exclusive interview with Dexcom CEO J\n",
      "\n",
      "[16/100] Fetching: MIX-LjSeGSM (waiting 7.4s)...\n",
      "⚠️  Rate limit detected (attempt 1/3)\n",
      "   Waiting 10.4s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/3)\n",
      "   Waiting 16.8s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/3)\n",
      "   Waiting 31.3s before retry...\n"
     ]
    }
   ],
   "source": [
    "youtube_videos_api = attach_transcripts(youtube_videos_api, delay_between_requests = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a08c10",
   "metadata": {},
   "source": [
    "## Add transcripts to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrote 97 transcripts from cache\n"
     ]
    }
   ],
   "source": [
    "# Overwrite in-memory transcripts with cache values when available\n",
    "\n",
    "youtube_videos_api = refresh_transcripts_in_dict(youtube_videos_api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d0e87",
   "metadata": {},
   "source": [
    "## Check rotating proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing IP rotation with Webshare:\n",
      "Request 1: IP = 23.27.208.120\n",
      "Request 2: IP = 64.137.96.74\n",
      "Request 3: IP = 23.26.71.145\n",
      "Request 4: IP = 142.111.48.253\n",
      "Request 5: IP = 142.111.48.253\n",
      "Request 6: IP = 107.172.163.27\n",
      "Request 7: IP = 142.111.48.253\n",
      "Request 8: IP = 23.26.71.145\n",
      "Request 9: IP = 23.95.150.145\n",
      "Request 10: IP = 64.137.96.74\n",
      "\n",
      "Total requests: 10\n",
      "Unique IPs: 6\n",
      "IPs are ROTATING ✓\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['23.27.208.120',\n",
       " '64.137.96.74',\n",
       " '23.26.71.145',\n",
       " '142.111.48.253',\n",
       " '142.111.48.253',\n",
       " '107.172.163.27',\n",
       " '142.111.48.253',\n",
       " '23.26.71.145',\n",
       " '23.95.150.145',\n",
       " '64.137.96.74']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "username = os.getenv(\"PROXY_USER\")\n",
    "password = os.getenv(\"PROXY_PASS\")\n",
    "endpoint = os.getenv(\"PROXY_HOST\") +  \":\" + str(os.getenv(\"PROXY_PORT\"))\n",
    "\n",
    "proxy_url = f\"http://{username}:{password}@{endpoint}\"\n",
    "\n",
    "def check_ip_rotation(num_requests=10):\n",
    "    \"\"\"Check if proxy IPs are rotating\"\"\"\n",
    "    proxies = {\n",
    "        'http': proxy_url,\n",
    "        'https': proxy_url\n",
    "    }\n",
    "    \n",
    "    ips = []\n",
    "    for i in range(num_requests):\n",
    "        try:\n",
    "            # Using http instead of https for simpler testing\n",
    "            response = requests.get('http://ipinfo.io/json', \n",
    "                                   proxies=proxies, \n",
    "                                   timeout=10)\n",
    "            ip = response.json().get('ip')\n",
    "            ips.append(ip)\n",
    "            print(f\"Request {i+1}: IP = {ip}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Request {i+1} failed: {e}\")\n",
    "    \n",
    "    unique_ips = set(ips)\n",
    "    print(f\"\\nTotal requests: {len(ips)}\")\n",
    "    print(f\"Unique IPs: {len(unique_ips)}\")\n",
    "    print(f\"IPs are {'ROTATING ✓' if len(unique_ips) > 1 else 'NOT ROTATING ✗'}\")\n",
    "    return ips\n",
    "\n",
    "# Test rotation\n",
    "print(\"Testing IP rotation with Webshare:\")\n",
    "check_ip_rotation(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff8479",
   "metadata": {},
   "source": [
    "# After Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17316fe",
   "metadata": {},
   "source": [
    "## Sentiment and Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007c875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing first video with debug output:\n",
      "\n",
      "Title: Lightning Round: Sony is a buy here, says Jim Cramer\n",
      "   Transcript length: 1632 chars, 316 words\n",
      "   Word count: 316\n",
      "   Generating summary...\n",
      "   First 200 chars: STOCK PRESENTATION. MY STAFF PREPARES THE GRAPHICS. WE PLAN THIS OUT AND THEN THE LIGHTNING ROUND IS OVER. ARE YOU READY, SKI! DADDY! LANCE AND MERLIN. LANCE. >> THANK YOU FOR TAKING MY CALL. MR. CRAM\n",
      "   Summary: Jim's stock is down a dollar and a half from his recent high. A company CEO says the stock is undervaluing the company. A five-year-old boy thanks Jim for making his money. The stock market is down 1.7% this year. The Dow Jones Industrial Average is down 0.3%. The S&P 500 is up 0.2%.\n",
      "   Sentiment: {'label': 'negative', 'score': 0.9727199077606201}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      " Analyzing all videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos: 100%|██████████| 100/100 [20:54<00:00, 12.54s/video]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "def chunk_text_words(text, chunk_words=500):\n",
    "    words = text.split()\n",
    "    return [\n",
    "        \" \".join(words[i:i+chunk_words])\n",
    "        for i in range(0, len(words), chunk_words)\n",
    "    ]\n",
    "\n",
    "def summarize_long_text(text):\n",
    "    chunks = chunk_text_words(text, chunk_words=500)\n",
    "\n",
    "    partial_summaries = []\n",
    "    for chunk in chunks:\n",
    "        result = summarizer(\n",
    "            chunk,\n",
    "            max_length=120,\n",
    "            min_length=40,\n",
    "            do_sample=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        partial_summaries.append(result[0][\"summary_text\"])\n",
    "\n",
    "    combined = \" \".join(partial_summaries)\n",
    "\n",
    "    final = summarizer(\n",
    "        combined,\n",
    "        max_length=180,\n",
    "        min_length=60,\n",
    "        do_sample=False,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return final[0][\"summary_text\"]\n",
    "\n",
    "\n",
    "def analyze_video_sentiment(video, debug=False):\n",
    "    \"\"\"Analyze sentiment of title and transcript separately\"\"\"\n",
    "    \n",
    "    # Title: Direct sentiment (no summarization)\n",
    "    title = video.get('title', '')\n",
    "    if title:\n",
    "        try:\n",
    "            title_sentiment = sentiment_analyzer(title[:512])[0]\n",
    "            video['title_sentiment'] = title_sentiment\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"    Title sentiment failed: {e}\")\n",
    "            video['title_sentiment'] = None\n",
    "    else:\n",
    "        video['title_sentiment'] = None\n",
    "    \n",
    "    # Transcript: Summarize → Sentiment\n",
    "    transcript_text = video.get('transcript_text', '')\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   Transcript length: {len(transcript_text)} chars, {len(transcript_text.split())} words\")\n",
    "    \n",
    "    # Check if transcript exists and is long enough\n",
    "    if not transcript_text or len(transcript_text.strip()) < 200:\n",
    "        if debug:\n",
    "            print(f\"    Transcript too short or missing\")\n",
    "        video['transcript_summary'] = None\n",
    "        video['transcript_sentiment'] = None\n",
    "        return video\n",
    "    \n",
    "    try:\n",
    "        # Clean and truncate transcript\n",
    "        transcript_text = transcript_text.strip()\n",
    "        words = transcript_text.split()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Word count: {len(words)}\")\n",
    "        \n",
    "        # BART works best with 100-1024 tokens\n",
    "        if len(words) < 100:\n",
    "            if debug:\n",
    "                print(f\"    Too few words: {len(words)}\")\n",
    "            video['transcript_summary'] = None\n",
    "            video['transcript_sentiment'] = None\n",
    "            return video\n",
    "        \n",
    "        if len(words) > 1000:\n",
    "            transcript_text = ' '.join(words[:1000])\n",
    "            if debug:\n",
    "                print(f\"    Truncated to 1000 words\")\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Generating summary...\")\n",
    "            print(f\"   First 200 chars: {transcript_text[:200]}\")\n",
    "        \n",
    "        # Generate summary with better parameters\n",
    "\n",
    "        summary = summarize_long_text(transcript_text)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Summary: {summary}\")\n",
    "        \n",
    "        # Sentiment of summary\n",
    "        transcript_sentiment = sentiment_analyzer(summary[:512])[0]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Sentiment: {transcript_sentiment}\")\n",
    "        \n",
    "        video['transcript_summary'] = summary\n",
    "        video['transcript_sentiment'] = transcript_sentiment\n",
    "        \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"   Error: {type(e).__name__}: {str(e)}\")\n",
    "        video['transcript_summary'] = None\n",
    "        try:\n",
    "            video['transcript_sentiment'] = sentiment_analyzer(transcript_text[:512])[0]\n",
    "        except Exception:\n",
    "            video['transcript_sentiment'] = None\n",
    "    \n",
    "    return video\n",
    "\n",
    "# Test on first video with debug output\n",
    "print(\"\\nTesting first video with debug output:\\n\")\n",
    "if youtube_videos_api:\n",
    "    test_video = youtube_videos_api[0].copy()\n",
    "    print(f\"Title: {test_video.get('title')}\")\n",
    "    analyze_video_sentiment(test_video, debug=True)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Ask user if they want to continue\n",
    "response = input(\"Continue with all videos? (y/n): \")\n",
    "\n",
    "if response.lower() == 'y':\n",
    "    print(\"\\n Analyzing all videos...\")\n",
    "    \n",
    "    for video in tqdm(youtube_videos_api, desc=\"Processing videos\", unit=\"video\"):\n",
    "        if video.get('transcript_text'):\n",
    "            analyze_video_sentiment(video, debug=False)\n",
    "        else:\n",
    "            video['title_sentiment'] = None\n",
    "            video['transcript_summary'] = None\n",
    "            video['transcript_sentiment'] = None\n",
    "    \n",
    "    print(\"Analysis complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(videos, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(videos, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# usage\n",
    "save_to_json(youtube_videos_api, \"youtube_analysis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac76c1",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c37da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_entity(name: str) -> str:\n",
    "    if not name:\n",
    "        return name\n",
    "\n",
    "    n = name.strip().lower()\n",
    "\n",
    "    n = re.sub(r\"^(the|a|an)\\s+\", \"\", n)\n",
    "    n = re.sub(r\"[^\\w\\s]\", \"\", n)\n",
    "    n = re.sub(r\"\\s+\", \" \", n)\n",
    "\n",
    "    if n in ENTITY_ALIASES:\n",
    "        return ENTITY_ALIASES[n]\n",
    "\n",
    "    return n.upper() if n.isupper() else n.title()\n",
    "\n",
    "def sentiment_to_score(sentiment):\n",
    "    if not sentiment:\n",
    "        return None\n",
    "    label = str(sentiment.get('label', '')).upper()\n",
    "    score = float(sentiment.get('score', 0))\n",
    "    if 'POS' in label:\n",
    "        return score\n",
    "    if 'NEG' in label:\n",
    "        return -score\n",
    "    return 0.0\n",
    "\n",
    "def extract_video_text(video, prefer_summary=True):\n",
    "    title = video.get('title', '')\n",
    "    transcript = ''\n",
    "    if prefer_summary and video.get('transcript_summary'):\n",
    "        transcript = video['transcript_summary']\n",
    "    elif video.get('transcript_text'):\n",
    "        transcript = video['transcript_text']\n",
    "    combined = f\"{title} {transcript}\".strip()\n",
    "    return combined\n",
    "\n",
    "def analyze_video_entities_split(video):\n",
    "    title = video.get('title', '') or ''\n",
    "    summary = video.get('transcript_summary', '') or ''\n",
    "\n",
    "    title_doc = nlp(title) if title else None\n",
    "    summary_doc = nlp(summary) if summary else None\n",
    "\n",
    "    title_tickers = set(get_tickers(title)) if title else set()\n",
    "    title_companies = set(get_companies(title_doc)) if title_doc else set()\n",
    "    title_sectors = set(get_sectors(title.lower())) if title else set()\n",
    "\n",
    "    summary_tickers = set(get_tickers(summary)) if summary else set()\n",
    "    summary_companies = set(get_companies(summary_doc)) if summary_doc else set()\n",
    "    summary_sectors = set(get_sectors(summary.lower())) if summary else set()\n",
    "\n",
    "    title_score = sentiment_to_score(video.get('title_sentiment'))\n",
    "    summary_score = sentiment_to_score(video.get('transcript_sentiment'))\n",
    "\n",
    "    return {\n",
    "        \"title\": (title_tickers, title_companies, title_sectors, title_score),\n",
    "        \"summary\": (summary_tickers, summary_companies, summary_sectors, summary_score),\n",
    "    }\n",
    "\n",
    "def aggregate_youtube_entities(videos):\n",
    "\n",
    "    def new_bucket():\n",
    "        return {\n",
    "            \"title_mentions\": 0,\n",
    "            \"title_scores\": [],\n",
    "            \"summary_mentions\": 0,\n",
    "            \"summary_scores\": [],\n",
    "        }\n",
    "\n",
    "    stock_stats = defaultdict(new_bucket)\n",
    "    company_stats = defaultdict(new_bucket)\n",
    "    sector_stats = defaultdict(new_bucket)\n",
    "\n",
    "    for video in videos:\n",
    "        parts = analyze_video_entities_split(video)\n",
    "\n",
    "        for part_name, (tickers, companies, sectors, score) in parts.items():\n",
    "\n",
    "            for t in tickers:\n",
    "                t = normalize_entity(t)\n",
    "                stock_stats[t][f\"{part_name}_mentions\"] += 1\n",
    "                if score is not None:\n",
    "                    stock_stats[t][f\"{part_name}_scores\"].append(score)\n",
    "\n",
    "            for c in companies:\n",
    "                c = normalize_entity(c)\n",
    "                company_stats[c][f\"{part_name}_mentions\"] += 1\n",
    "                if score is not None:\n",
    "                    company_stats[c][f\"{part_name}_scores\"].append(score)\n",
    "\n",
    "            for s in sectors:\n",
    "                s = normalize_entity(s)\n",
    "                sector_stats[s][f\"{part_name}_mentions\"] += 1\n",
    "                if score is not None:\n",
    "                    sector_stats[s][f\"{part_name}_scores\"].append(score)\n",
    "\n",
    "    def finalize(stats):\n",
    "        rows = []\n",
    "        for name, data in stats.items():\n",
    "            rows.append({\n",
    "                \"name\": name,\n",
    "\n",
    "                \"title_mentions\": data[\"title_mentions\"],\n",
    "                \"avg_title_sentiment\": (\n",
    "                    sum(data[\"title_scores\"]) / len(data[\"title_scores\"])\n",
    "                    if data[\"title_scores\"] else None\n",
    "                ),\n",
    "\n",
    "                \"summary_mentions\": data[\"summary_mentions\"],\n",
    "                \"avg_summary_sentiment\": (\n",
    "                    sum(data[\"summary_scores\"]) / len(data[\"summary_scores\"])\n",
    "                    if data[\"summary_scores\"] else None\n",
    "                ),\n",
    "            })\n",
    "\n",
    "        rows.sort(key=lambda x: (x[\"title_mentions\"] + x[\"summary_mentions\"]), reverse=True)\n",
    "        return rows\n",
    "\n",
    "    return {\n",
    "        \"stocks\": finalize(stock_stats),\n",
    "        \"companies\": finalize(company_stats),\n",
    "        \"sectors\": finalize(sector_stats),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8984daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = aggregate_youtube_entities(youtube_videos_api)\n",
    "save_to_json(result, \"entity_mentions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7a12a1b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Federal Reserve',\n",
       "  'title_mentions': 19,\n",
       "  'avg_title_sentiment': -0.1264430516295963,\n",
       "  'summary_mentions': 29,\n",
       "  'avg_summary_sentiment': -0.50068156986401},\n",
       " {'name': 'CNN',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 16,\n",
       "  'avg_summary_sentiment': -0.40045169927179813},\n",
       " {'name': 'Trump',\n",
       "  'title_mentions': 8,\n",
       "  'avg_title_sentiment': -0.23264212267739431,\n",
       "  'summary_mentions': 2,\n",
       "  'avg_summary_sentiment': -0.6485797017812729},\n",
       " {'name': 'Department of Justice',\n",
       "  'title_mentions': 2,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 7,\n",
       "  'avg_summary_sentiment': -0.3467265622956412},\n",
       " {'name': 'GOOGL',\n",
       "  'title_mentions': 4,\n",
       "  'avg_title_sentiment': 0.6990521252155304,\n",
       "  'summary_mentions': 5,\n",
       "  'avg_summary_sentiment': 0.11893515586853028},\n",
       " {'name': 'Aapl',\n",
       "  'title_mentions': 3,\n",
       "  'avg_title_sentiment': 0.4568253556887309,\n",
       "  'summary_mentions': 6,\n",
       "  'avg_summary_sentiment': -0.038184781869252525},\n",
       " {'name': 'Cnbc',\n",
       "  'title_mentions': 4,\n",
       "  'avg_title_sentiment': 0.03192700445652008,\n",
       "  'summary_mentions': 3,\n",
       "  'avg_summary_sentiment': -0.49025561412175495},\n",
       " {'name': 'META',\n",
       "  'title_mentions': 2,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 3,\n",
       "  'avg_summary_sentiment': 0.35367051760355633},\n",
       " {'name': 'Dow',\n",
       "  'title_mentions': 2,\n",
       "  'avg_title_sentiment': 0.2811986804008484,\n",
       "  'summary_mentions': 2,\n",
       "  'avg_summary_sentiment': -0.228132963180542},\n",
       " {'name': 'Amzn',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 4,\n",
       "  'avg_summary_sentiment': 0.2119535356760025},\n",
       " {'name': 'White House',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 4,\n",
       "  'avg_summary_sentiment': -0.2299765795469284},\n",
       " {'name': 'Federal Reserves',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 3,\n",
       "  'avg_summary_sentiment': -0.5648639897505442},\n",
       " {'name': 'Sp',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 2,\n",
       "  'avg_summary_sentiment': -0.0807027816772461},\n",
       " {'name': 'Powell',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 2,\n",
       "  'avg_summary_sentiment': -0.8621830344200134},\n",
       " {'name': 'Tcw',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 2,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Dallas Fed',\n",
       "  'title_mentions': 2,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Gmo',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.6692070364952087,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.9483267068862915},\n",
       " {'name': 'Congress',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 2,\n",
       "  'avg_summary_sentiment': -0.48011523485183716},\n",
       " {'name': 'Unitedhealth',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Freddie Mac',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.9266495108604431,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.9688693881034851},\n",
       " {'name': 'Fannie Mae',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.9266495108604431,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.9688693881034851},\n",
       " {'name': 'Supreme Court',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 2,\n",
       "  'avg_summary_sentiment': -0.25880610942840576},\n",
       " {'name': 'Paramount',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.871120274066925,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.4354870617389679},\n",
       " {'name': 'Wmt',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 2,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'State',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.5874197483062744,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Gild',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.9037797451019287,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.5497453808784485},\n",
       " {'name': 'Andrew',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 2,\n",
       "  'avg_summary_sentiment': 0.43049356341362},\n",
       " {'name': 'Sony',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.7237778902053833,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Novo',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Major Barrier',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Nordisk',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Justice Department',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.8044597506523132},\n",
       " {'name': 'Justice Depts',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Justice Dept',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Anf',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Arikayce',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.9064690470695496},\n",
       " {'name': 'Vanic Funds',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Rbc',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.5959663391113281,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Jp Morgan',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Wedbush',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.7584325671195984,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Commodity',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.4934401512145996},\n",
       " {'name': 'Too Long',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.7076756954193115},\n",
       " {'name': 'Capitol Hill',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.6183993220329285,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Fbi',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.7995039224624634},\n",
       " {'name': 'Ai',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.5946757793426514},\n",
       " {'name': 'Institution',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.7953710556030273},\n",
       " {'name': 'Sincere',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.7953710556030273},\n",
       " {'name': 'Oppenheimer',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Bitco',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Sec',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Investment Committees Reaction',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Us Td Cowen',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.7866692543029785},\n",
       " {'name': 'Slow Bank Deregulation',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.7866692543029785},\n",
       " {'name': 'Llm Gemini',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.7159762382507324,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Opec',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.8629179000854492,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Us Interior',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.8629179000854492,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Wind Power Can Alter The Outcome',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.9492429494857788},\n",
       " {'name': 'Seymour Asset Management',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.9531546235084534},\n",
       " {'name': 'Gemini Ai',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.6120434999465942,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Techcheck',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Customer',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Lind Sey Group',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.7468382716178894},\n",
       " {'name': 'Quality',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.9483267068862915},\n",
       " {'name': 'Fhfa',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Vaneck',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Leveraged',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Trow',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.8515549302101135},\n",
       " {'name': 'Senate',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.8515549302101135},\n",
       " {'name': 'Invest',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.8515549302101135},\n",
       " {'name': 'Android',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Kbw',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Pncs Yungyu Ma',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Mortgage News Daily',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.9688693881034851},\n",
       " {'name': 'Worldpay Business Is Very Complementary',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.746510922908783},\n",
       " {'name': 'Un Security Council',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.7778963446617126},\n",
       " {'name': 'Office',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.7034963369369507},\n",
       " {'name': 'Cea',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Successful',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.5926058292388916},\n",
       " {'name': 'Shopify',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'National Retail Federation',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Truist Cio',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.9246731400489807,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Wbd',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': -0.871120274066925,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Warner Brothers',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.4354870617389679},\n",
       " {'name': 'Cramers Stop Trading',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Stupid',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Apple Tv',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Apple Icloud',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Maps',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Eddy',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Mad',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Jpm',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.8237844705581665},\n",
       " {'name': 'Jpmorgan Chase Health Care Conference',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.8237844705581665},\n",
       " {'name': 'Maduro',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Biden Administrations',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.7470796704292297},\n",
       " {'name': 'Ms',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Wired',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Forbes',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'White House National Economic Council',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Nrf Retail Monitor',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.736825168132782,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Gs',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.5935381054878235},\n",
       " {'name': 'Ai Debt Advisor',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Better Home And Finance',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Buy',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Usually Banks',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Klarna',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Credit',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Cnn Techs',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Widlitz Walmart',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.8553714156150818,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Emed',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Nfl',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Federal Deposit Insurance Corporation',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Us Senate',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Nasdaq',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.5039645433425903},\n",
       " {'name': 'C',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.8505191802978516},\n",
       " {'name': 'Jp Morgans',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.8505191802978516},\n",
       " {'name': 'Slb',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Ba',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Spy',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Eli Lilly',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.9251247048377991,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Ipo',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.84781414270401},\n",
       " {'name': 'Bank Of Americas',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Bac',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.86098712682724},\n",
       " {'name': 'Vst',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.86098712682724},\n",
       " {'name': 'Wells Fargos',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.743007242679596,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Wells Fargo Securities',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.440754234790802},\n",
       " {'name': 'Actually Clean',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': -0.440754234790802},\n",
       " {'name': 'Dcla',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Sarat Sethi',\n",
       "  'title_mentions': 1,\n",
       "  'avg_title_sentiment': 0.0,\n",
       "  'summary_mentions': 0,\n",
       "  'avg_summary_sentiment': None},\n",
       " {'name': 'Hecla',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0},\n",
       " {'name': 'Mines',\n",
       "  'title_mentions': 0,\n",
       "  'avg_title_sentiment': None,\n",
       "  'summary_mentions': 1,\n",
       "  'avg_summary_sentiment': 0.0}]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['companies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f717f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
