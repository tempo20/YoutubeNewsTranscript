{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d7d2e4",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f6f3f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8632f9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\News_Majority\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "import trafilatura\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import spacy\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "# import io\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "from youtube_transcript_api.proxies import WebshareProxyConfig, GenericProxyConfig\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cbf278",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d868a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Installing spaCy model...\")\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cd40c",
   "metadata": {},
   "source": [
    "## Sentiment Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb502e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    tokenizer=\"ProsusAI/finbert\"\n",
    ")\n",
    "\n",
    "def get_text_sentiment_score(text: str, max_chars=512) -> float:\n",
    "    if not text or not text.strip():\n",
    "        return 0.0\n",
    "\n",
    "    chunks = [text[i:i+max_chars] for i in range(0, len(text), max_chars)]\n",
    "    scores = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        result = sentiment_analyzer(chunk)[0]\n",
    "        label = result[\"label\"].upper()\n",
    "        score = float(result[\"score\"])\n",
    "\n",
    "        if \"POS\" in label:\n",
    "            scores.append(score)\n",
    "        elif \"NEG\" in label:\n",
    "            scores.append(-score)\n",
    "        else:\n",
    "            scores.append(0.0)\n",
    "\n",
    "    return sum(scores) / len(scores) if scores else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b072542",
   "metadata": {},
   "source": [
    "## Entity Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b44139",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SENTENCES = 1\n",
    "TICKER_LIST_PATH = Path(\"tickers.csv\")  # optional: columns ticker,name\n",
    "\n",
    "TICKER_RE = re.compile(r\"(?<![A-Z])\\$?[A-Z]{2,5}(?![A-Z])\")\n",
    "TICKER_STOP = {\n",
    "    \"A\", \"AN\", \"AND\", \"ARE\", \"AS\", \"AT\", \"BE\", \"BUT\", \"BY\", \"CAN\", \"CO\", \"FOR\",\n",
    "    \"FROM\", \"HAS\", \"HAVE\", \"IN\", \"IS\", \"IT\", \"ITS\", \"NOT\", \"OF\", \"ON\", \"OR\",\n",
    "    \"THE\", \"TO\", \"WAS\", \"WERE\", \"WILL\", \"WITH\",\n",
    "}\n",
    "\n",
    "ENTITY_ALIASES = {\n",
    "    # companies\n",
    "    \"meta\": \"META\",\n",
    "    \"facebook\": \"META\",\n",
    "\n",
    "    \"google\": \"GOOGL\",\n",
    "    \"alphabet\": \"GOOGL\",\n",
    "\n",
    "    \"apple\": \"AAPL\",\n",
    "    \"amazon\": \"AMZN\",\n",
    "    \"microsoft\": \"MSFT\",\n",
    "\n",
    "    # institutions\n",
    "    \"fed\": \"Federal Reserve\",\n",
    "    \"federal reserve\": \"Federal Reserve\",\n",
    "    \"doj\": \"Department of Justice\",\n",
    "    \"department of justice\": \"Department of Justice\",\n",
    "    \"supreme court\": \"Supreme Court\",\n",
    "    \"cnn\": \"CNN\",\n",
    "}\n",
    "\n",
    "SECTOR_KEYWORDS = {\n",
    "    \"Technology\": [\"tech\", \"software\", \"technology\", \"cloud\", \"ai\", \"artificial intelligence\",\n",
    "                   \"chip\", \"semiconductor\", \"digital\", \"platform\", \"app\", \"data\", \"cyber\"],\n",
    "    \"Finance\": [\"bank\", \"financial\", \"finance\", \"investment\", \"trading\", \"market\",\n",
    "                \"stock\", \"equity\", \"bond\", \"credit\", \"lending\", \"mortgage\"],\n",
    "    \"Healthcare\": [\"health\", \"medical\", \"pharmaceutical\", \"drug\", \"biotech\", \"hospital\",\n",
    "                    \"treatment\", \"patient\", \"fda\", \"clinical\", \"therapy\"],\n",
    "    \"Energy\": [\"oil\", \"gas\", \"energy\", \"petroleum\", \"renewable\", \"solar\", \"wind\",\n",
    "               \"electric\", \"power\", \"fuel\", \"drilling\", \"crude\"],\n",
    "    \"Retail\": [\"retail\", \"store\", \"shopping\", \"consumer\", \"e-commerce\", \"online shopping\",\n",
    "               \"merchandise\", \"sales\", \"retailer\"],\n",
    "    \"Automotive\": [\"car\", \"automotive\", \"vehicle\", \"auto\", \"truck\", \"electric vehicle\",\n",
    "                   \"ev\", \"manufacturing\", \"tesla\"],\n",
    "    \"Real Estate\": [\"real estate\", \"property\", \"housing\", \"construction\", \"mortgage\",\n",
    "                    \"development\", \"reit\"],\n",
    "    \"Telecommunications\": [\"telecom\", \"communication\", \"wireless\", \"5g\", \"network\", \"internet\"],\n",
    "    \"Aerospace\": [\"aerospace\", \"aircraft\", \"defense\", \"boeing\", \"space\"],\n",
    "    \"Consumer Goods\": [\"consumer goods\", \"packaged goods\", \"cpg\"],\n",
    "}\n",
    "\n",
    "def normalize_company_name(name):\n",
    "    return name.lower().replace(\"inc.\", \"\").replace(\"corp.\", \"\").replace(\"corporation\", \"\").strip()\n",
    "\n",
    "def extract_article_text(url: str) -> str | None:\n",
    "    downloaded = trafilatura.fetch_url(url)\n",
    "    if not downloaded:\n",
    "        return None\n",
    "\n",
    "    text = trafilatura.extract(\n",
    "        downloaded,\n",
    "        include_comments=False,\n",
    "        include_tables=False,\n",
    "        include_formatting=False\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def load_ticker_map(path: Path):\n",
    "    ticker_to_name = {}\n",
    "    name_to_ticker = {}\n",
    "    if not path.exists():\n",
    "        return ticker_to_name, name_to_ticker\n",
    "\n",
    "    with path.open(\"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            ticker = (row.get(\"ticker\") or \"\").strip().upper()\n",
    "            name = (row.get(\"name\") or \"\").strip()\n",
    "            if not ticker or not name:\n",
    "                continue\n",
    "            ticker_to_name[ticker] = name\n",
    "            name_to_ticker[normalize_company_name(name)] = ticker\n",
    "\n",
    "    return ticker_to_name, name_to_ticker\n",
    "\n",
    "\n",
    "ticker_to_name, name_to_ticker = load_ticker_map(TICKER_LIST_PATH)\n",
    "\n",
    "\n",
    "def fetch_articles(feed_url, max_items=30):\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    articles = []\n",
    "    for entry in feed.entries[:max_items]:\n",
    "        text = extract_article_text(entry.link)\n",
    "        if not text:\n",
    "            continue\n",
    "        articles.append({\n",
    "            \"title\": entry.title,\n",
    "            \"url\": entry.link,\n",
    "            \"published\": entry.get(\"published\"),\n",
    "            \"text\": text,\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "\n",
    "def get_tickers(text):\n",
    "    tickers = set()\n",
    "    for m in TICKER_RE.findall(text):\n",
    "        t = m.replace(\"$\", \"\").upper()\n",
    "        if t in TICKER_STOP:\n",
    "            continue\n",
    "        if ticker_to_name and t not in ticker_to_name:\n",
    "            continue\n",
    "        tickers.add(t)\n",
    "\n",
    "    return list(tickers)\n",
    "\n",
    "def get_companies(doc):\n",
    "    mapped = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ != \"ORG\":\n",
    "            continue\n",
    "        key = normalize_company_name(ent.text)\n",
    "        if key in name_to_ticker:\n",
    "            mapped.append(name_to_ticker[key])   # return ticker\n",
    "        else:\n",
    "            mapped.append(ent.text)\n",
    "    return mapped\n",
    "\n",
    "\n",
    "def get_sectors(text_lower):\n",
    "    return [\n",
    "        sector for sector, keywords in SECTOR_KEYWORDS.items()\n",
    "        if any(kw in text_lower for kw in keywords)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ce0b7",
   "metadata": {},
   "source": [
    "## Youtube Data Api Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6f0685",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "YOUTUBE_API_KEY = os.getenv(\"API_KEY\")\n",
    "CHANNEL_ID = \"UCrp_UI8XtuYfpiqluWLD7Lw\"  # CNBC channel\n",
    "MAX_VIDEOS = 100\n",
    "\n",
    "def fetch_youtube_videos_with_api(channel_id, api_key, max_results=100):\n",
    "    \"\"\"Fetch YouTube videos using Data API (no transcripts needed)\"\"\"\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "    uploads_playlist_id = None\n",
    "    \n",
    "    print(f\"Fetching videos from channel {channel_id}...\")\n",
    "    \n",
    "    while len(videos) < max_results:\n",
    "        try:\n",
    "            # First, get the uploads playlist ID for the channel\n",
    "            if uploads_playlist_id is None:  # Only need to do this once\n",
    "                channel_response = youtube.channels().list(\n",
    "                    part='contentDetails',\n",
    "                    id=channel_id\n",
    "                ).execute()\n",
    "                \n",
    "                if not channel_response.get('items'):\n",
    "                    print(f\"❌ Channel {channel_id} not found\")\n",
    "                    break\n",
    "                \n",
    "                uploads_playlist_id = channel_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "            \n",
    "            # Get videos from uploads playlist\n",
    "            if uploads_playlist_id:\n",
    "                request = youtube.playlistItems().list(\n",
    "                    part='snippet,contentDetails',\n",
    "                    playlistId=uploads_playlist_id,\n",
    "                    maxResults=min(max_results, max_results - len(videos)),\n",
    "                    pageToken=next_page_token\n",
    "                )\n",
    "            else:\n",
    "                # Fallback: search for videos from channel\n",
    "                request = youtube.search().list(\n",
    "                    part='snippet',\n",
    "                    channelId=channel_id,\n",
    "                    type='video',\n",
    "                    maxResults=min(max_results, max_results - len(videos)),\n",
    "                    pageToken=next_page_token,\n",
    "                    order='date'\n",
    "                )\n",
    "            \n",
    "            response = request.execute()\n",
    "            \n",
    "            # Get video IDs\n",
    "            video_ids = []\n",
    "            for item in response['items']:\n",
    "                if 'contentDetails' in item:\n",
    "                    video_ids.append(item['contentDetails']['videoId'])\n",
    "                elif 'id' in item and 'videoId' in item['id']:\n",
    "                    video_ids.append(item['id']['videoId'])\n",
    "            \n",
    "            # Get detailed video information\n",
    "            if video_ids:\n",
    "                video_details = youtube.videos().list(\n",
    "                    part='snippet,statistics',\n",
    "                    id=','.join(video_ids)\n",
    "                ).execute()\n",
    "                \n",
    "                for item in video_details['items']:\n",
    "                    snippet = item['snippet']\n",
    "                    videos.append({\n",
    "                        'title': snippet.get('title', ''),\n",
    "                        'video_id': item['id'],\n",
    "                        'url': f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                        'published': snippet.get('publishedAt', ''),\n",
    "                        'published_date': snippet.get('publishedAt', ''),\n",
    "                        'author': snippet.get('channelTitle', ''),\n",
    "                        'summary': snippet.get('description', ''),  # Full description\n",
    "                        'transcript_text': None,  # No transcript (IP banned)\n",
    "                        'view_count': item['statistics'].get('viewCount', 0),\n",
    "                        'like_count': item['statistics'].get('likeCount', 0),\n",
    "                    })\n",
    "            \n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "                \n",
    "            print(f\"  Fetched {len(videos)} videos so far...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error fetching videos: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"✅ Total videos fetched: {len(videos)}\")\n",
    "    return videos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8293213d",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bfcd3d",
   "metadata": {},
   "source": [
    "## Retrieve latest 100 videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d85a61",
   "metadata": {},
   "source": [
    "## Retrieve and cache Transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c27c6d",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "print(\"HTTP_PROXY =\", os.environ.get(\"HTTP_PROXY\"))\n",
    "print(\"HTTPS_PROXY =\", os.environ.get(\"HTTPS_PROXY\"))\n",
    "print(\"Proxies seen by requests:\", requests.utils.get_environ_proxies(\"https://www.youtube.com\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d55d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "# URL-encode credentials\n",
    "proxy_user = quote(os.getenv(\"PROXY_USER\"), safe='')\n",
    "proxy_pass = quote(os.getenv(\"PROXY_PASS\"), safe='')\n",
    "\n",
    "# Set rotating proxy globally\n",
    "proxy_url = f\"http://{proxy_user}:{proxy_pass}@p.webshare.io:80\"\n",
    "os.environ['HTTP_PROXY'] = proxy_url\n",
    "os.environ['HTTPS_PROXY'] = proxy_url\n",
    "\n",
    "# Test rotation by checking your IP multiple times\n",
    "print(\"Testing proxy rotation...\")\n",
    "for i in range(5):\n",
    "    try:\n",
    "        response = requests.get('https://api.ipify.org?format=json', \n",
    "                               proxies={'http': proxy_url, 'https': proxy_url},\n",
    "                               timeout=10)\n",
    "        ip = response.json().get('ip')\n",
    "        print(f\"Request {i+1}: IP = {ip}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Request {i+1}: Error = {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c3abb11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FetchedTranscript(snippets=[FetchedTranscriptSnippet(text='these LLMs, these foundation models like', start=4.319, duration=3.361), FetchedTranscriptSnippet(text=\"Gemini, they're getting better and\", start=6.24, duration=3.84), FetchedTranscriptSnippet(text='better with each iteration and um we see', start=7.68, duration=4.24), FetchedTranscriptSnippet(text='no end to that. Um but on the other', start=10.08, duration=4.0), FetchedTranscriptSnippet(text=\"hand, to get to full AGI, there's some\", start=11.92, duration=3.679), FetchedTranscriptSnippet(text='missing capabilities still.', start=14.08, duration=5.52), FetchedTranscriptSnippet(text='>> Deis Hosabis, CEO of Google Deep Mind at', start=15.599, duration=6.321), FetchedTranscriptSnippet(text='the World Economic Forum in Davos,', start=19.6, duration=5.12), FetchedTranscriptSnippet(text=\"Switzerland. He runs Google's AI\", start=21.92, duration=5.119), FetchedTranscriptSnippet(text='research lab, which put out for all of', start=24.72, duration=5.04), FetchedTranscriptSnippet(text='us Gemini. different models are getting', start=27.039, duration=3.841), FetchedTranscriptSnippet(text='good at different things. You know,', start=29.76, duration=3.28), FetchedTranscriptSnippet(text='maybe like Claude for Co specifically,', start=30.88, duration=3.679), FetchedTranscriptSnippet(text='uh things like Gemini, amazing for', start=33.04, duration=3.44), FetchedTranscriptSnippet(text='multimodal understanding and you know,', start=34.559, duration=3.601), FetchedTranscriptSnippet(text='image on image generation, things like', start=36.48, duration=2.16), FetchedTranscriptSnippet(text='that.', start=38.16, duration=3.2), FetchedTranscriptSnippet(text='>> Hazabis says there is still a lot left', start=38.64, duration=3.759), FetchedTranscriptSnippet(text='to innovate.', start=41.36, duration=2.64), FetchedTranscriptSnippet(text=\">> They don't do continual learning. They\", start=42.399, duration=3.201), FetchedTranscriptSnippet(text=\"don't have true creativity yet. They\", start=44.0, duration=2.8), FetchedTranscriptSnippet(text=\"don't do long-term planning and\", start=45.6, duration=2.0), FetchedTranscriptSnippet(text='reasoning.', start=46.8, duration=4.4), FetchedTranscriptSnippet(text='>> Plus, how hyped is too hyped. AI is all', start=47.6, duration=6.08), FetchedTranscriptSnippet(text='the rage for investors, but part of the', start=51.2, duration=5.28), FetchedTranscriptSnippet(text='industry could be in a bubble. If you', start=53.68, duration=4.719), FetchedTranscriptSnippet(text='look at things like uh the new hot', start=56.48, duration=3.68), FetchedTranscriptSnippet(text='startups that raising billions of', start=58.399, duration=3.521), FetchedTranscriptSnippet(text='dollars in a seed round with, you know,', start=60.16, duration=3.84), FetchedTranscriptSnippet(text='no product or technology yet, that seems', start=61.92, duration=3.76), FetchedTranscriptSnippet(text='a little bit frothy to me.', start=64.0, duration=4.159), FetchedTranscriptSnippet(text=\">> I'm CNBC producer Cameron Costa.\", start=65.68, duration=5.68), FetchedTranscriptSnippet(text=\"SquawkPod reports from Davos's 2026\", start=68.159, duration=6.561), FetchedTranscriptSnippet(text=\"Google's Deep Mind CEO Demis Hosabes\", start=71.36, duration=7.48), FetchedTranscriptSnippet(text='begins right now.', start=74.72, duration=4.12), FetchedTranscriptSnippet(text='The World Economic Forum in Davos is a', start=84.08, duration=6.0), FetchedTranscriptSnippet(text='marathon few days. Joe Kernan, Becky', start=87.439, duration=4.961), FetchedTranscriptSnippet(text='Quick, and Andrew Ross Orcin are meeting', start=90.08, duration=4.079), FetchedTranscriptSnippet(text='with and interviewing some of the', start=92.4, duration=4.56), FetchedTranscriptSnippet(text=\"world's most influential leaders. And\", start=94.159, duration=6.161), FetchedTranscriptSnippet(text='this year, few are as influential as', start=96.96, duration=6.08), FetchedTranscriptSnippet(text=\"those leaders in AI. You're about to\", start=100.32, duration=6.079), FetchedTranscriptSnippet(text='hear from Deis Hosabes. He co-founded an', start=103.04, duration=6.079), FetchedTranscriptSnippet(text='AI research lab called Deep Mind, which', start=106.399, duration=5.841), FetchedTranscriptSnippet(text='Google acquired about 12 years ago. And', start=109.119, duration=6.081), FetchedTranscriptSnippet(text='just about a decade ago, DeepMind blew', start=112.24, duration=6.159), FetchedTranscriptSnippet(text='every other mind when its Alph Go AI', start=115.2, duration=6.32), FetchedTranscriptSnippet(text=\"model beat the world's best human Go\", start=118.399, duration=5.521), FetchedTranscriptSnippet(text='player. Now, that story is completely', start=121.52, duration=4.72), FetchedTranscriptSnippet(text='amazing and thrilling even 10 years', start=123.92, duration=4.8), FetchedTranscriptSnippet(text='later. I highly recommend you go check', start=126.24, duration=6.28), FetchedTranscriptSnippet(text='it out on YouTube.', start=128.72, duration=3.8), FetchedTranscriptSnippet(text='Nowadays, Hosabast is heading up', start=133.2, duration=4.32), FetchedTranscriptSnippet(text=\"Google's AI strategy at DeepMind. He\", start=135.04, duration=4.64), FetchedTranscriptSnippet(text='pushed out Gemini, the latest version of', start=137.52, duration=4.88), FetchedTranscriptSnippet(text=\"which recently won a deal to run Apple's\", start=139.68, duration=6.4), FetchedTranscriptSnippet(text='Siri intelligence. He is a visionary, a', start=142.4, duration=6.559), FetchedTranscriptSnippet(text=\"longtime builder of the AI that's now\", start=146.08, duration=6.0), FetchedTranscriptSnippet(text='ubiquitous in 2026, and he sat down with', start=148.959, duration=6.321), FetchedTranscriptSnippet(text='Andrew Ross outside of course at the', start=152.08, duration=7.12), FetchedTranscriptSnippet(text='World Economic Forum in Switzerland.', start=155.28, duration=5.84), FetchedTranscriptSnippet(text=\"For the last year and a half, there's\", start=159.2, duration=3.679), FetchedTranscriptSnippet(text='been this battle playing out between uh', start=161.12, duration=4.24), FetchedTranscriptSnippet(text='Open AI and Anthropic and you and I', start=162.879, duration=3.681), FetchedTranscriptSnippet(text='think there was a view that you thought', start=165.36, duration=3.04), FetchedTranscriptSnippet(text='you needed to catch up. I think I think', start=166.56, duration=3.6), FetchedTranscriptSnippet(text='think behind the scenes you you thought', start=168.4, duration=3.199), FetchedTranscriptSnippet(text='you were going to get there, but', start=170.16, duration=3.04), FetchedTranscriptSnippet(text='something happened this year. What do', start=171.599, duration=3.36), FetchedTranscriptSnippet(text='you think it was? Because I think at', start=173.2, duration=4.319), FetchedTranscriptSnippet(text='this moment your model may very well be', start=174.959, duration=4.721), FetchedTranscriptSnippet(text='at the top of the the charts, if you', start=177.519, duration=3.841), FetchedTranscriptSnippet(text=\"will. I think it's look it's been a it's\", start=179.68, duration=3.76), FetchedTranscriptSnippet(text=\"been a it's been a a kind of long\", start=181.36, duration=4.159), FetchedTranscriptSnippet(text='journey last couple of years to kind of', start=183.44, duration=4.56), FetchedTranscriptSnippet(text='corral together all of the assets that', start=185.519, duration=4.401), FetchedTranscriptSnippet(text='that we have as Google and Deep Mind', start=188.0, duration=5.36), FetchedTranscriptSnippet(text='incredible research bench our TPUs um', start=189.92, duration=5.28), FetchedTranscriptSnippet(text=\"and all of the kind of research we've\", start=193.36, duration=3.28), FetchedTranscriptSnippet(text='been doing over the last decade plus', start=195.2, duration=3.2), FetchedTranscriptSnippet(text='really that underpins a lot of the AI', start=196.64, duration=3.36), FetchedTranscriptSnippet(text='industry so we always had all the', start=198.4, duration=3.919), FetchedTranscriptSnippet(text='ingredients amazing product surfaces to', start=200.0, duration=4.4), FetchedTranscriptSnippet(text='plug AI into and I think in the last', start=202.319, duration=3.761), FetchedTranscriptSnippet(text=\"year what people are feeling is we've\", start=204.4, duration=3.28), FetchedTranscriptSnippet(text='kind of organized that all in a really', start=206.08, duration=4.0), FetchedTranscriptSnippet(text='efficient uh efficient Okay. Okay. So,', start=207.68, duration=4.08), FetchedTranscriptSnippet(text='where where are you in the journey', start=210.08, duration=4.079), FetchedTranscriptSnippet(text='though in terms of what comes next about', start=211.76, duration=4.96), FetchedTranscriptSnippet(text='sort of productizing all of this really', start=214.159, duration=4.8), FetchedTranscriptSnippet(text='getting it into all of these different', start=216.72, duration=4.239), FetchedTranscriptSnippet(text='services? When am I going to see it in', start=218.959, duration=3.28), FetchedTranscriptSnippet(text='Gmail for example?', start=220.959, duration=2.881), FetchedTranscriptSnippet(text='>> Yeah. So, look the everything starts', start=222.239, duration=3.041), FetchedTranscriptSnippet(text='first of all with the research and the', start=223.84, duration=3.44), FetchedTranscriptSnippet(text='model quality. So, as you mentioned,', start=225.28, duration=3.679), FetchedTranscriptSnippet(text=\"we're very happy with how our latest\", start=227.28, duration=3.679), FetchedTranscriptSnippet(text='Gemini model, Gemini 3 is working. We', start=228.959, duration=3.121), FetchedTranscriptSnippet(text=\"think it's, you know, it's topping most\", start=230.959, duration=2.321), FetchedTranscriptSnippet(text='of the leaderboards on most of the', start=232.08, duration=3.36), FetchedTranscriptSnippet(text='benchmarks. And um the other thing we', start=233.28, duration=3.84), FetchedTranscriptSnippet(text='worked hard on in the last year is', start=235.44, duration=3.84), FetchedTranscriptSnippet(text='accelerating the infrastructure and you', start=237.12, duration=3.6), FetchedTranscriptSnippet(text='know actually doing a lot of rewriting', start=239.28, duration=3.039), FetchedTranscriptSnippet(text='of the infrastructure to get the model', start=240.72, duration=3.519), FetchedTranscriptSnippet(text='quality as quickly as possible into our', start=242.319, duration=3.521), FetchedTranscriptSnippet(text=\"product services. So you're starting to\", start=244.239, duration=2.64), FetchedTranscriptSnippet(text=\"see that in you know we're\", start=245.84, duration=3.28), FetchedTranscriptSnippet(text='simultaneously shipping in search um', start=246.879, duration=4.321), FetchedTranscriptSnippet(text='obviously in the Gemini app and then uh', start=249.12, duration=3.679), FetchedTranscriptSnippet(text=\"what you'll see this year is appearing\", start=251.2, duration=3.28), FetchedTranscriptSnippet(text='in more and more places across all of', start=252.799, duration=3.28), FetchedTranscriptSnippet(text='our product services especially things', start=254.48, duration=2.479), FetchedTranscriptSnippet(text='like Gmail.', start=256.079, duration=2.961), FetchedTranscriptSnippet(text='>> So Apple has now chosen to work with', start=256.959, duration=4.96), FetchedTranscriptSnippet(text='you. How transformational is that for', start=259.04, duration=4.879), FetchedTranscriptSnippet(text='Gemini and for your what your work is?', start=261.919, duration=4.081), FetchedTranscriptSnippet(text=\">> Well, I think it's a massive uh sort of\", start=263.919, duration=4.241), FetchedTranscriptSnippet(text='vote of confidence in in in in the', start=266.0, duration=3.68), FetchedTranscriptSnippet(text=\"quality of our models. So, we're very\", start=268.16, duration=3.52), FetchedTranscriptSnippet(text='pleased um uh with that partnership.', start=269.68, duration=3.12), FetchedTranscriptSnippet(text=\"Obviously, it's an amazing partnership\", start=271.68, duration=3.28), FetchedTranscriptSnippet(text='for us, very important one. Uh Apple ran', start=272.8, duration=4.16), FetchedTranscriptSnippet(text='a very rigorous evaluation process and', start=274.96, duration=4.16), FetchedTranscriptSnippet(text='you know, Gemini came top of that and uh', start=276.96, duration=3.76), FetchedTranscriptSnippet(text=\"I think that's that's testament to the\", start=279.12, duration=2.48), FetchedTranscriptSnippet(text=\"work that we've we've done.\", start=280.72, duration=2.88), FetchedTranscriptSnippet(text='>> Were you surprised that they decided', start=281.6, duration=3.52), FetchedTranscriptSnippet(text='they wanted to work with you as opposed', start=283.6, duration=3.2), FetchedTranscriptSnippet(text='to did you think that they would', start=285.12, duration=2.799), FetchedTranscriptSnippet(text='ultimately try to create their own', start=286.8, duration=2.56), FetchedTranscriptSnippet(text='model? Well, you know, creating models', start=287.919, duration=3.041), FetchedTranscriptSnippet(text='cost', start=289.36, duration=3.36), FetchedTranscriptSnippet(text='a lot of a huge amount of resources and', start=290.96, duration=5.2), FetchedTranscriptSnippet(text='money and also um a certain type of uh', start=292.72, duration=4.88), FetchedTranscriptSnippet(text='research base, you know, in terms of', start=296.16, duration=3.599), FetchedTranscriptSnippet(text='like the research teams and so on. So, I', start=297.6, duration=4.8), FetchedTranscriptSnippet(text='think um it makes sense now uh these m', start=299.759, duration=4.641), FetchedTranscriptSnippet(text=\"these models are mature. Uh there's a\", start=302.4, duration=3.6), FetchedTranscriptSnippet(text='kind of ferocious battle going on on the', start=304.4, duration=4.079), FetchedTranscriptSnippet(text=\"frontier um to make if you're a product\", start=306.0, duration=5.039), FetchedTranscriptSnippet(text='company to make use of those uh models', start=308.479, duration=4.401), FetchedTranscriptSnippet(text='uh and and then downstream do amazing', start=311.039, duration=3.281), FetchedTranscriptSnippet(text='things with it. You used to use the word', start=312.88, duration=4.08), FetchedTranscriptSnippet(text='interesting word mature that the models', start=314.32, duration=4.159), FetchedTranscriptSnippet(text='are mature.', start=316.96, duration=3.84), FetchedTranscriptSnippet(text='>> When people talk about getting to AGI', start=318.479, duration=4.481), FetchedTranscriptSnippet(text='and sort of how quickly all of this can', start=320.8, duration=4.08), FetchedTranscriptSnippet(text=\"scale there has been there's sort of two\", start=322.96, duration=4.64), FetchedTranscriptSnippet(text='views. One is if you just throw more', start=324.88, duration=5.28), FetchedTranscriptSnippet(text='compute processing power at this we can', start=327.6, duration=4.24), FetchedTranscriptSnippet(text=\"get there. There's another view that\", start=330.16, duration=3.84), FetchedTranscriptSnippet(text='there needs to be some kind of really', start=331.84, duration=4.96), FetchedTranscriptSnippet(text='breakthrough uh scientific', start=334.0, duration=5.36), FetchedTranscriptSnippet(text='research shift. Which is it? Well,', start=336.8, duration=4.8), FetchedTranscriptSnippet(text='actually I have a kind of um uh in', start=339.36, duration=3.92), FetchedTranscriptSnippet(text=\"between view which is that I think it's\", start=341.6, duration=4.4), FetchedTranscriptSnippet(text='an empirical question. So I feel like um', start=343.28, duration=4.24), FetchedTranscriptSnippet(text=\"we're still getting lots of amazing\", start=346.0, duration=2.96), FetchedTranscriptSnippet(text='gains out of pushing the existing', start=347.52, duration=3.28), FetchedTranscriptSnippet(text='paradigms. These LLMs, these foundation', start=348.96, duration=3.44), FetchedTranscriptSnippet(text=\"models like Gemini, they're getting\", start=350.8, duration=3.04), FetchedTranscriptSnippet(text='better and better with each iteration', start=352.4, duration=4.4), FetchedTranscriptSnippet(text='and um we see no end to that. Um but on', start=353.84, duration=4.96), FetchedTranscriptSnippet(text='the other hand, to get to full AGI,', start=356.8, duration=3.6), FetchedTranscriptSnippet(text=\"there's some missing capabilities still.\", start=358.8, duration=3.76), FetchedTranscriptSnippet(text=\"Um they don't do continual learning. Um\", start=360.4, duration=3.76), FetchedTranscriptSnippet(text=\"they can't they don't have true\", start=362.56, duration=3.28), FetchedTranscriptSnippet(text=\"creativity yet. They don't do long-term\", start=364.16, duration=3.599), FetchedTranscriptSnippet(text=\"planning and reasoning. So it's a\", start=365.84, duration=4.0), FetchedTranscriptSnippet(text='question mark about are some a couple', start=367.759, duration=4.0), FetchedTranscriptSnippet(text='more maybe a handful you know two or', start=369.84, duration=3.76), FetchedTranscriptSnippet(text='three new big breakthroughs needed in', start=371.759, duration=3.921), FetchedTranscriptSnippet(text='order to get to all the way to AGI or', start=373.6, duration=3.76), FetchedTranscriptSnippet(text='will scaling up the existing techniques', start=375.68, duration=3.359), FetchedTranscriptSnippet(text=\"be enough and we're doing both. So we're\", start=377.36, duration=3.839), FetchedTranscriptSnippet(text='in we have the luxury of basically', start=379.039, duration=3.921), FetchedTranscriptSnippet(text='having the deepest and and broadest', start=381.199, duration=3.44), FetchedTranscriptSnippet(text='research bench and we can push both to', start=382.96, duration=2.48), FetchedTranscriptSnippet(text='the maximum.', start=384.639, duration=2.321), FetchedTranscriptSnippet(text='>> How much do you worry that all these', start=385.44, duration=3.68), FetchedTranscriptSnippet(text='large language models ultimately', start=386.96, duration=4.16), FetchedTranscriptSnippet(text='converge meaning become they become very', start=389.12, duration=4.0), FetchedTranscriptSnippet(text='similar in terms of what they can all do', start=391.12, duration=4.56), FetchedTranscriptSnippet(text='and then what differentiates them? Yeah,', start=393.12, duration=3.84), FetchedTranscriptSnippet(text=\"I don't think we're seeing that. I think\", start=395.68, duration=3.359), FetchedTranscriptSnippet(text='if you look at the last year, um models', start=396.96, duration=3.519), FetchedTranscriptSnippet(text='are actually getting good, different', start=399.039, duration=2.561), FetchedTranscriptSnippet(text='models are getting good at different', start=400.479, duration=2.641), FetchedTranscriptSnippet(text='things. You know, maybe like claude for', start=401.6, duration=3.2), FetchedTranscriptSnippet(text='code specifically, uh things like', start=403.12, duration=3.44), FetchedTranscriptSnippet(text='Gemini, amazing for multimodal', start=404.8, duration=3.679), FetchedTranscriptSnippet(text='understanding and you know, image image', start=406.56, duration=3.6), FetchedTranscriptSnippet(text='generation, things like that. So,', start=408.479, duration=2.961), FetchedTranscriptSnippet(text=\"actually, I think you're seeing quite a\", start=410.16, duration=3.28), FetchedTranscriptSnippet(text='lot of differentiation at the frontier', start=411.44, duration=4.479), FetchedTranscriptSnippet(text='and my my expectation is um that will', start=413.44, duration=4.72), FetchedTranscriptSnippet(text='actually increase uh the gaps in that', start=415.919, duration=3.601), FetchedTranscriptSnippet(text='and the differences between the models', start=418.16, duration=1.92), FetchedTranscriptSnippet(text='this year.', start=419.52, duration=1.84), FetchedTranscriptSnippet(text=\">> Um there's a big question about whether\", start=420.08, duration=2.8), FetchedTranscriptSnippet(text=\"there's an AI bubble. You've heard that\", start=421.36, duration=3.36), FetchedTranscriptSnippet(text='over and over again.', start=422.88, duration=2.96), FetchedTranscriptSnippet(text='Is there a bubble?', start=424.72, duration=3.44), FetchedTranscriptSnippet(text=\">> Well, I don't think it's a binary yes or\", start=425.84, duration=4.88), FetchedTranscriptSnippet(text='no. I think some the industry is big now', start=428.16, duration=5.12), FetchedTranscriptSnippet(text=\"and there's many parts to it. I think um\", start=430.72, duration=5.919), FetchedTranscriptSnippet(text='some parts of the of the AI uh field may', start=433.28, duration=5.199), FetchedTranscriptSnippet(text='be in a bubble. I mean, if you look at', start=436.639, duration=4.481), FetchedTranscriptSnippet(text='things like uh the new hot startups that', start=438.479, duration=4.241), FetchedTranscriptSnippet(text='raising billions of dollars in a seed', start=441.12, duration=3.359), FetchedTranscriptSnippet(text='round with, you know, no product or', start=442.72, duration=3.52), FetchedTranscriptSnippet(text='technology yet, that seems a little bit', start=444.479, duration=3.681), FetchedTranscriptSnippet(text='frothy to me and perhaps unsustainable.', start=446.24, duration=3.359), FetchedTranscriptSnippet(text='But on the other hand, you know, there', start=448.16, duration=3.52), FetchedTranscriptSnippet(text='are lots of amazing use cases going on', start=449.599, duration=3.921), FetchedTranscriptSnippet(text='um and products being used.', start=451.68, duration=3.68), FetchedTranscriptSnippet(text='>> Clearly, Alphabet and Google have', start=453.52, duration=3.6), FetchedTranscriptSnippet(text='remarkable financial backing, but', start=455.36, duration=4.559), FetchedTranscriptSnippet(text=\"there's a lot of uh even big\", start=457.12, duration=4.32), FetchedTranscriptSnippet(text=\"independents, and I'm thinking of\", start=459.919, duration=3.521), FetchedTranscriptSnippet(text=\"anthropic, I'm thinking of of Open AI\", start=461.44, duration=5.039), FetchedTranscriptSnippet(text='that have to continue to uh get new', start=463.44, duration=7.599), FetchedTranscriptSnippet(text='capital to really meet some of the um', start=466.479, duration=7.041), FetchedTranscriptSnippet(text=\"spending that that's necessary to get to\", start=471.039, duration=4.481), FetchedTranscriptSnippet(text='the next place. Do you worry about', start=473.52, duration=3.44), FetchedTranscriptSnippet(text=\"whether they'll be able to get there?\", start=475.52, duration=2.959), FetchedTranscriptSnippet(text='Well, I think I mean I mostly worry', start=476.96, duration=3.679), FetchedTranscriptSnippet(text='about what we need to do and and um', start=478.479, duration=3.601), FetchedTranscriptSnippet(text='going back to your bubble question, I', start=480.639, duration=3.041), FetchedTranscriptSnippet(text='think my job is as as head of Google', start=482.08, duration=4.08), FetchedTranscriptSnippet(text='DeepMind is to make sure whatever', start=483.68, duration=4.239), FetchedTranscriptSnippet(text='happens whether there is a bubble and it', start=486.16, duration=3.28), FetchedTranscriptSnippet(text='bursts or whether, you know, the', start=487.919, duration=3.12), FetchedTranscriptSnippet(text='trajectory continues of, you know, the', start=489.44, duration=3.599), FetchedTranscriptSnippet(text='the bullcase continues from here, we', start=491.039, duration=3.521), FetchedTranscriptSnippet(text=\"we're in the, you know, a winning\", start=493.039, duration=3.041), FetchedTranscriptSnippet(text='position and and doing really well. And', start=494.56, duration=3.199), FetchedTranscriptSnippet(text='I think I think because of our balance', start=496.08, duration=3.119), FetchedTranscriptSnippet(text='sheet and all of the existing amazing', start=497.759, duration=3.12), FetchedTranscriptSnippet(text='products we have that are natural fits', start=499.199, duration=3.521), FetchedTranscriptSnippet(text='for AI, you mentioned, you know, email', start=500.879, duration=3.44), FetchedTranscriptSnippet(text=\"and there's Chrome and there's, you\", start=502.72, duration=3.039), FetchedTranscriptSnippet(text=\"know, search of course. So there's all\", start=504.319, duration=3.041), FetchedTranscriptSnippet(text='these amazing uh products billions of', start=505.759, duration=3.681), FetchedTranscriptSnippet(text='people use every day that I think um AI', start=507.36, duration=3.92), FetchedTranscriptSnippet(text=\"can enhance and you know we're we're\", start=509.44, duration=3.279), FetchedTranscriptSnippet(text='only scratching the surface I think of', start=511.28, duration=2.8), FetchedTranscriptSnippet(text=\"what we can do there and there's going\", start=512.719, duration=3.361), FetchedTranscriptSnippet(text='to be a lot more uh I think amazing kind', start=514.08, duration=3.6), FetchedTranscriptSnippet(text='of product features uh this year', start=516.08, duration=3.12), FetchedTranscriptSnippet(text='>> in terms of spending people talk a lot', start=517.68, duration=3.2), FetchedTranscriptSnippet(text='about spending on data centers and the', start=519.2, duration=4.56), FetchedTranscriptSnippet(text='need for new chips when you think about', start=520.88, duration=4.88), FetchedTranscriptSnippet(text='sort of what comes next do you think we', start=523.76, duration=4.72), FetchedTranscriptSnippet(text='could have a a deepse like mind deepseek', start=525.76, duration=5.36), FetchedTranscriptSnippet(text=\"like moment rather where there's some\", start=528.48, duration=4.799), FetchedTranscriptSnippet(text='kind of major technological', start=531.12, duration=4.399), FetchedTranscriptSnippet(text='uh revolution where people say, \"Oh,', start=533.279, duration=3.441), FetchedTranscriptSnippet(text=\"maybe we actually don't need all this\", start=535.519, duration=2.081), FetchedTranscriptSnippet(text='processing power.\"', start=536.72, duration=2.32), FetchedTranscriptSnippet(text=\">> Yeah, I think it's very unlikely. And I\", start=537.6, duration=3.12), FetchedTranscriptSnippet(text='also think the Deep Seek moment was was', start=539.04, duration=3.12), FetchedTranscriptSnippet(text='a little bit overblown in my opinion.', start=540.72, duration=3.119), FetchedTranscriptSnippet(text='Like um they had to utilize some of the', start=542.16, duration=3.44), FetchedTranscriptSnippet(text='Western models in order to to kind of', start=543.839, duration=4.241), FetchedTranscriptSnippet(text='fine-tune against and to train from. Um', start=545.6, duration=3.919), FetchedTranscriptSnippet(text=\"so it wasn't you know that what they\", start=548.08, duration=3.12), FetchedTranscriptSnippet(text='reported this very small training number', start=549.519, duration=4.32), FetchedTranscriptSnippet(text=\"wasn't quite the full picture. Um it is\", start=551.2, duration=4.319), FetchedTranscriptSnippet(text='possible that one of you know these', start=553.839, duration=3.041), FetchedTranscriptSnippet(text='there could be a left field breakthrough', start=555.519, duration=3.521), FetchedTranscriptSnippet(text='that does um increase the efficiency of', start=556.88, duration=3.04), FetchedTranscriptSnippet(text='things. Maybe things like', start=559.04, duration=3.04), FetchedTranscriptSnippet(text='self-improvement um where you can', start=559.92, duration=3.76), FetchedTranscriptSnippet(text=\"imagine there's some cycle which\", start=562.08, duration=4.319), FetchedTranscriptSnippet(text='requires less compute power but for now', start=563.68, duration=4.719), FetchedTranscriptSnippet(text='um you know we think that more compute', start=566.399, duration=3.44), FetchedTranscriptSnippet(text='you need compute for lots of things for', start=568.399, duration=3.44), FetchedTranscriptSnippet(text='training for serving and for exploring', start=569.839, duration=4.321), FetchedTranscriptSnippet(text=\"new ideas um and but of course we're\", start=571.839, duration=4.0), FetchedTranscriptSnippet(text='also all of us are trying to make our', start=574.16, duration=3.2), FetchedTranscriptSnippet(text='models our leading models as efficient', start=575.839, duration=3.68), FetchedTranscriptSnippet(text='as possible so in fact of the Gemini 3', start=577.36, duration=4.159), FetchedTranscriptSnippet(text='models our flash model which is sort of', start=579.519, duration=3.681), FetchedTranscriptSnippet(text='the workhorse model may be the most', start=581.519, duration=3.041), FetchedTranscriptSnippet(text='important because we can deploy that', start=583.2, duration=1.92), FetchedTranscriptSnippet(text='everywhere', start=584.56, duration=1.76), FetchedTranscriptSnippet(text=\">> there's also a big question about the\", start=585.12, duration=3.52), FetchedTranscriptSnippet(text='depreciation schedule of these chips', start=586.32, duration=4.48), FetchedTranscriptSnippet(text='meaning, do they really last for 4', start=588.64, duration=3.759), FetchedTranscriptSnippet(text='years? Do they last for 7 years? And', start=590.8, duration=3.92), FetchedTranscriptSnippet(text=\"it's not so much do they last, are they\", start=592.399, duration=4.241), FetchedTranscriptSnippet(text='useful in the same way that you probably', start=594.72, duration=3.92), FetchedTranscriptSnippet(text='buy a new phone every year or two?', start=596.64, duration=3.12), FetchedTranscriptSnippet(text=\"You're going to want people are going to\", start=598.64, duration=3.199), FetchedTranscriptSnippet(text='want to buy and need the next cutting', start=599.76, duration=4.639), FetchedTranscriptSnippet(text='edge chip. And so, are we laying down', start=601.839, duration=5.761), FetchedTranscriptSnippet(text='um, you know, railroad tracks or are we', start=604.399, duration=4.721), FetchedTranscriptSnippet(text='laying down railroad tracks that need to', start=607.6, duration=3.2), FetchedTranscriptSnippet(text='be replaced every couple years?', start=609.12, duration=2.88), FetchedTranscriptSnippet(text=\">> Yeah. Well, I think that's one of the\", start=610.8, duration=2.96), FetchedTranscriptSnippet(text=\"advantage we have is we're full stack.\", start=612.0, duration=3.44), FetchedTranscriptSnippet(text=\"So I think we're the only organization\", start=613.76, duration=3.84), FetchedTranscriptSnippet(text='really that has a frontier lab and our', start=615.44, duration=4.32), FetchedTranscriptSnippet(text='own chips in TPUs and our cloud', start=617.6, duration=4.56), FetchedTranscriptSnippet(text='business. So um we have a lot of sort of', start=619.76, duration=5.199), FetchedTranscriptSnippet(text='ways of of utilizing any spare compute', start=622.16, duration=5.2), FetchedTranscriptSnippet(text='anywhere on our on our you know on our', start=624.959, duration=4.641), FetchedTranscriptSnippet(text='on our systems and data centers and', start=627.36, duration=4.159), FetchedTranscriptSnippet(text='maybe as compute gets older the older', start=629.6, duration=3.28), FetchedTranscriptSnippet(text='generations you start moving them', start=631.519, duration=3.681), FetchedTranscriptSnippet(text='towards serving or maybe like labeling', start=632.88, duration=5.28), FetchedTranscriptSnippet(text='data for you. So you can always utilize', start=635.2, duration=4.879), FetchedTranscriptSnippet(text='um you know even quite older sets of', start=638.16, duration=3.919), FetchedTranscriptSnippet(text='generations of chips for for useful', start=640.079, duration=4.161), FetchedTranscriptSnippet(text=\"useful work. Uh we've mentioned OpenAI,\", start=642.079, duration=3.841), FetchedTranscriptSnippet(text=\"we've mentioned Anthropic, we have not\", start=644.24, duration=4.159), FetchedTranscriptSnippet(text='mentioned um Meta and we have not', start=645.92, duration=6.159), FetchedTranscriptSnippet(text='mentioned Elon Musk and XAI. Where do', start=648.399, duration=6.88), FetchedTranscriptSnippet(text='they sit in this uh sort of competitive', start=652.079, duration=4.241), FetchedTranscriptSnippet(text='stack for you right now?', start=655.279, duration=1.841), FetchedTranscriptSnippet(text=\">> Well, obviously they're, you know,\", start=656.32, duration=2.079), FetchedTranscriptSnippet(text=\"they're extraordinary companies and\", start=657.12, duration=3.36), FetchedTranscriptSnippet(text=\"they're they're led by very ambitious\", start=658.399, duration=4.801), FetchedTranscriptSnippet(text='and and uh you know um uh aggressive', start=660.48, duration=4.72), FetchedTranscriptSnippet(text=\"leaders and I think um they're working\", start=663.2, duration=3.68), FetchedTranscriptSnippet(text=\"on very interesting things and we'll see\", start=665.2, duration=3.199), FetchedTranscriptSnippet(text='how how what comes out of that this', start=666.88, duration=2.72), FetchedTranscriptSnippet(text='year, especially on Meta. I guess', start=668.399, duration=2.88), FetchedTranscriptSnippet(text=\"they've rebooted their their sort of\", start=669.6, duration=4.0), FetchedTranscriptSnippet(text=\"research division and um you know we're\", start=671.279, duration=3.521), FetchedTranscriptSnippet(text=\"all waiting to see what they're going to\", start=673.6, duration=2.08), FetchedTranscriptSnippet(text='come up with next.', start=674.8, duration=2.159), FetchedTranscriptSnippet(text=\">> But do you think that ultimately there's\", start=675.68, duration=2.719), FetchedTranscriptSnippet(text='going to be one or two or three model', start=676.959, duration=3.601), FetchedTranscriptSnippet(text=\"big models and that's how it's I mean if\", start=678.399, duration=3.601), FetchedTranscriptSnippet(text=\"we're sitting here together 5 years from\", start=680.56, duration=2.88), FetchedTranscriptSnippet(text='now what does this look like to you?', start=682.0, duration=3.2), FetchedTranscriptSnippet(text=\">> I think there's two pictures from here.\", start=683.44, duration=3.28), FetchedTranscriptSnippet(text=\"I think there could be room. It's such\", start=685.2, duration=2.639), FetchedTranscriptSnippet(text='you know AI is going to be so', start=686.72, duration=2.559), FetchedTranscriptSnippet(text=\"transformative and I think it's going to\", start=687.839, duration=3.44), FetchedTranscriptSnippet(text='create so much new opportunities. I', start=689.279, duration=3.601), FetchedTranscriptSnippet(text='think there is plenty of room especially', start=691.279, duration=3.601), FetchedTranscriptSnippet(text='on the enterprise side for you know', start=692.88, duration=5.199), FetchedTranscriptSnippet(text='maybe two three four winners. Um and but', start=694.88, duration=4.639), FetchedTranscriptSnippet(text=\"I think each year it's getting harder\", start=698.079, duration=3.921), FetchedTranscriptSnippet(text='because the pace is ferocious. Um these', start=699.519, duration=3.76), FetchedTranscriptSnippet(text='are very capable companies and', start=702.0, duration=2.88), FetchedTranscriptSnippet(text='organizations and research groups that', start=703.279, duration=3.841), FetchedTranscriptSnippet(text=\"are pushing the frontier and we're all\", start=704.88, duration=4.0), FetchedTranscriptSnippet(text='um pushing that as hard and as fast as', start=707.12, duration=3.6), FetchedTranscriptSnippet(text='possible. You know I work at 100hour', start=708.88, duration=3.12), FetchedTranscriptSnippet(text=\"weeks. I've been doing that for the last\", start=710.72, duration=3.52), FetchedTranscriptSnippet(text='3 years and I see no end to that and I', start=712.0, duration=4.0), FetchedTranscriptSnippet(text='think um all the leading labs are doing', start=714.24, duration=4.24), FetchedTranscriptSnippet(text='that and um so for anyone to catch up to', start=716.0, duration=4.8), FetchedTranscriptSnippet(text=\"the frontier it's progressively a harder\", start=718.48, duration=3.84), FetchedTranscriptSnippet(text=\"harder problem. What's the defensive\", start=720.8, duration=3.039), FetchedTranscriptSnippet(text='moat around your business? And the', start=722.32, duration=3.28), FetchedTranscriptSnippet(text='reason I ask is I always thought that', start=723.839, duration=4.0), FetchedTranscriptSnippet(text='persistent memory, the idea that you', start=725.6, duration=4.32), FetchedTranscriptSnippet(text='would know me potentially better than', start=727.839, duration=4.081), FetchedTranscriptSnippet(text='somebody else would be the thing that', start=729.92, duration=4.32), FetchedTranscriptSnippet(text='would keep me using one model from', start=731.92, duration=4.479), FetchedTranscriptSnippet(text='switching to another. But I just', start=734.24, duration=4.159), FetchedTranscriptSnippet(text='recently learned that actually all of', start=736.399, duration=4.0), FetchedTranscriptSnippet(text='this is actually quite portable. So you', start=738.399, duration=3.68), FetchedTranscriptSnippet(text='could actually take whatever persistent', start=740.399, duration=4.24), FetchedTranscriptSnippet(text='memory Open AI has about me and actually', start=742.079, duration=4.161), FetchedTranscriptSnippet(text='hand it to Gemini.', start=744.639, duration=2.88), FetchedTranscriptSnippet(text='>> Yeah. So look, I think that', start=746.24, duration=2.0), FetchedTranscriptSnippet(text='>> and vice versa.', start=747.519, duration=2.56), FetchedTranscriptSnippet(text='>> Vice versa. Absolutely. I think the key', start=748.24, duration=4.64), FetchedTranscriptSnippet(text='is going to be um the quality of the of', start=750.079, duration=4.481), FetchedTranscriptSnippet(text='the models and their capabilities. I', start=752.88, duration=3.6), FetchedTranscriptSnippet(text=\"think people can feel that when it's\", start=754.56, duration=3.519), FetchedTranscriptSnippet(text=\"there's a differential there and they\", start=756.48, duration=3.52), FetchedTranscriptSnippet(text=\"can it's suddenly more useful for for\", start=758.079, duration=3.681), FetchedTranscriptSnippet(text='whatever use case they have. So I think', start=760.0, duration=3.44), FetchedTranscriptSnippet(text=\"that's going to continue. Um then\", start=761.76, duration=3.44), FetchedTranscriptSnippet(text=\"there's the product features. Um but\", start=763.44, duration=3.199), FetchedTranscriptSnippet(text='also I do think memory and', start=765.2, duration=3.199), FetchedTranscriptSnippet(text='personalization is going to be somewhat', start=766.639, duration=3.44), FetchedTranscriptSnippet(text='sticky and we we launched our first', start=768.399, duration=4.081), FetchedTranscriptSnippet(text='version of personal intelligence uh uh', start=770.079, duration=4.481), FetchedTranscriptSnippet(text='um couple of weeks ago and of course we', start=772.48, duration=3.76), FetchedTranscriptSnippet(text=\"have a whole ecosystem that you're\", start=774.56, duration=3.44), FetchedTranscriptSnippet(text='already using you know Google workspace', start=776.24, duration=4.719), FetchedTranscriptSnippet(text='and email and um and and uh of course', start=778.0, duration=4.56), FetchedTranscriptSnippet(text='search that we can kind of try and plug', start=780.959, duration=3.44), FetchedTranscriptSnippet(text='into obviously with the users permission', start=782.56, duration=3.76), FetchedTranscriptSnippet(text='if they want a more useful personalized', start=784.399, duration=3.921), FetchedTranscriptSnippet(text='assistant experience. a lot of hand', start=786.32, duration=4.24), FetchedTranscriptSnippet(text='ringing around here about jobs', start=788.32, duration=4.48), FetchedTranscriptSnippet(text='>> and what ultimately happens 10 years', start=790.56, duration=3.44), FetchedTranscriptSnippet(text=\"from now, maybe it's five years from\", start=792.8, duration=2.8), FetchedTranscriptSnippet(text=\"now. I don't know if you think even when\", start=794.0, duration=3.12), FetchedTranscriptSnippet(text='you look at employment numbers today', start=795.6, duration=3.6), FetchedTranscriptSnippet(text=\"that uh to the extent that there's any\", start=797.12, duration=5.36), FetchedTranscriptSnippet(text=\"softness, it's a function of AI and\", start=799.2, duration=4.96), FetchedTranscriptSnippet(text='productivity being more successful.', start=802.48, duration=3.039), FetchedTranscriptSnippet(text=\">> Yeah, I think it's early days. I don't\", start=804.16, duration=3.04), FetchedTranscriptSnippet(text=\"think there's any real evidence yet of\", start=805.519, duration=4.081), FetchedTranscriptSnippet(text='any change in the job market. um perhaps', start=807.2, duration=4.079), FetchedTranscriptSnippet(text='a little bit on the entry level maybe', start=809.6, duration=4.239), FetchedTranscriptSnippet(text='jobs and internships but I think um in', start=811.279, duration=4.56), FetchedTranscriptSnippet(text=\"the next five years at least uh it's\", start=813.839, duration=3.521), FetchedTranscriptSnippet(text='going to be more than made up for with', start=815.839, duration=3.12), FetchedTranscriptSnippet(text='extraordinary new opportunities these', start=817.36, duration=3.68), FetchedTranscriptSnippet(text='tools are going to are going to deliver', start=818.959, duration=4.721), FetchedTranscriptSnippet(text='especially for individual creators um', start=821.04, duration=4.56), FetchedTranscriptSnippet(text='you know artists and creators and game', start=823.68, duration=3.52), FetchedTranscriptSnippet(text='designers for example that can create', start=825.6, duration=4.239), FetchedTranscriptSnippet(text='whole uh apps almost on their own. So', start=827.2, duration=4.879), FetchedTranscriptSnippet(text='that should I think if uh if I was to', start=829.839, duration=4.401), FetchedTranscriptSnippet(text='advise uh the youth of today graduates', start=832.079, duration=3.841), FetchedTranscriptSnippet(text='tell the kids is get unbelievably', start=834.24, duration=3.44), FetchedTranscriptSnippet(text='proficient with the new tools immerse', start=835.92, duration=4.0), FetchedTranscriptSnippet(text='yourself become native with it and then', start=837.68, duration=4.159), FetchedTranscriptSnippet(text='leaprog you know um whatever', start=839.92, duration=3.44), FetchedTranscriptSnippet(text=\"professional ladder you're trying to get\", start=841.839, duration=3.68), FetchedTranscriptSnippet(text='onto leaprog the the incumbent people on', start=843.36, duration=4.08), FetchedTranscriptSnippet(text='that with these new skills which are', start=845.519, duration=4.161), FetchedTranscriptSnippet(text='going to change the workplace and I', start=847.44, duration=4.0), FetchedTranscriptSnippet(text=\"think there's going to be plenty of new\", start=849.68, duration=4.0), FetchedTranscriptSnippet(text=\"u um opportunities anytime there's a lot\", start=851.44, duration=3.6), FetchedTranscriptSnippet(text='of disruption', start=853.68, duration=1.68), FetchedTranscriptSnippet(text='>> thank you', start=855.04, duration=3.039), FetchedTranscriptSnippet(text='>> thank you very much', start=855.36, duration=2.719), FetchedTranscriptSnippet(text='Thank you for listening to this special', start=866.88, duration=5.28), FetchedTranscriptSnippet(text='SquawkPod reports from Davos. This is', start=868.959, duration=6.801), FetchedTranscriptSnippet(text='only one of many, many iconic interviews', start=872.16, duration=5.84), FetchedTranscriptSnippet(text='from the World Economic Forum this year.', start=875.76, duration=4.639), FetchedTranscriptSnippet(text=\"I promise it's worth following SquawkPod\", start=878.0, duration=4.24), FetchedTranscriptSnippet(text=\"wherever you're listening now to catch\", start=880.399, duration=4.401), FetchedTranscriptSnippet(text='each episode. Squawk Box is hosted by', start=882.24, duration=5.039), FetchedTranscriptSnippet(text='Joe Kernan, Becky Quick, and Andrew Ross', start=884.8, duration=5.279), FetchedTranscriptSnippet(text='Sorcin. Squawk Pod is produced by me,', start=887.279, duration=5.441), FetchedTranscriptSnippet(text='Cameron Costa, and Zach Visi. Katie', start=890.079, duration=4.481), FetchedTranscriptSnippet(text='Kramer has been with the anchors in', start=892.72, duration=4.32), FetchedTranscriptSnippet(text='Davos this week, and stateside with us', start=894.56, duration=5.76), FetchedTranscriptSnippet(text='is Julie Trass, our editor.', start=897.04, duration=7.159), FetchedTranscriptSnippet(text='Have a great day.', start=900.32, duration=3.879)], video_id='5XqDzEtYnqI', language='English (auto-generated)', language_code='en', is_generated=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytt_api = YouTubeTranscriptApi(\n",
    "    # proxy_config=WebshareProxyConfig(\n",
    "    #     proxy_username = os.getenv(\"PROXY_USER\"),\n",
    "    #     proxy_password = os.getenv(\"PROXY_PASS\"),\n",
    "    # )\n",
    ")\n",
    "transcript = ytt_api.fetch('5XqDzEtYnqI')\n",
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c0f03",
   "metadata": {},
   "source": [
    "### Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c444da",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSCRIPT_CACHE_PATH = Path('daily_transcripts.json')\n",
    "MAX_CACHE_SIZE = 100  # Maximum number of videos to keep in cache\n",
    "\n",
    "def load_transcript_cache(path):\n",
    "    if path.exists():\n",
    "        try:\n",
    "            content = path.read_text(encoding='utf-8')\n",
    "            if content.strip():\n",
    "                return json.loads(content)\n",
    "            else:\n",
    "                print(\"⚠️  Cache file is empty, starting fresh\")\n",
    "                return {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"⚠️  Cache file is corrupted: {e}\")\n",
    "            print(\"   Starting with fresh cache\")\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_transcript_cache(path, cache):\n",
    "    try:\n",
    "        path.write_text(json.dumps(cache, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Failed to save cache: {e}\")\n",
    "\n",
    "def fetch_transcript_with_backoff(video_id, max_retries=3):\n",
    "    \"\"\"\n",
    "    Fetch transcript with exponential backoff and jitter.\n",
    "    No proxy - relies on longer delays to avoid rate limits.\n",
    "    \"\"\"\n",
    "    base_delay = 5.0  # Longer initial delay without proxy\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            # Add random delay before each request (rate limit avoidance)\n",
    "            jitter = random.uniform(2, 5)\n",
    "            if attempt > 1:\n",
    "                time.sleep(jitter)\n",
    "            ytt_api = YouTubeTranscriptApi(\n",
    "                # proxy_config=WebshareProxyConfig(\n",
    "                #     proxy_username = os.getenv(\"PROXY_USER\"),\n",
    "                #     proxy_password = os.getenv(\"PROXY_PASS\"),\n",
    "                # )\n",
    "            )\n",
    "            transcript = ytt_api.fetch(video_id)\n",
    "            return ' '.join([seg.text for seg in transcript])\n",
    "            \n",
    "        except (TranscriptsDisabled, NoTranscriptFound):\n",
    "            # These are not rate limits, just unavailable transcripts\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e).lower()\n",
    "            \n",
    "            # Check for rate limit indicators\n",
    "            if any(indicator in error_msg for indicator in ['429', 'too many requests', 'rate limit', 'forbidden', '403']):\n",
    "                wait_time = base_delay * (2 ** (attempt - 1)) + random.uniform(5, 15)\n",
    "                print(f\"⚠️  Rate limit detected (attempt {attempt}/{max_retries})\")\n",
    "                print(f\"   Waiting {wait_time:.1f}s before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            # Other errors\n",
    "            error_type = type(e).__name__\n",
    "            print(f\"Attempt {attempt} failed for {video_id}: {error_type}: {str(e)[:100]}\")\n",
    "            # Skip immediately for unavailable/unplayable videos (no point retrying)\n",
    "            if any(skip in error_msg or skip in error_type.lower() for skip in \n",
    "                   ['unavailable', 'unplayable', 'private', 'deleted']):\n",
    "                print(f\"  ⏭️  Skipping (video unavailable)\")\n",
    "                return None\n",
    "            if attempt == max_retries:\n",
    "                return None\n",
    "            \n",
    "            # Exponential backoff for other errors\n",
    "            wait_time = base_delay * (1.5 ** attempt) + random.uniform(1, 3)\n",
    "            time.sleep(wait_time)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def attach_transcripts(videos, cache_path=TRANSCRIPT_CACHE_PATH, max_cache_size=MAX_CACHE_SIZE, delay_between_requests=3.0):\n",
    "    \"\"\"\n",
    "    Attach transcripts to videos with aggressive rate limit avoidance.\n",
    "    \n",
    "    Args:\n",
    "        videos: List of video dictionaries\n",
    "        cache_path: Path to cache file\n",
    "        max_cache_size: Maximum number of videos to keep in cache\n",
    "        delay_between_requests: Base delay between requests in seconds (default: 3.0)\n",
    "    \"\"\"\n",
    "    latest_ids = [v.get('video_id') for v in videos if v.get('video_id')]\n",
    "    total_videos = len(latest_ids)\n",
    "    print(f\"\\n📝 Processing {total_videos} videos for transcripts...\\n\")\n",
    "    print(f\"⏱️  Using delays to avoid rate limits (no proxy)\\n\")\n",
    "\n",
    "    # Load cache\n",
    "    cache = load_transcript_cache(cache_path)\n",
    "    old_cache_size = len(cache)\n",
    "    \n",
    "    # Create ordered list: newest videos first\n",
    "    all_video_ids = latest_ids.copy()\n",
    "    \n",
    "    # Add old cached videos that aren't in the new list\n",
    "    for old_vid in cache.keys():\n",
    "        if old_vid not in all_video_ids:\n",
    "            all_video_ids.append(old_vid)\n",
    "    \n",
    "    # Keep only the newest MAX_CACHE_SIZE videos\n",
    "    videos_to_keep = all_video_ids[:max_cache_size]\n",
    "    \n",
    "    # Filter cache\n",
    "    filtered_cache = {vid: cache[vid] for vid in videos_to_keep if vid in cache}\n",
    "    removed_count = old_cache_size - len(filtered_cache)\n",
    "    \n",
    "    print(f\"📦 Cache status: {old_cache_size} total → keeping {len(filtered_cache)} (removed {removed_count} oldest)\\n\")\n",
    "    \n",
    "    cache = filtered_cache\n",
    "\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    cached_count = 0\n",
    "    actual_idx = 0\n",
    "\n",
    "    for idx, video in enumerate(videos, start=1):\n",
    "        vid = video.get('video_id')\n",
    "        if not vid:\n",
    "            continue\n",
    "        \n",
    "        actual_idx = idx\n",
    "        \n",
    "        # Check cache first\n",
    "        if vid in cache:\n",
    "            video['transcript_text'] = cache[vid]\n",
    "            cached_count += 1\n",
    "            print(f\"[{idx}/{total_videos}] ✓ Cached: {vid} - {video.get('title', 'N/A')[:50]}\")\n",
    "            if cache[vid]:\n",
    "                print(f\"  Preview: {cache[vid][:150]}...\\n\")\n",
    "            continue\n",
    "        \n",
    "        # Add delay between requests to avoid rate limits\n",
    "        delay = delay_between_requests + random.uniform(1, 3)\n",
    "        print(f\"[{idx}/{total_videos}] Fetching: {vid} (waiting {delay:.1f}s)...\")\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        # Fetch transcript\n",
    "        try:\n",
    "            transcript_text = fetch_transcript_with_backoff(vid)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Unexpected error: {e}\")\n",
    "            transcript_text = None\n",
    "        \n",
    "        video['transcript_text'] = transcript_text\n",
    "        cache[vid] = transcript_text\n",
    "        \n",
    "        if transcript_text:\n",
    "            success_count += 1\n",
    "            print(f\"✓ Success: {video.get('title', 'N/A')[:50]}\")\n",
    "            print(f\"  Preview: {transcript_text[:150]}...\\n\")\n",
    "        else:\n",
    "            failed_count += 1\n",
    "            print(f\"✗ Failed/No transcript: {video.get('title', 'N/A')[:50]}\\n\")\n",
    "        \n",
    "        # Save cache periodically\n",
    "        if idx % 10 == 0:\n",
    "            save_transcript_cache(cache_path, cache)\n",
    "            print(f\"  💾 Cache saved at {idx} videos\\n\")\n",
    "\n",
    "    # Final save\n",
    "    save_transcript_cache(cache_path, cache)\n",
    "    \n",
    "    print(f\"\\n📊 Summary:\")\n",
    "    print(f\"  ✓ Successfully fetched: {success_count}\")\n",
    "    print(f\"  ✓ From cache: {cached_count}\")\n",
    "    print(f\"  ✗ Failed/No transcript: {failed_count}\")\n",
    "    print(f\"  Total processed: {actual_idx}/{total_videos}\")\n",
    "    print(f\"  📦 Final cache size: {len(cache)}/{max_cache_size}\")\n",
    "    print()\n",
    "    \n",
    "    return videos\n",
    "\n",
    "def refresh_transcripts_in_dict(videos, cache_path=Path('daily_transcripts.json')):\n",
    "    \"\"\"Refresh transcript data from cache file\"\"\"\n",
    "    if not cache_path.exists():\n",
    "        return videos\n",
    "    cache = json.loads(cache_path.read_text(encoding='utf-8'))\n",
    "    updated = 0\n",
    "    for video in videos:\n",
    "        vid = video.get('video_id')\n",
    "        if not vid:\n",
    "            continue\n",
    "        cached_value = cache.get(vid)\n",
    "        if cached_value is not None:\n",
    "            if video.get('transcript_text') != cached_value:\n",
    "                video['transcript_text'] = cached_value\n",
    "                updated += 1\n",
    "    print(f'Overwrote {updated} transcripts from cache')\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8575db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching videos from channel UCrp_UI8XtuYfpiqluWLD7Lw...\n",
      "  Fetched 50 videos so far...\n",
      "  Fetched 100 videos so far...\n",
      "✅ Total videos fetched: 100\n"
     ]
    }
   ],
   "source": [
    "youtube_videos_api = fetch_youtube_videos_with_api(CHANNEL_ID, YOUTUBE_API_KEY, max_results=MAX_VIDEOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d570f82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Processing 100 videos for transcripts...\n",
      "\n",
      "⏱️  Using delays to avoid rate limits (no proxy)\n",
      "\n",
      "📦 Cache status: 100 total → keeping 0 (removed 100 oldest)\n",
      "\n",
      "[1/100] Fetching: Bwzflqmqq3o (waiting 7.6s)...\n",
      "✗ Failed/No transcript: GRAPHIC WARNING: Federal immigration agents confro\n",
      "\n",
      "[2/100] Fetching: PLPOcORokNM (waiting 7.7s)...\n",
      "✗ Failed/No transcript: GRAPHIC WARNING: Video shows moment federal immigr\n",
      "\n",
      "[3/100] Fetching: rZSNZ4JQlQs (waiting 7.6s)...\n",
      "✗ Failed/No transcript: Video shows moments before federal immigration age\n",
      "\n",
      "[4/100] Fetching: -P_JxHFZkJc (waiting 7.5s)...\n",
      "✓ Success: Squawk Pod: Davos 2026: Binance Founder CZ - 01/25\n",
      "  Preview: My first roommate is a double murderer. Uh, and but he's been there for like 12 years. He still have 18 years to go. And then, you know, he got moved ...\n",
      "\n",
      "[5/100] Fetching: 5XqDzEtYnqI (waiting 7.4s)...\n",
      "✓ Success: Squawk Pod: Davos 2026: Google DeepMind CEO Demis \n",
      "  Preview: these LLMs, these foundation models like Gemini, they're getting better and better with each iteration and um we see no end to that. Um but on the oth...\n",
      "\n",
      "[6/100] Fetching: rzJfXrAjODY (waiting 7.7s)...\n",
      "✓ Success: Mad Money 01/23/26 | Audio Only\n",
      "  Preview: HEY, I'M KRAMER. WELCOME TO MONEY. Welcome to Kramer America. My friends, I'm just trying to make a little bit of money here. My job is not just to en...\n",
      "\n",
      "[7/100] Fetching: l16qQTmbzuc (waiting 6.3s)...\n",
      "✓ Success: Cramer's week ahead: It's a jam-packed week of ear\n",
      "  Preview: Money. Welcome to Cramer aka Adobe my friends I'm just trying to make a little bit of money here. My job is not just to entertain but to educate to so...\n",
      "\n",
      "[8/100] Fetching: kREwg6zOjBc (waiting 6.4s)...\n",
      "✓ Success: Gold and silver extend record run\n",
      "  Preview: NOVEMBER AND FALLING INTO THE RED FOR THE WEEK. THE RUSSELL STILL VASTLY OUTPACING THE OTHER MAJOR INDICES THIS YEAR, UP MORE THAN 7%. MEANTIME, GOLD ...\n",
      "\n",
      "[9/100] Fetching: RxVIpDX1DQk (waiting 6.8s)...\n",
      "✓ Success: Final Trade: VST, EEK, MSFT, DRI\n",
      "  Preview: MIKE KO. >> YOU KNOW, THE CHART LOOKS A LITTLE BIT DIFFICULT, I WOULD SAY IN VISTRA, BUT RIGHT NOW IT'S TRADING 18 TIMES FORWARD EARNINGS AND THAT'S A...\n",
      "\n",
      "[10/100] Fetching: dw4ZNaiMWUQ (waiting 7.9s)...\n",
      "✓ Success: 'Owning Manhattan' star Peter Zaitzeff says there \n",
      "  Preview: MILLION DUPLEX IN MANHATTAN LAST YEAR, AND SAYS THE UBER WEALTHY AREN'T GETTING SPOOKED BY MAYOR MAMDANI'S THREATS TO TAX THE RICH. PETER JOINS US HER...\n",
      "\n",
      "  💾 Cache saved at 10 videos\n",
      "\n",
      "[11/100] Fetching: goD-9mjkZ4c (waiting 6.1s)...\n",
      "✓ Success: Winter storm's impact on business and markets coul\n",
      "  Preview: respectable amount. And they seem to be willing to pivot when they need to. So, you know, it's hard to argue with the amount of cash that company gene...\n",
      "\n",
      "[12/100] Fetching: 8vLpEVl6ZJU (waiting 6.7s)...\n",
      "✓ Success: 'Fast Money' traders talk what's ahead for Big Tec\n",
      "  Preview: way up more than 6% for the week. Tesla and Microsoft also seeing gains. Apple though an outlier, it is down nearly 3%. So will the group find redempt...\n",
      "\n",
      "[13/100] Fetching: 7GZjGpTGqxc (waiting 6.2s)...\n",
      "✓ Success: Disappointing earnings season could drive continge\n",
      "  Preview: IS ON TRACK FOR ITS BIGGEST MONTHLY INFLOW SINCE ITS INCEPTION IN 2012. IT IS NOW OUTPERFORMING THE S&P BY THE MOST SINCE MARCH. OUR NEXT GUEST SAYS T...\n",
      "\n",
      "[14/100] Fetching: _mSfBuv4WSc (waiting 6.9s)...\n",
      "✓ Success: Tech Investor Dan Niles talks what to expect from \n",
      "  Preview: THE ENTIRE S&P 500. THOSE NAMES HAVE BEEN THE FOUR WORST PERFORMING STOCKS OF THE MACH SEVEN SO FAR IN 2026, ALL UNDERPERFORMING THE S&P AND THE NASDA...\n",
      "\n",
      "[15/100] Fetching: 3DJhxmY3IEk (waiting 7.8s)...\n",
      "✓ Success: Loss of intermediate tech momentum 'has been weath\n",
      "  Preview: SINCE THE S&P HIT WHAT WAS THEN AN INTRADAY HIGH OF AROUND 6920 BACK IN OCTOBER, IT HAS GONE NOWHERE. IN FACT, CLOSING BELOW THAT LEVEL TODAY. LET'S G...\n",
      "\n",
      "[16/100] Fetching: an3mGYzy718 (waiting 6.1s)...\n",
      "✓ Success: Alaska Airlines CEO on potential impact to flights\n",
      "  Preview: Our meteorologist just said he thinks that air travel is going to be ground to a halt for the next week. Is that true? What's going on behind the scen...\n",
      "\n",
      "[17/100] Fetching: O6GlSjQNwvk (waiting 6.9s)...\n",
      "✓ Success: Breakout in regional banks is good news, says Rena\n",
      "  Preview: FOR STOCKS, THERE WERE SOME NOTABLE BREAKOUTS. JEFF DEGRAFF IS CHAIRMAN AND HEAD OF TECHNICAL RESEARCH AT RENAISSANCE MACRO. HE JOINS US ONCE AGAIN NO...\n",
      "\n",
      "[18/100] Fetching: 7mU_fe4cQEA (waiting 7.5s)...\n",
      "⚠️  Rate limit detected (attempt 1/3)\n",
      "   Waiting 18.3s before retry...\n",
      "⚠️  Rate limit detected (attempt 2/3)\n",
      "   Waiting 21.6s before retry...\n",
      "⚠️  Rate limit detected (attempt 3/3)\n",
      "   Waiting 30.0s before retry...\n",
      "✗ Failed/No transcript: Meta has the most riding on earnings next week, sa\n",
      "\n",
      "[19/100] Fetching: lnebKhWPVyw (waiting 7.3s)...\n",
      "⚠️  Rate limit detected (attempt 1/3)\n",
      "   Waiting 15.4s before retry...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIpBlocked\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mfetch_transcript_with_backoff\u001b[39m\u001b[34m(video_id, max_retries)\u001b[39m\n\u001b[32m     38\u001b[39m ytt_api = YouTubeTranscriptApi(\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# proxy_config=WebshareProxyConfig(\u001b[39;00m\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m#     proxy_username = os.getenv(\"PROXY_USER\"),\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m#     proxy_password = os.getenv(\"PROXY_PASS\"),\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     43\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m transcript = \u001b[43mytt_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join([seg.text \u001b[38;5;28;01mfor\u001b[39;00m seg \u001b[38;5;129;01min\u001b[39;00m transcript])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_api.py:73\u001b[39m, in \u001b[36mYouTubeTranscriptApi.fetch\u001b[39m\u001b[34m(self, video_id, languages, preserve_formatting)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03mRetrieves the transcript for a single video. This is just a shortcut for\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03mcalling:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     68\u001b[39m \u001b[33;03m:param preserve_formatting: whether to keep select HTML text formatting\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_transcript\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreserve_formatting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreserve_formatting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:139\u001b[39m, in \u001b[36mTranscript.fetch\u001b[39m\u001b[34m(self, preserve_formatting)\u001b[39m\n\u001b[32m    137\u001b[39m response = \u001b[38;5;28mself\u001b[39m._http_client.get(\u001b[38;5;28mself\u001b[39m._url)\n\u001b[32m    138\u001b[39m snippets = _TranscriptParser(preserve_formatting=preserve_formatting).parse(\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[43m_raise_http_errors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvideo_id\u001b[49m\u001b[43m)\u001b[49m.text,\n\u001b[32m    140\u001b[39m )\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m FetchedTranscript(\n\u001b[32m    142\u001b[39m     snippets=snippets,\n\u001b[32m    143\u001b[39m     video_id=\u001b[38;5;28mself\u001b[39m.video_id,\n\u001b[32m   (...)\u001b[39m\u001b[32m    146\u001b[39m     is_generated=\u001b[38;5;28mself\u001b[39m.is_generated,\n\u001b[32m    147\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\News_Majority\\venv\\Lib\\site-packages\\youtube_transcript_api\\_transcripts.py:96\u001b[39m, in \u001b[36m_raise_http_errors\u001b[39m\u001b[34m(response, video_id)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m429\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IpBlocked(video_id)\n\u001b[32m     97\u001b[39m response.raise_for_status()\n",
      "\u001b[31mIpBlocked\u001b[39m: \nCould not retrieve a transcript for the video https://www.youtube.com/watch?v=lnebKhWPVyw! This is most likely caused by:\n\nYouTube is blocking requests from your IP. This usually is due to one of the following reasons:\n- You have done too many requests and your IP has been blocked by YouTube\n- You are doing requests from an IP belonging to a cloud provider (like AWS, Google Cloud Platform, Azure, etc.). Unfortunately, most IPs from cloud providers are blocked by YouTube.\n\nWays to work around this are explained in the \"Working around IP bans\" section of the README (https://github.com/jdepoix/youtube-transcript-api?tab=readme-ov-file#working-around-ip-bans-requestblocked-or-ipblocked-exception).\n\n\nIf you are sure that the described cause is not responsible for this error and that a transcript should be retrievable, please create an issue at https://github.com/jdepoix/youtube-transcript-api/issues. Please add which version of youtube_transcript_api you are using and provide the information needed to replicate the error. Also make sure that there are no open issues which already describe your problem!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m youtube_videos_api = \u001b[43mattach_transcripts\u001b[49m\u001b[43m(\u001b[49m\u001b[43myoutube_videos_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelay_between_requests\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36mattach_transcripts\u001b[39m\u001b[34m(videos, cache_path, max_cache_size, delay_between_requests)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Fetch transcript\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     transcript_text = \u001b[43mfetch_transcript_with_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m⚠️  Unexpected error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mfetch_transcript_with_backoff\u001b[39m\u001b[34m(video_id, max_retries)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m⚠️  Rate limit detected (attempt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwait_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms before retry...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Other errors\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "youtube_videos_api = attach_transcripts(youtube_videos_api, delay_between_requests = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a08c10",
   "metadata": {},
   "source": [
    "## Add transcripts to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6508b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite in-memory transcripts with cache values when available\n",
    "\n",
    "youtube_videos_api = refresh_transcripts_in_dict(youtube_videos_api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9d0e87",
   "metadata": {},
   "source": [
    "## Check rotating proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = os.getenv(\"PROXY_USER\")\n",
    "password = os.getenv(\"PROXY_PASS\")\n",
    "endpoint = os.getenv(\"PROXY_HOST\") +  \":\" + str(os.getenv(\"PROXY_PORT\"))\n",
    "\n",
    "proxy_url = f\"http://{username}:{password}@{endpoint}\"\n",
    "\n",
    "def check_ip_rotation(num_requests=10):\n",
    "    \"\"\"Check if proxy IPs are rotating\"\"\"\n",
    "    proxies = {\n",
    "        'http': proxy_url,\n",
    "        'https': proxy_url\n",
    "    }\n",
    "    \n",
    "    ips = []\n",
    "    for i in range(num_requests):\n",
    "        try:\n",
    "            # Using http instead of https for simpler testing\n",
    "            response = requests.get('http://ipinfo.io/json', \n",
    "                                   proxies=proxies, \n",
    "                                   timeout=10)\n",
    "            ip = response.json().get('ip')\n",
    "            ips.append(ip)\n",
    "            print(f\"Request {i+1}: IP = {ip}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Request {i+1} failed: {e}\")\n",
    "    \n",
    "    unique_ips = set(ips)\n",
    "    print(f\"\\nTotal requests: {len(ips)}\")\n",
    "    print(f\"Unique IPs: {len(unique_ips)}\")\n",
    "    print(f\"IPs are {'ROTATING ✓' if len(unique_ips) > 1 else 'NOT ROTATING ✗'}\")\n",
    "    return ips\n",
    "\n",
    "# Test rotation\n",
    "print(\"Testing IP rotation with Webshare:\")\n",
    "check_ip_rotation(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ff8479",
   "metadata": {},
   "source": [
    "# After Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17316fe",
   "metadata": {},
   "source": [
    "## Sentiment and Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0007c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "def chunk_text_words(text, chunk_words=500):\n",
    "    words = text.split()\n",
    "    return [\n",
    "        \" \".join(words[i:i+chunk_words])\n",
    "        for i in range(0, len(words), chunk_words)\n",
    "    ]\n",
    "\n",
    "def summarize_long_text(text):\n",
    "    chunks = chunk_text_words(text, chunk_words=500)\n",
    "\n",
    "    partial_summaries = []\n",
    "    for chunk in chunks:\n",
    "        result = summarizer(\n",
    "            chunk,\n",
    "            max_length=120,\n",
    "            min_length=40,\n",
    "            do_sample=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        partial_summaries.append(result[0][\"summary_text\"])\n",
    "\n",
    "    combined = \" \".join(partial_summaries)\n",
    "\n",
    "    final = summarizer(\n",
    "        combined,\n",
    "        max_length=180,\n",
    "        min_length=60,\n",
    "        do_sample=False,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    return final[0][\"summary_text\"]\n",
    "\n",
    "\n",
    "def analyze_video_sentiment(video, debug=False):\n",
    "    \"\"\"Analyze sentiment of title and transcript separately\"\"\"\n",
    "    \n",
    "    # Title: Direct sentiment (no summarization)\n",
    "    title = video.get('title', '')\n",
    "    if title:\n",
    "        try:\n",
    "            title_sentiment = sentiment_analyzer(title[:512])[0]\n",
    "            video['title_sentiment'] = title_sentiment\n",
    "        except Exception as e:\n",
    "            if debug:\n",
    "                print(f\"    Title sentiment failed: {e}\")\n",
    "            video['title_sentiment'] = None\n",
    "    else:\n",
    "        video['title_sentiment'] = None\n",
    "    \n",
    "    # Transcript: Summarize → Sentiment\n",
    "    transcript_text = video.get('transcript_text', '')\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"   Transcript length: {len(transcript_text)} chars, {len(transcript_text.split())} words\")\n",
    "    \n",
    "    # Check if transcript exists and is long enough\n",
    "    if not transcript_text or len(transcript_text.strip()) < 200:\n",
    "        if debug:\n",
    "            print(f\"    Transcript too short or missing\")\n",
    "        video['transcript_summary'] = None\n",
    "        video['transcript_sentiment'] = None\n",
    "        return video\n",
    "    \n",
    "    try:\n",
    "        # Clean and truncate transcript\n",
    "        transcript_text = transcript_text.strip()\n",
    "        words = transcript_text.split()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Word count: {len(words)}\")\n",
    "        \n",
    "        # BART works best with 100-1024 tokens\n",
    "        if len(words) < 100:\n",
    "            if debug:\n",
    "                print(f\"    Too few words: {len(words)}\")\n",
    "            video['transcript_summary'] = None\n",
    "            video['transcript_sentiment'] = None\n",
    "            return video\n",
    "        \n",
    "        if len(words) > 1000:\n",
    "            transcript_text = ' '.join(words[:1000])\n",
    "            if debug:\n",
    "                print(f\"    Truncated to 1000 words\")\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Generating summary...\")\n",
    "            print(f\"   First 200 chars: {transcript_text[:200]}\")\n",
    "        \n",
    "        # Generate summary with better parameters\n",
    "\n",
    "        summary = summarize_long_text(transcript_text)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Summary: {summary}\")\n",
    "        \n",
    "        # Sentiment of summary\n",
    "        transcript_sentiment = sentiment_analyzer(summary[:512])[0]\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Sentiment: {transcript_sentiment}\")\n",
    "        \n",
    "        video['transcript_summary'] = summary\n",
    "        video['transcript_sentiment'] = transcript_sentiment\n",
    "        \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"   Error: {type(e).__name__}: {str(e)}\")\n",
    "        video['transcript_summary'] = None\n",
    "        try:\n",
    "            video['transcript_sentiment'] = sentiment_analyzer(transcript_text[:512])[0]\n",
    "        except Exception:\n",
    "            video['transcript_sentiment'] = None\n",
    "    \n",
    "    return video\n",
    "\n",
    "# Test on first video with debug output\n",
    "print(\"\\nTesting first video with debug output:\\n\")\n",
    "if youtube_videos_api:\n",
    "    test_video = youtube_videos_api[0].copy()\n",
    "    print(f\"Title: {test_video.get('title')}\")\n",
    "    analyze_video_sentiment(test_video, debug=True)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Ask user if they want to continue\n",
    "response = input(\"Continue with all videos? (y/n): \")\n",
    "\n",
    "if response.lower() == 'y':\n",
    "    print(\"\\n Analyzing all videos...\")\n",
    "    \n",
    "    for video in tqdm(youtube_videos_api, desc=\"Processing videos\", unit=\"video\"):\n",
    "        if video.get('transcript_text'):\n",
    "            analyze_video_sentiment(video, debug=False)\n",
    "        else:\n",
    "            video['title_sentiment'] = None\n",
    "            video['transcript_summary'] = None\n",
    "            video['transcript_sentiment'] = None\n",
    "    \n",
    "    print(\"Analysis complete!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(videos, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(videos, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# usage\n",
    "save_to_json(youtube_videos_api, \"youtube_analysis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac76c1",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_entity(name: str) -> str:\n",
    "    if not name:\n",
    "        return name\n",
    "\n",
    "    n = name.strip().lower()\n",
    "\n",
    "    n = re.sub(r\"^(the|a|an)\\s+\", \"\", n)\n",
    "    n = re.sub(r\"[^\\w\\s]\", \"\", n)\n",
    "    n = re.sub(r\"\\s+\", \" \", n)\n",
    "\n",
    "    if n in ENTITY_ALIASES:\n",
    "        return ENTITY_ALIASES[n]\n",
    "\n",
    "    return n.upper() if n.isupper() else n.title()\n",
    "\n",
    "def sentiment_to_score(sentiment):\n",
    "    if not sentiment:\n",
    "        return None\n",
    "    label = str(sentiment.get('label', '')).upper()\n",
    "    score = float(sentiment.get('score', 0))\n",
    "    if 'POS' in label:\n",
    "        return score\n",
    "    if 'NEG' in label:\n",
    "        return -score\n",
    "    return 0.0\n",
    "\n",
    "def extract_video_text(video, prefer_summary=True):\n",
    "    title = video.get('title', '')\n",
    "    transcript = ''\n",
    "    if prefer_summary and video.get('transcript_summary'):\n",
    "        transcript = video['transcript_summary']\n",
    "    elif video.get('transcript_text'):\n",
    "        transcript = video['transcript_text']\n",
    "    combined = f\"{title} {transcript}\".strip()\n",
    "    return combined\n",
    "\n",
    "def analyze_video_entities_split(video):\n",
    "    title = video.get('title', '') or ''\n",
    "\n",
    "    # Prefer summary, but fall back to full transcript_text if no summary\n",
    "    raw_summary = video.get('transcript_summary') or video.get('transcript_text') or ''\n",
    "    summary = raw_summary\n",
    "\n",
    "    title_doc = nlp(title) if title else None\n",
    "    summary_doc = nlp(summary) if summary else None\n",
    "\n",
    "    title_tickers = set(get_tickers(title)) if title else set()\n",
    "    title_companies = set(get_companies(title_doc)) if title_doc else set()\n",
    "    title_sectors = set(get_sectors(title.lower())) if title else set()\n",
    "\n",
    "    summary_tickers = set(get_tickers(summary)) if summary else set()\n",
    "    summary_companies = set(get_companies(summary_doc)) if summary_doc else set()\n",
    "    summary_sectors = set(get_sectors(summary.lower())) if summary else set()\n",
    "\n",
    "    title_score = sentiment_to_score(video.get('title_sentiment'))\n",
    "    summary_score = sentiment_to_score(video.get('transcript_sentiment'))\n",
    "\n",
    "    return {\n",
    "        \"title\": (title_tickers, title_companies, title_sectors, title_score),\n",
    "        \"summary\": (summary_tickers, summary_companies, summary_sectors, summary_score),\n",
    "    }\n",
    "\n",
    "def aggregate_youtube_entities(videos):\n",
    "\n",
    "    def new_bucket():\n",
    "        return {\n",
    "            \"title_mentions\": 0,\n",
    "            \"title_scores\": [],  # One score per video where entity appears in title\n",
    "            \"summary_mentions\": 0,\n",
    "            \"summary_scores\": [],  # One score per video where entity appears in summary\n",
    "        }\n",
    "\n",
    "    stock_stats = defaultdict(new_bucket)\n",
    "    company_stats = defaultdict(new_bucket)\n",
    "    sector_stats = defaultdict(new_bucket)\n",
    "\n",
    "    for video in videos:\n",
    "        parts = analyze_video_entities_split(video)\n",
    "\n",
    "        # Track which entities we've already counted for this video (per part)\n",
    "        # to avoid adding the same score multiple times\n",
    "        title_entities_seen = set()\n",
    "        summary_entities_seen = set()\n",
    "\n",
    "        for part_name, (tickers, companies, sectors, score) in parts.items():\n",
    "            is_title = (part_name == \"title\")\n",
    "            seen_set = title_entities_seen if is_title else summary_entities_seen\n",
    "\n",
    "            for t in tickers:\n",
    "                t = normalize_entity(t)\n",
    "                stock_stats[t][f\"{part_name}_mentions\"] += 1\n",
    "                # Only add score once per video per part\n",
    "                if t not in seen_set and score is not None:\n",
    "                    stock_stats[t][f\"{part_name}_scores\"].append(score)\n",
    "                    seen_set.add(t)\n",
    "\n",
    "            for c in companies:\n",
    "                c = normalize_entity(c)\n",
    "                company_stats[c][f\"{part_name}_mentions\"] += 1\n",
    "                # Only add score once per video per part\n",
    "                if c not in seen_set and score is not None:\n",
    "                    company_stats[c][f\"{part_name}_scores\"].append(score)\n",
    "                    seen_set.add(c)\n",
    "\n",
    "            for s in sectors:\n",
    "                s = normalize_entity(s)\n",
    "                sector_stats[s][f\"{part_name}_mentions\"] += 1\n",
    "                # Only add score once per video per part\n",
    "                if s not in seen_set and score is not None:\n",
    "                    sector_stats[s][f\"{part_name}_scores\"].append(score)\n",
    "                    seen_set.add(s)\n",
    "\n",
    "    def finalize(stats):\n",
    "        rows = []\n",
    "        for name, data in stats.items():\n",
    "            rows.append({\n",
    "                \"name\": name,\n",
    "\n",
    "                \"title_mentions\": data[\"title_mentions\"],\n",
    "                \"avg_title_sentiment\": (\n",
    "                    sum(data[\"title_scores\"]) / len(data[\"title_scores\"])\n",
    "                    if data[\"title_scores\"] else None\n",
    "                ),\n",
    "\n",
    "                \"summary_mentions\": data[\"summary_mentions\"],\n",
    "                \"avg_summary_sentiment\": (\n",
    "                    sum(data[\"summary_scores\"]) / len(data[\"summary_scores\"])\n",
    "                    if data[\"summary_scores\"] else None\n",
    "                ),\n",
    "            })\n",
    "\n",
    "        rows.sort(key=lambda x: (x[\"title_mentions\"] + x[\"summary_mentions\"]), reverse=True)\n",
    "        return rows\n",
    "\n",
    "    return {\n",
    "        \"stocks\": finalize(stock_stats),\n",
    "        \"companies\": finalize(company_stats),\n",
    "        \"sectors\": finalize(sector_stats),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8984daab",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = aggregate_youtube_entities(youtube_videos_api)\n",
    "save_to_json(result, \"entity_mentions.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['companies']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e665887a",
   "metadata": {},
   "source": [
    "## Turn Mentions into readable txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599f717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sentiment(score):\n",
    "    \"\"\"Format sentiment score for display.\"\"\"\n",
    "    if score is None:\n",
    "        return \"N/A\"\n",
    "    return f\"{score:+.4f}\"\n",
    "\n",
    "def format_mentions(item):\n",
    "    \"\"\"Format a single item's mention data.\"\"\"\n",
    "    total_mentions = item.get(\"title_mentions\", 0) + item.get(\"summary_mentions\", 0)\n",
    "    title_sent = format_sentiment(item.get(\"avg_title_sentiment\"))\n",
    "    summary_sent = format_sentiment(item.get(\"avg_summary_sentiment\"))\n",
    "    \n",
    "    lines = [\n",
    "        f\"  Name: {item['name']}\",\n",
    "        f\"  Total Mentions: {total_mentions}\",\n",
    "        f\"    - Title Mentions: {item.get('title_mentions', 0)} (Sentiment: {title_sent})\",\n",
    "        f\"    - Summary Mentions: {item.get('summary_mentions', 0)} (Sentiment: {summary_sent})\"\n",
    "    ]\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = Path(\"entity_mentions.json\")\n",
    "with open(json_path, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build the output text\n",
    "output_lines = []\n",
    "output_lines.append(\"=\" * 80)\n",
    "output_lines.append(\"ENTITY MENTIONS REPORT\")\n",
    "output_lines.append(\"=\" * 80)\n",
    "output_lines.append(\"\")\n",
    "\n",
    "# Stocks Section\n",
    "output_lines.append(\"STOCKS\")\n",
    "output_lines.append(\"-\" * 80)\n",
    "if data.get(\"stocks\"):\n",
    "    for i, stock in enumerate(data[\"stocks\"], 1):\n",
    "        output_lines.append(f\"\\n{i}. {format_mentions(stock)}\")\n",
    "else:\n",
    "    output_lines.append(\"  No stocks found.\")\n",
    "output_lines.append(\"\")\n",
    "output_lines.append(\"\")\n",
    "\n",
    "# Companies Section\n",
    "output_lines.append(\"COMPANIES\")\n",
    "output_lines.append(\"-\" * 80)\n",
    "if data.get(\"companies\"):\n",
    "    for i, company in enumerate(data[\"companies\"], 1):\n",
    "        output_lines.append(f\"\\n{i}. {format_mentions(company)}\")\n",
    "else:\n",
    "    output_lines.append(\"  No companies found.\")\n",
    "output_lines.append(\"\")\n",
    "output_lines.append(\"\")\n",
    "\n",
    "# Sectors Section\n",
    "output_lines.append(\"SECTORS\")\n",
    "output_lines.append(\"-\" * 80)\n",
    "if data.get(\"sectors\"):\n",
    "    for i, sector in enumerate(data[\"sectors\"], 1):\n",
    "        output_lines.append(f\"\\n{i}. {format_mentions(sector)}\")\n",
    "else:\n",
    "    output_lines.append(\"  No sectors found.\")\n",
    "\n",
    "output_lines.append(\"\")\n",
    "output_lines.append(\"=\" * 80)\n",
    "\n",
    "# Join all lines\n",
    "output_text = \"\\n\".join(output_lines)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf4eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path(\"entity_mentions.txt\")\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(output_text)\n",
    "\n",
    "print(f\"Successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def28e16",
   "metadata": {},
   "source": [
    "# Filter by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('100vids.json', 'r', encoding='utf-8') as file:\n",
    "    # Use json.load() to convert the file content to a Python object\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def filter_by_date_range(videos, chosen_date_str):\n",
    "    \"\"\"\n",
    "    chosen_date_str format: 'YYYY-MM-DD'\n",
    "    \"\"\"\n",
    "    chosen_date = datetime.strptime(chosen_date_str, \"%Y-%m-%d\")\n",
    "    start_date = chosen_date - timedelta(days=7)\n",
    "\n",
    "    filtered = []\n",
    "\n",
    "    for v in videos:\n",
    "        published_str = v.get(\"published\") or v.get(\"published_date\")\n",
    "        if not published_str:\n",
    "            continue\n",
    "\n",
    "        published_dt = datetime.fromisoformat(published_str.replace(\"Z\", \"\"))\n",
    "\n",
    "        if start_date <= published_dt <= chosen_date:\n",
    "            filtered.append(v)\n",
    "\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c1bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a705a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_date = \"2026-01-15\"\n",
    "\n",
    "filtered_videos = filter_by_date_range(data, chosen_date)\n",
    "filtered_videos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
